From 506f84949a955107926ce66f9c9153327ae79e8a Mon Sep 17 00:00:00 2001
From: Mike Lui <mike.d.lui@gmail.com>
Date: Wed, 7 Mar 2018 13:39:34 -0500
Subject: [PATCH] Added new gengrind implementation

Gengrind is from scratch, compared to Sigrind, which is
a modified Callgrind implementation. Gengrind tries to
as much instrumentation as possible with VEX IR instead of
inserting function "Dirty Calls", which potentially incur
more overhead.

Gengrind is still in development as of this release.

Current issues and features still to be included are:

1) Conditional event generation is incomplete. I am still
working out how to conditionally execute inserted IRStmts.
If this is not possible, then there will be two implementations,
one fast one that unconditionally generates events with VEX IR,
and another slower one that conditionally generates events inside
Dirty Calls.

2) The multithreaded implementation is still in development,
but nearly complete. Thread support requires conditional event
generation, so that events are not generated when inside, e.g.,
a pthread call. So 2) is dependent on 1).
---
 Makefile.am                   |    2 +
 configure.ac                  |    4 +
 gengrind/Makefile.am          |   68 ++
 gengrind/gn.h                 |   74 ++
 gengrind/gn_bb.c              |  190 +++++
 gengrind/gn_bb.h              |   91 ++
 gengrind/gn_callstack.c       |  377 +++++++++
 gengrind/gn_callstack.h       |   86 ++
 gengrind/gn_clo.c             |   48 ++
 gengrind/gn_clo.h             |   38 +
 gengrind/gn_crq.c             |  150 ++++
 gengrind/gn_crq.h             |  271 ++++++
 gengrind/gn_debug.c           |   81 ++
 gengrind/gn_debug.h           |   39 +
 gengrind/gn_events.c          |  936 +++++++++++++++++++++
 gengrind/gn_events.h          |  102 +++
 gengrind/gn_fn.c              |  495 +++++++++++
 gengrind/gn_fn.h              |   83 ++
 gengrind/gn_ipc.c             |  350 ++++++++
 gengrind/gn_ipc.h             |   43 +
 gengrind/gn_jumps.c           |  160 ++++
 gengrind/gn_jumps.h           |   60 ++
 gengrind/gn_main.c            |  258 ++++++
 gengrind/gn_sync.h            |   57 ++
 gengrind/gn_sync_intercepts.c |   57 ++
 gengrind/gn_threads.c         |  178 ++++
 gengrind/gn_threads.h         |   40 +
 gengrind/tests/Makefile.am    |    5 +
 sigrind/.ycm_extra_conf.py    |  177 ++++
 sigrind/Makefile.am           |   83 ++
 sigrind/bb.c                  |  345 ++++++++
 sigrind/bbcc.c                |  872 +++++++++++++++++++
 sigrind/callgrind.h           |  363 ++++++++
 sigrind/callstack.c           |  425 ++++++++++
 sigrind/clo.c                 |  687 +++++++++++++++
 sigrind/context.c             |  332 ++++++++
 sigrind/debug.c               |  447 ++++++++++
 sigrind/events.c              |  261 ++++++
 sigrind/events.h              |  133 +++
 sigrind/fn.c                  |  686 +++++++++++++++
 sigrind/global.h              |  886 +++++++++++++++++++
 sigrind/jumps.c               |  233 +++++
 sigrind/log_events.c          |  239 ++++++
 sigrind/log_events.h          |   63 ++
 sigrind/sg_main.c             | 1872 +++++++++++++++++++++++++++++++++++++++++
 sigrind/sigil2_ipc.c          |  266 ++++++
 sigrind/sigil2_ipc.h          |   28 +
 sigrind/tests/Makefile.am     |    5 +
 sigrind/threads.c             |  451 ++++++++++
 49 files changed, 13197 insertions(+)
 create mode 100644 gengrind/Makefile.am
 create mode 100644 gengrind/gn.h
 create mode 100644 gengrind/gn_bb.c
 create mode 100644 gengrind/gn_bb.h
 create mode 100644 gengrind/gn_callstack.c
 create mode 100644 gengrind/gn_callstack.h
 create mode 100644 gengrind/gn_clo.c
 create mode 100644 gengrind/gn_clo.h
 create mode 100644 gengrind/gn_crq.c
 create mode 100644 gengrind/gn_crq.h
 create mode 100644 gengrind/gn_debug.c
 create mode 100644 gengrind/gn_debug.h
 create mode 100644 gengrind/gn_events.c
 create mode 100644 gengrind/gn_events.h
 create mode 100644 gengrind/gn_fn.c
 create mode 100644 gengrind/gn_fn.h
 create mode 100644 gengrind/gn_ipc.c
 create mode 100644 gengrind/gn_ipc.h
 create mode 100644 gengrind/gn_jumps.c
 create mode 100644 gengrind/gn_jumps.h
 create mode 100644 gengrind/gn_main.c
 create mode 100644 gengrind/gn_sync.h
 create mode 100644 gengrind/gn_sync_intercepts.c
 create mode 100644 gengrind/gn_threads.c
 create mode 100644 gengrind/gn_threads.h
 create mode 100644 gengrind/tests/Makefile.am
 create mode 100644 sigrind/.ycm_extra_conf.py
 create mode 100644 sigrind/Makefile.am
 create mode 100644 sigrind/bb.c
 create mode 100644 sigrind/bbcc.c
 create mode 100644 sigrind/callgrind.h
 create mode 100644 sigrind/callstack.c
 create mode 100644 sigrind/clo.c
 create mode 100644 sigrind/context.c
 create mode 100644 sigrind/debug.c
 create mode 100644 sigrind/events.c
 create mode 100644 sigrind/events.h
 create mode 100644 sigrind/fn.c
 create mode 100644 sigrind/global.h
 create mode 100644 sigrind/jumps.c
 create mode 100644 sigrind/log_events.c
 create mode 100644 sigrind/log_events.h
 create mode 100644 sigrind/sg_main.c
 create mode 100644 sigrind/sigil2_ipc.c
 create mode 100644 sigrind/sigil2_ipc.h
 create mode 100644 sigrind/tests/Makefile.am
 create mode 100644 sigrind/threads.c

diff --git a/Makefile.am b/Makefile.am
index fdce3cf9f..6859f9611 100644
--- a/Makefile.am
+++ b/Makefile.am
@@ -10,6 +10,8 @@ TOOLS =		memcheck \
 		lackey \
 		none \
 		helgrind \
+		sigrind \
+		gengrind \
 		drd
 
 EXP_TOOLS = 	exp-sgcheck \
diff --git a/configure.ac b/configure.ac
index 4d4521896..624c52200 100644
--- a/configure.ac
+++ b/configure.ac
@@ -4523,6 +4523,10 @@ AC_CONFIG_FILES([
    exp-dhat/tests/Makefile
    shared/Makefile
    solaris/Makefile
+   sigrind/Makefile
+   sigrind/tests/Makefile
+   gengrind/Makefile
+   gengrind/tests/Makefile
 ])
 AC_CONFIG_FILES([coregrind/link_tool_exe_linux],
                 [chmod +x coregrind/link_tool_exe_linux])
diff --git a/gengrind/Makefile.am b/gengrind/Makefile.am
new file mode 100644
index 000000000..263117117
--- /dev/null
+++ b/gengrind/Makefile.am
@@ -0,0 +1,68 @@
+include $(top_srcdir)/Makefile.tool.am
+
+EXTRA_DIST = docs/gn-manual.xml
+
+#----------------------------------------------------------------------------
+# gengrind-<platform>
+#----------------------------------------------------------------------------
+
+noinst_PROGRAMS  = gengrind-@VGCONF_ARCH_PRI@-@VGCONF_OS@
+if VGCONF_HAVE_PLATFORM_SEC
+noinst_PROGRAMS += gengrind-@VGCONF_ARCH_SEC@-@VGCONF_OS@
+endif
+
+GENGRIND_SOURCES_COMMON = gn_main.c\
+						  gn_events.c\
+						  gn_bb.c\
+						  gn_callstack.c\
+						  gn_jumps.c\
+						  gn_fn.c\
+						  gn_threads.c\
+						  gn_crq.c\
+						  gn_ipc.c\
+						  gn_clo.c\
+						  gn_debug.c
+
+GENGRIND_CFLAGS_COMMON =  -I$(top_srcdir)/../../.. -I$(top_srcdir)/include -I$(top_srcdir)/VEX/pub
+
+gengrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_SOURCES      = \
+	$(GENGRIND_SOURCES_COMMON)
+gengrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CPPFLAGS     = \
+	$(AM_CPPFLAGS_@VGCONF_PLATFORM_PRI_CAPS@)
+gengrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CFLAGS       = \
+	$(AM_CFLAGS_@VGCONF_PLATFORM_PRI_CAPS@) $(GENGRIND_CFLAGS_COMMON)
+gengrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_DEPENDENCIES = \
+	$(TOOL_DEPENDENCIES_@VGCONF_PLATFORM_PRI_CAPS@)
+gengrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDADD        = \
+	$(TOOL_LDADD_@VGCONF_PLATFORM_PRI_CAPS@)
+gengrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDFLAGS      = \
+	$(TOOL_LDFLAGS_@VGCONF_PLATFORM_PRI_CAPS@)
+gengrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LINK = \
+	$(top_builddir)/coregrind/link_tool_exe_@VGCONF_OS@ \
+	@VALT_LOAD_ADDRESS_PRI@ \
+	$(LINK) \
+	$(gengrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CFLAGS) \
+	$(gengrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDFLAGS)
+
+if VGCONF_HAVE_PLATFORM_SEC
+gengrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_SOURCES      = \
+	$(GENGRIND_SOURCES_COMMON)
+gengrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CPPFLAGS     = \
+	$(AM_CPPFLAGS_@VGCONF_PLATFORM_SEC_CAPS@)
+gengrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CFLAGS       = \
+	$(AM_CFLAGS_@VGCONF_PLATFORM_SEC_CAPS@) $(GENGRIND_CFLAGS_COMMON)
+gengrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_DEPENDENCIES = \
+	$(TOOL_DEPENDENCIES_@VGCONF_PLATFORM_SEC_CAPS@)
+gengrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDADD        = \
+	$(TOOL_LDADD_@VGCONF_PLATFORM_SEC_CAPS@)
+gengrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDFLAGS      = \
+	$(TOOL_LDFLAGS_@VGCONF_PLATFORM_SEC_CAPS@)
+gengrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LINK = \
+	$(top_builddir)/coregrind/link_tool_exe_@VGCONF_OS@ \
+	@VALT_LOAD_ADDRESS_SEC@ \
+	$(LINK) \
+	$(gengrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CFLAGS) \
+	$(gengrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDFLAGS)
+endif
+
+
diff --git a/gengrind/gn.h b/gengrind/gn.h
new file mode 100644
index 000000000..4cfb3d801
--- /dev/null
+++ b/gengrind/gn.h
@@ -0,0 +1,74 @@
+#ifndef GN_H
+#define GN_H
+
+#include "pub_tool_basics.h"
+#include "pub_tool_tooliface.h"
+#include "pub_tool_options.h"
+#include "pub_tool_debuginfo.h"
+#include "pub_tool_libcbase.h"
+#include "pub_tool_libcassert.h"
+#include "pub_tool_libcfile.h"
+#include "pub_tool_libcprint.h"
+#include "pub_tool_libcproc.h"
+#include "pub_tool_machine.h"
+#include "pub_tool_mallocfree.h"
+#include "Core/EventBuffer.h"
+
+#define GN_ENABLE_DEBUG 1
+
+#ifdef VGA_amd64
+#include "VEX/pub/libvex_guest_amd64.h"
+#endif
+
+#define GN_(str) VGAPPEND(vgGengrind_,str)
+/* The convention is to prepend global funcs/vars/structs with GN_(str).
+ * file/local scoped funcs/vars/structs do not have a naming convention,
+ * but can be prepended with 'gn'. */
+
+#define MEMBER_SIZE(type, member) sizeof(((type *)0)->member)
+
+//-------------------------------------------------------------------------------------------------
+/** Platform specific defines **/
+#if defined(VG_BIGENDIAN)
+#define ENDNESS Iend_BE
+#elif defined(VG_LITTLEENDIAN)
+#define ENDNESS Iend_LE
+#else
+#error "Unknown endianness"
+#endif
+
+#ifdef VGA_amd64
+#define OFFB_RIP offsetof(VexGuestAMD64State,guest_RIP)
+#define IRCONST_PTR(...) IRConst_U64(__VA_ARGS__)
+#define IRTYPE_PTR Ity_I64
+#define IOP_ADD_PTR Iop_Add64
+#define IOP_SUB_PTR Iop_Sub64
+#define IOP_CMPLT_PTR Iop_CmpLT64U
+#else
+#error "Unsupported platform"
+#endif
+
+//-------------------------------------------------------------------------------------------------
+/** Forward declare typedefs **/
+
+/* BB lookup type declarations */
+typedef struct _BBState BBState;
+typedef struct _BBInfo BBInfo;
+typedef struct _BBTable BBTable;
+
+/* Callstack tracking type declarations */
+typedef struct _CallStack CallStack;
+typedef struct _CallEntry CallEntry;
+
+/* Jumps type declarations */
+typedef struct _JumpNode JumpNode;
+typedef struct _JumpTable JumpTable;
+typedef enum _GnJumpKind GnJumpKind;
+
+/* Function lookup type declarations */
+typedef struct _FnNode FnNode;
+typedef struct _FileNode FileNode;
+typedef struct _ObjNode ObjNode;
+
+
+#endif
diff --git a/gengrind/gn_bb.c b/gengrind/gn_bb.c
new file mode 100644
index 000000000..d7022a682
--- /dev/null
+++ b/gengrind/gn_bb.c
@@ -0,0 +1,190 @@
+#include "gn_bb.h"
+#include "gn_fn.h"
+#include "gn_clo.h"
+#include "gn_debug.h"
+
+
+BBTable GN_(allBBs);
+BBInfo *GN_(lastBB);
+ULong GN_(bbExecutions);
+ULong GN_(bbSeen);
+
+
+//-------------------------------------------------------------------------------------------------
+/** Helper function definitions **/
+
+static inline Int bbHashIdx(ObjNode *obj, PtrdiffT offset, UInt size)
+{
+    return (((Addr)obj + offset) % size);
+}
+
+
+static void resizeBBTable(void)
+{
+    /* Double size
+     * Requires rehashing all BBs in the current table */
+
+    UInt conflicts1 = 0, conflicts2 = 0; // for hash table stats
+
+    UInt newcap = 2 * GN_(allBBs).capacity + 3;
+    BBInfo **newtable = VG_(malloc)("gn.bb.resize.1", newcap * sizeof(BBInfo*));
+
+    for (UInt i=0; i<newcap; ++i)
+        newtable[i] = NULL;
+
+    for (UInt i=0; i<GN_(allBBs).capacity; ++i) {
+
+        if (GN_(allBBs).table[i] == NULL)
+            continue;
+
+        BBInfo *curr = GN_(allBBs).table[i];
+        while (curr != NULL) {
+            BBInfo *next = curr->next;
+            UInt newidx = bbHashIdx(curr->obj, curr->offset, newcap);
+
+            curr->next = newtable[newidx];
+            newtable[newidx] = curr;
+
+            /* collect some hash table stats */
+            if (curr->next) {
+                conflicts1++;
+                if (curr->next->next) {
+                    conflicts2++;
+                }
+            }
+
+            curr = next;
+        }
+    }
+
+    VG_(free)(GN_(allBBs).table);
+    GN_(allBBs).capacity = newcap;
+    GN_(allBBs).table = newtable;
+}
+
+
+static BBInfo* newBB(ObjNode *obj, PtrdiffT offset)
+{
+    BBInfo *bb;
+
+    GN_(allBBs).entries++;
+
+    /* Use Callgrind's hash implementation
+     * resize if filled more than 80% */
+    if (10 * GN_(allBBs).entries / GN_(allBBs).capacity > 8)
+        resizeBBTable();
+
+    bb = VG_(malloc)("gn.bb.newbb.1", sizeof(BBInfo));
+
+    bb->offset   = offset;
+    bb->obj      = obj;
+    bb->sectKind = VG_(DebugInfo_sect_kind)(NULL, offset + obj->offset);
+
+    // To be set during instrumentation
+    bb->eventsTotal     = 0;
+    bb->jmpCount        = 0;
+    bb->jmps            = NULL;
+    bb->lruToJmp        = NULL;
+    bb->lruFromJmp      = NULL;
+    bb->lastJmpInverted = False;
+
+    bb->uid = GN_(bbSeen)++;
+
+    /* insert into hash */
+    UInt idx = bbHashIdx(obj, offset, GN_(allBBs).capacity);
+    bb->next = GN_(allBBs).table[idx];
+    GN_(allBBs).table[idx] = bb;
+
+    /* set up function info */
+    bb->fn = NULL;
+    bb->isFnEntry = False;
+    GN_(getFnNode)(bb);
+
+    return bb;
+}
+
+
+//-------------------------------------------------------------------------------------------------
+/** External function definitions **/
+
+BBInfo* GN_(getBBInfo)(Addr addr)
+{
+    BBInfo *bb;
+
+    ObjNode *obj = GN_(getObjNodeByAddr)(addr);
+    PtrdiffT offset = addr - obj->offset;
+
+    Int idx = bbHashIdx(obj, offset, GN_(allBBs).capacity);
+    bb = GN_(allBBs).table[idx];
+
+    while (bb) {
+        if (bb->obj == obj && bb->offset == offset)
+            break;
+        bb = bb->next;
+    }
+
+    if (bb == NULL)
+        bb = newBB(obj, offset);
+
+    return bb;
+}
+
+
+Int GN_(initBBState)(BBState *bbState, IRSB *obb, IRType hWordTy)
+{
+    Int i = 0;
+
+    bbState->obb = obb;
+    bbState->eventsToFlush = 0;
+
+    // Set up NBB for instrumentation
+    bbState->nbb = deepCopyIRSBExceptStmts(obb);
+    bbState->hWordTy = hWordTy;
+
+    // Copy verbatim any IR preamble preceding the first IMark
+    while (i < obb->stmts_used && obb->stmts[i]->tag != Ist_IMark) {
+        addStmtToIRSB( bbState->nbb, obb->stmts[i] );
+        i++;
+    }
+
+    // Get the first statement
+    GN_ASSERT(obb->stmts_used > 0);
+    GN_ASSERT(i < obb->stmts_used);
+    GN_ASSERT(Ist_IMark == obb->stmts[i]->tag);
+
+    // Any extra state needed
+    bbState->jmpsPassed = 0;
+    if (GN_(clo).bbinfo_needed == True) {
+        const IRStmt *st = obb->stmts[i];
+        Addr origAddr = st->Ist.IMark.addr + st->Ist.IMark.delta;
+        bbState->bbInfo = GN_(getBBInfo)(origAddr);
+    }
+    else {
+        bbState->bbInfo = NULL;
+    }
+
+    /* return index of first instruction IR */
+    return i;
+}
+
+
+void GN_(initAllBBsTable)()
+{
+    /* Once again, taken from Callgrind */
+    GN_(allBBs).capacity = 8347;
+    GN_(allBBs).entries = 0;
+    GN_(allBBs).table = VG_(malloc)("gn.bb.inittable.1",
+                                    GN_(allBBs).capacity * sizeof(BBInfo*));
+
+    for (UInt i=0; i<GN_(allBBs).capacity; ++i)
+        GN_(allBBs).table[i] = NULL;
+}
+
+
+void GN_(initBB)()
+{
+    GN_(initAllBBsTable)();
+    GN_(lastBB) = NULL;
+    GN_(bbExecutions) = 0;
+    GN_(bbSeen) = 0;
+}
diff --git a/gengrind/gn_bb.h b/gengrind/gn_bb.h
new file mode 100644
index 000000000..41e098a8c
--- /dev/null
+++ b/gengrind/gn_bb.h
@@ -0,0 +1,91 @@
+#ifndef GN_BB_H
+#define GN_BB_H
+
+#include "gn.h"
+
+extern BBTable GN_(allBBs);
+extern BBInfo *GN_(lastBB);
+extern ULong GN_(bbExecutions);
+extern ULong GN_(bbSeen);
+
+//-------------------------------------------------------------------------------------------------
+/** BB-tracking type definitions **/
+
+struct _BBState {
+    /* Tracks the state during instrumentation.
+     * Useful for passing between instrumentation functions */
+
+    IRType hWordTy;
+
+    IRSB *obb;
+    // The original basic block
+
+    IRSB *nbb;
+    // The new basic block with instrumentation
+
+    BBInfo *bbInfo;
+    // metadata for this basic block
+    
+    UInt eventsToFlush;
+    // Number of static events to-be-added for the current basic block
+    // Events are held in a separate global array
+
+    Int jmpsPassed;
+};
+
+
+struct _BBInfo {
+    /* Useful metadata valid outside of instrumentation,
+     * e.g. which function/ELF object contain this BB
+     * Some vars are unused if function-tracking is not enabled */
+
+    BBInfo* next;
+    // chaining for hash collisions
+
+    UInt eventsTotal;
+    // events are periodically flushed before an exit
+
+    PtrdiffT offset;
+    // offset of BB in ELF object
+
+    ObjNode *obj;
+    VgSectKind sectKind;
+    // ELF object
+
+    FnNode *fn;
+    Bool isFnEntry;
+    // Do not access directly, use helper function getFnNode
+    // Containing function; evaluated on-demand and cached
+    // Only valid if function enter/exit events are enabled
+
+    UInt jmpCount;
+    GnJumpKind *jmps;
+    JumpNode *lruToJmp;
+    JumpNode *lruFromJmp;
+    Bool lastJmpInverted;
+    // all side exits
+    // Only valid if certain events are enabled (e.g. functions)
+    
+    UInt uid;
+    UInt instrCount;
+    // for debugging purposes
+};
+
+
+struct _BBTable {
+    /* all basic blocks saved here */
+    UInt entries;
+    UInt capacity;
+    BBInfo **table;
+};
+
+//-------------------------------------------------------------------------------------------------
+/** BB-tracking function declarations **/
+
+void GN_(initBB)(void);
+void GN_(initAllBBsTable)(void);
+
+BBInfo* GN_(getBBInfo)(Addr addr);
+Int GN_(initBBState)(BBState *bbState, IRSB *origBB, IRType hWordTy);
+
+#endif
diff --git a/gengrind/gn_callstack.c b/gengrind/gn_callstack.c
new file mode 100644
index 000000000..a3daaf52b
--- /dev/null
+++ b/gengrind/gn_callstack.c
@@ -0,0 +1,377 @@
+#include "gn.h"
+#include "gn_events.h"
+#include "gn_bb.h"
+#include "gn_fn.h"
+#include "gn_callstack.h"
+#include "gn_jumps.h"
+#include "gn_debug.h"
+#include "pub_tool_threadstate.h"
+
+
+static CallStack currentCallStack;
+Bool GN_(afterStartFunc);
+Bool GN_(afterEndFunc);
+
+
+static inline Bool isFirstBB(void) { return GN_(lastBB) == NULL; }
+
+
+#define N_CALL_STACK_INITIAL_ENTRIES 500
+static void initCallStack(CallStack *s)
+{
+    GN_ASSERT(s != NULL);
+    s->capacity = N_CALL_STACK_INITIAL_ENTRIES;
+    s->entry = VG_(malloc)("gn.callstack.initcallstack.1",
+                           s->capacity * sizeof(CallEntry));
+    s->tos = 0;
+}
+
+
+static void resizeCurrentCallStack(void)
+{
+    if ((Long)currentCallStack.tos < currentCallStack.capacity)
+        return;
+
+    UInt newcap = 2 * currentCallStack.capacity;
+    currentCallStack.entry = VG_(realloc)("gn.callstack.resize.1",
+                                          currentCallStack.entry,
+                                          newcap * sizeof(CallEntry));
+    currentCallStack.capacity = newcap;
+}
+
+
+typedef struct _GnCallstackJumpKind {
+    /* package extra data depending on jump kind */
+    GnJumpKind jk;
+    Bool wasConditionalJmp;
+    union {
+        Int popCountOnReturn;
+        /* if jk_Return, what is the minimum number of pops to the callstack? */
+
+        Bool callEmulation;
+        /* if jk_Call, is this actual call or faked */
+    };
+} GnCallstackJumpKind;
+
+
+static Addr adjustSP(GnCallstackJumpKind cjk, Addr sp)
+{
+    Int cstop = currentCallStack.tos;
+    Addr newsp = sp;
+
+    if (cjk.jk == jk_Return) {
+        GN_ASSERT(cjk.popCountOnReturn == 1);
+    }
+    else {
+        if (cjk.jk == jk_Call &&
+            cjk.callEmulation == True &&
+            cstop > 0) {
+
+            /* If there wasn't really a CALL,
+             * then use the SP from last actual CALL
+             * This is useful for stack unwinding */
+            newsp = (currentCallStack.entry[cstop-1]).sp;
+        }
+    }
+
+    return newsp;
+}
+
+
+static GnCallstackJumpKind determineJumpKind(BBInfo *bb, Addr sp)
+{
+    GN_DEBUG(6, "+ determineJumpKind\n");
+
+    GnJumpKind jmpkind;
+    Bool wasConditionalJmp;
+    Bool callEmulation = False;
+
+    GN_DEBUGIF(4) {
+        GN_(printBBno)();
+        GN_(printTabs)(1);
+        VG_(printf)("SP: 0x%lx\n", sp);
+    }
+
+    if (isFirstBB()) {
+        jmpkind = jk_None;
+        wasConditionalJmp = False;
+    }
+    else {
+        GN_DEBUGIF(4) {
+            if (GN_(lastBB)->jmpCount < GN_(lastJmpsPassed)) {
+                GN_(printBBno)();
+                VG_(printf)("%d %d\n", GN_(lastBB)->jmpCount, GN_(lastJmpsPassed));
+            }
+        }
+
+        GN_ASSERT(GN_(lastBB)->jmpCount >= GN_(lastJmpsPassed));
+
+        jmpkind = GN_(lastBB)->jmps[GN_(lastJmpsPassed)];
+        wasConditionalJmp = GN_(lastJmpsPassed) < (GN_(lastBB)->jmpCount-1);
+
+        GN_DEBUGIF(4) {
+            GN_(printBBno)();
+            BBInfo *lastbb = GN_(lastBB);
+            if (lastbb) {
+                HChar *from = lastbb->fn->name;
+                HChar *to = bb->fn->name;
+                UInt fromId = lastbb->uid;
+                UInt toId = bb->uid;
+                GN_(printTabs)(1);
+                VG_(printf)("JmpKind: %s; From BB/Fn: %d/%s to To BB/Fn: %d/%s\n",
+                            GN_(getJumpStr)(jmpkind, wasConditionalJmp),
+                            fromId, from,
+                            toId, to);
+            }
+        }
+    }
+
+    Int popCountOnReturn = 1;
+    Bool retWithoutCall = False;
+
+    Int cstop = currentCallStack.tos;
+    Int up_one = cstop-1;
+    CallEntry *topce = &(currentCallStack.entry[up_one]); // The call entry just returned from
+
+    if ((jmpkind == jk_Return) && (cstop > 0)) {
+        /* Check if there is a return that doesn't match up with the callstack,
+         * in which case it will be treated as a normal jump */
+
+        if (sp < topce->sp) {
+            /* If current sp < last call sp (sp still in frame),
+             * then this is not a 'real' return.
+             * MDL20180103 When does this happen? */
+            popCountOnReturn = 0;
+
+            GN_DEBUGIF(4) {
+                VG_(printf)("\tReturn detected but still in frame:"
+                            " Current SP - 0x%lx, Top SP - 0x%lx\n", sp, topce->sp);
+            }
+        }
+        else if (sp == topce->sp) {
+            /* This is a special case that Callgrind handles for PPC
+             * Gengrind doesn't support this yet
+             * Callgrind modifies popCountOnReturn here */
+            VG_(printf)("Error: unexpected stack pointer\n"
+                        "Is this a supported platform?");
+            GN_ASSERT(False);
+        }
+        else {
+            // Typical case by convention; no special handling
+        }
+
+        if (popCountOnReturn == 0) {
+            jmpkind = jk_Jump;
+            retWithoutCall = True;
+        }
+    }
+
+    if ((jmpkind != jk_Return) && (jmpkind != jk_Call) && !isFirstBB()) {
+        if (retWithoutCall ||
+            bb->isFnEntry ||
+            (GN_(lastBB)->sectKind != bb->sectKind) ||
+            (GN_(lastBB)->obj->number != bb->obj->number)) {
+            /* a JMP/Cont is converted to a CALL if:
+             * - the jump is to another ELF object or section
+             * - the jump is to the beginning of a function (tail recursion) */
+            jmpkind = jk_Call;
+            callEmulation = True;
+        }
+        else if (sp < topce->sp) {
+            /* Sometimes the SP will be pushed within the same function,
+             * and won't be popped to the same SP
+             */
+        }
+    }
+
+    GnCallstackJumpKind cjk;
+    cjk.jk = jmpkind;
+    cjk.wasConditionalJmp = wasConditionalJmp;
+    if (cjk.jk == jk_Call) {
+        cjk.callEmulation = callEmulation;
+    }
+    else {
+        cjk.popCountOnReturn = popCountOnReturn;
+    }
+
+    GN_DEBUG(6, "- determineJumpKind\n");
+
+    return cjk;
+}
+
+
+static void pushCallStack(BBInfo *from, UInt jmp,
+                          BBInfo *to, Addr sp)
+{
+    /* assume first BB is not called */
+    GN_ASSERT(from != NULL);
+
+    FnNode *to_fn = to->fn;
+
+    /* set entry in callstack */
+
+    CallEntry *callee = &currentCallStack.entry[currentCallStack.tos];
+    callee->sp = sp;
+
+    /* signal this function is skipped by NULLing jn */
+    JumpNode *jn;
+    if (to_fn->skip == True)
+        jn = NULL;
+    else
+        jn = GN_(getJumpNode)(from, jmp, to);
+    callee->jn = jn;
+
+    if (GN_(clo).start_collect_func != NULL &&
+            GN_(afterStartFunc) == False &&
+            VG_(strcmp)(to_fn->name, GN_(clo).start_collect_func) == 0) {
+        GN_(afterStartFunc) = True;
+        GN_(updateEventGeneration)();
+    }
+
+    if (to_fn->skip == False) {
+        GN_(flush_FnEnter)(to_fn->name);
+    }
+
+    currentCallStack.tos++;
+
+    if ((Long)currentCallStack.tos >= currentCallStack.capacity)
+        resizeCurrentCallStack();
+
+    GN_DEBUGIF(4) {
+        VG_(printf)("\tPUSH: JN - %p, SP - 0x%lx\n", jn, sp);
+    }
+}
+
+
+static void popCallStack(void)
+{
+    CallEntry *caller = &currentCallStack.entry[currentCallStack.tos-1];
+    JumpNode *jn = caller->jn;
+
+    if (jn != NULL) {
+        FnNode *to_fn = jn->to->fn;
+
+        if (GN_(clo).stop_collect_func != NULL &&
+                GN_(afterEndFunc) == False &&
+                VG_(strcmp)(to_fn->name, GN_(clo).stop_collect_func) == 0) {
+            GN_(afterEndFunc) = True;
+            GN_(updateEventGeneration)();
+        }
+
+        GN_(flush_FnExit)(to_fn->name);
+    }
+
+    currentCallStack.tos--;
+
+    GN_DEBUGIF(4) {
+        VG_(printf)("\tPOP: %p\n", jn);
+    }
+}
+
+
+static Int unwindCurrentCallStack(Addr sp, Int minpops)
+{
+    GN_DEBUG(6, "+ unwindCurrentCallStack (sp = 0x%lx, minpops = %d)\n", sp, minpops);
+    /* Assume general (x86?) calling conventions
+     * After CALL: SP = p
+     * After RETURN: SP = s
+     * Generally, s>p, because the RETURN addr is always additionally popped by
+     * the callee */
+
+    Int unwinds = 0;
+    Int cstop = currentCallStack.tos;
+    while (cstop > 0) {
+        CallEntry *ce = &(currentCallStack.entry[cstop-1]);
+        if (ce->sp < sp ||
+            (ce->sp == sp && minpops>0)) {
+            /* ML: when does ce->sp == sp ? Some cases exist in amd64 */
+
+            GN_DEBUGIF(4) {
+                GN_(printTabs)(1);
+                VG_(printf)("Popping: CurrentSP - 0x%lx; TopSP - 0x%lx\n",
+                            sp, ce->sp);
+            }
+
+            popCallStack();
+            cstop = currentCallStack.tos;
+            unwinds++;
+            minpops--;
+            continue;
+        }
+        break;
+    }
+
+    GN_DEBUG(6, "- unwindCurrentCallStack\n");
+
+    return unwinds;
+}
+
+//-------------------------------------------------------------------------------------------------
+/** External function definitions **/
+
+void GN_(trackFns)(BBInfo *bb)
+{
+    GN_DEBUG(6, "+ trackFns\n");
+
+    /* Check how this BB was entered (call/return/jmp/et al from prevous BB)
+     * and manually track the callstack.
+     * This is a more general and robust method than tracking CALL/RETURN
+     * instrs, at the cost of additional overhead */
+
+    Addr sp = VG_(get_SP)(VG_(get_running_tid)());
+    GnCallstackJumpKind cjk = determineJumpKind(bb, sp);
+    sp = adjustSP(cjk, sp);
+
+    if (cjk.jk == jk_Call) {
+        pushCallStack(GN_(lastBB), GN_(lastJmpsPassed), bb, sp);
+    }
+    else if (cjk.jk == jk_Return) {
+        unwindCurrentCallStack(sp, cjk.popCountOnReturn);
+    }
+    else {
+        /* Callgrind does an unwind, anyway, to check if there was a return.
+         * jk_Return seems to be only used if the last exit of a BB is a return,
+         * and is never seen on conditional jumps earlier in the BB. */
+        Int unwinds = unwindCurrentCallStack(sp, 0);
+        if (unwinds > 0)
+            cjk.jk = jk_Return;
+    }
+
+    GN_DEBUGIF(4) {
+        if (GN_(lastBB)) {
+            Int jmpkind = GN_(lastBB)->jmps[GN_(lastJmpsPassed)];
+            if (jmpkind != cjk.jk) {
+                GN_(printTabs)(1);
+                VG_(printf)("JumpKind adjusted to: %s\n", GN_(getJumpStr)(cjk.jk,
+                                                                          cjk.wasConditionalJmp));
+            }
+            Addr origSP = VG_(get_SP)(VG_(get_running_tid)());
+            if (origSP != sp) {
+                GN_(printTabs)(1);
+                VG_(printf)("SP adjusted to: 0x%lx\n", sp);
+            }
+        }
+    }
+
+    /* Setup state for next BB */
+    /* TODO(someday) Clean up and add this in another dirty call at the end of the BB */
+    GN_(lastBB) = bb;
+    GN_(lastJmpsPassed) = 0;
+
+    GN_(bbExecutions)++;
+}
+
+
+void GN_(initCallStack)()
+{
+    initCallStack(&currentCallStack);
+    GN_(afterStartFunc) = True;
+    GN_(afterEndFunc) = False;
+}
+
+
+void finishCallstack()
+{
+    while (currentCallStack.tos > 0)
+        popCallStack();
+}
diff --git a/gengrind/gn_callstack.h b/gengrind/gn_callstack.h
new file mode 100644
index 000000000..77cda3067
--- /dev/null
+++ b/gengrind/gn_callstack.h
@@ -0,0 +1,86 @@
+#ifndef GN_CALLSTACK_H
+#define GN_CALLSTACK_H
+
+/* Manually track callstacks for function entry/exit event generation
+ *
+ * Callgrind's callstack tracking implementation is used as a base for
+ * this implementation
+ *
+ * In general, tracking function contexts is demanding for a number
+ * of reasons including:
+ * - availability of debug info
+ * - varying calling conventions 
+ * - frequency of calls
+ * - shared library mapping
+ * - object loading
+ * - IR manipulations
+ */
+
+#include "gn.h"
+
+extern UInt GN_(lastJmpsPassed);
+extern Bool GN_(afterStartFunc);
+extern Bool GN_(afterEndFunc);
+
+//-------------------------------------------------------------------------------------------------
+/** Callstack-tracking type definitions **/
+
+struct _CallEntry {
+    /* An entry in the callstack */
+
+    JumpNode* jn;
+    /* JumpNode for this call */
+
+    Addr sp;
+    /* stack pointer directly after this call (at the beginning of the call) */
+
+    Addr ret_addr;
+    /* ML: not implemented, see Callgrind if needed
+     *
+     * Address to which to return to
+     * Is 0 on a simulated call
+     *
+     * Return addr only useful for REAL CALL
+     * Used to detect RETURN w/o CALL */
+
+    Int fn_sp;
+    /* unused, function stack index before call */
+};
+
+
+struct _CallStack {
+    /* The full callstack */
+
+    Int tos;
+    // top-of-stack/idx into call entries
+
+    UInt capacity;
+    CallEntry* entry;
+};
+
+
+struct _ThreadInfo {
+    /* Thread State
+     *
+     * This structure stores thread specific info while a thread is *not* running.
+     */
+
+    /* state */
+    CallStack calls;   /* context call arc stack */
+//  ExecStack states;  /* execution states interrupted by signals */
+//
+//  /* thread specific data structure containers */
+//  fn_array fn_active;
+//  jcc_hash jccs;
+//  bbcc_hash bbccs;
+};
+
+
+//-------------------------------------------------------------------------------------------------
+/** Callstack-tracking function declarations **/
+
+void GN_(trackFns)(BBInfo *thisBB);
+void GN_(initCallStack)(void);
+void finishCallstack(void);
+
+#endif
diff --git a/gengrind/gn_clo.c b/gengrind/gn_clo.c
new file mode 100644
index 000000000..e0763f3cc
--- /dev/null
+++ b/gengrind/gn_clo.c
@@ -0,0 +1,48 @@
+#include "gn_clo.h"
+
+GN_(CommandLineOptions) GN_(clo);
+
+void GN_(setCloDefaults)(void)
+{
+    GN_(clo).enable_instrumentation = True;
+    GN_(clo).standalone_test        = False;
+    GN_(clo).ipc_dir                = NULL;
+    GN_(clo).collect_func           = NULL;
+    GN_(clo).start_collect_func     = NULL;
+    GN_(clo).stop_collect_func      = NULL;
+    GN_(clo).gen_mem                = False;
+    GN_(clo).gen_comp               = False;
+    GN_(clo).gen_cf                 = False;
+    GN_(clo).gen_sync               = False;
+    GN_(clo).gen_instr              = False;
+    GN_(clo).gen_bb                 = False;
+    GN_(clo).gen_fn                 = False;
+    GN_(clo).gen_thr                = False;
+    GN_(clo).skip_plt               = True;
+    GN_(clo).bbinfo_needed          = False;
+#if GN_ENABLE_DEBUG
+    GN_(clo).verbose                = 0;
+#endif
+}
+
+Bool GN_(processCmdLineOption)(const HChar* arg)
+{
+    if      VG_STR_CLO(arg,  "--ipc-dir",    GN_(clo).ipc_dir) {}
+    else if VG_STR_CLO(arg,  "--at-func",    GN_(clo).collect_func) {}
+    else if VG_STR_CLO(arg,  "--start-func", GN_(clo).start_collect_func) {}
+    else if VG_STR_CLO(arg,  "--stop-func",  GN_(clo).stop_collect_func) {}
+    else if VG_BOOL_CLO(arg, "--gen-mem",    GN_(clo).gen_mem) {}
+    else if VG_BOOL_CLO(arg, "--gen-comp",   GN_(clo).gen_comp) {}
+    else if VG_BOOL_CLO(arg, "--gen-sync",   GN_(clo).gen_sync) {}
+    else if VG_BOOL_CLO(arg, "--gen-instr",  GN_(clo).gen_instr) {}
+    else if VG_BOOL_CLO(arg, "--gen-fn",     GN_(clo).gen_fn) {}
+    else if VG_BOOL_CLO(arg, "--gen-cf",     GN_(clo).gen_cf) {}
+    else if VG_BOOL_CLO(arg, "--gen-bb",     GN_(clo).gen_bb) {}
+    else if VG_BOOL_CLO(arg, "--enable",     GN_(clo).enable_instrumentation) {}
+    else if VG_BOOL_CLO(arg, "--test",       GN_(clo).standalone_test) {}
+#if GN_ENABLE_DEBUG
+    else if VG_INT_CLO(arg, "--verbose",     GN_(clo).verbose) {}
+#endif
+
+    return True;
+}
diff --git a/gengrind/gn_clo.h b/gengrind/gn_clo.h
new file mode 100644
index 000000000..1e565e5c0
--- /dev/null
+++ b/gengrind/gn_clo.h
@@ -0,0 +1,38 @@
+#ifndef GN_CLO_H
+#define GN_CLO_H
+
+#include "gn.h"
+
+typedef struct {
+  const HChar* ipc_dir;
+  const HChar* collect_func;
+  const HChar* start_collect_func;
+  const HChar* stop_collect_func;
+  Bool enable_instrumentation;
+  Bool standalone_test;
+  Bool gen_mem;
+  Bool gen_comp;
+  Bool gen_cf;
+  Bool gen_sync;
+  Bool gen_instr;
+  Bool gen_bb;
+  Bool gen_fn;
+  Bool gen_thr;
+
+  Bool skip_plt;
+
+  Bool bbinfo_needed;
+
+#if GN_ENABLE_DEBUG
+  Int verbose;
+#endif
+
+} GN_(CommandLineOptions);
+
+extern GN_(CommandLineOptions) GN_(clo);
+
+//-------------------------------------------------------------------------------------------------
+void GN_(setCloDefaults)(void);
+Bool GN_(processCmdLineOption)(const HChar* arg);
+
+#endif
diff --git a/gengrind/gn_crq.c b/gengrind/gn_crq.c
new file mode 100644
index 000000000..2e44d4a02
--- /dev/null
+++ b/gengrind/gn_crq.c
@@ -0,0 +1,150 @@
+#include "gn_crq.h"
+#include "gn_threads.h"
+#include "gn_events.h"
+#include "gn_debug.h"
+
+#define UNUSED_SYNC_DATA 0
+
+Bool GN_(handleClientRequest)(ThreadId tid, UWord *args, UWord *ret)
+{
+    if (!VG_IS_TOOL_USERREQ('G', 'G', args[0]))
+        return False;
+
+    switch(args[0]) 
+    {
+    case VG_USERREQ__TOGGLE_COLLECT:
+    case VG_USERREQ__START_INSTRUMENTATION:
+    case VG_USERREQ__STOP_INSTRUMENTATION:
+        /* unimplemented */
+        GN_ASSERT(False);
+        *ret = 0; // meaningless
+        break;
+
+    /*******************************************
+     * Synchronization API intercepts 
+     *******************************************/
+    case VG_USERREQ__GN_PTHREAD_CREATE_ENTER:
+        GN_(setInSyncCall)(tid);
+        break;
+    case VG_USERREQ__GN_PTHREAD_CREATE_LEAVE:
+        /* enable and log once the thread has been CREATED and waiting */
+        GN_(resetInSyncCall)(tid);
+        /* sync event generated in a separate Valgrind hook that
+         * captures raw thread creation */
+
+    case VG_USERREQ__GN_PTHREAD_JOIN_ENTER:
+        /* log when the thread join is ENTERED and disable */
+        if (GN_(EventGenerationEnabled))
+            GN_(flush_Sync)((UChar)SGLPRIM_SYNC_JOIN, (SyncID*)&args[1], 1);
+        GN_(setInSyncCall)(tid);
+        break;
+    case VG_USERREQ__GN_PTHREAD_JOIN_LEAVE:
+        GN_(resetInSyncCall)(tid);
+        break;
+
+    case VG_USERREQ__GN_GOMP_LOCK_ENTER:
+    case VG_USERREQ__GN_GOMP_SETLOCK_ENTER:
+    case VG_USERREQ__GN_GOMP_CRITSTART_ENTER:
+    case VG_USERREQ__GN_GOMP_CRITNAMESTART_ENTER:
+    case VG_USERREQ__GN_GOMP_ATOMICSTART_ENTER:
+    case VG_USERREQ__GN_PTHREAD_LOCK_ENTER:
+        GN_(setInSyncCall)(tid);
+        break;
+    case VG_USERREQ__GN_GOMP_SETLOCK_LEAVE:
+    case VG_USERREQ__GN_GOMP_LOCK_LEAVE:
+    case VG_USERREQ__GN_GOMP_CRITSTART_LEAVE:
+    case VG_USERREQ__GN_GOMP_CRITNAMESTART_LEAVE:
+    case VG_USERREQ__GN_GOMP_ATOMICSTART_LEAVE:
+    case VG_USERREQ__GN_PTHREAD_LOCK_LEAVE:
+        /* enable and log once the lock has been acquired */
+        GN_(resetInSyncCall)(tid);
+        if (GN_(EventGenerationEnabled))
+            GN_(flush_Sync)((UChar)SGLPRIM_SYNC_LOCK, (SyncID*)&args[1], 1);
+        break;
+
+    case VG_USERREQ__GN_GOMP_UNLOCK_ENTER:
+    case VG_USERREQ__GN_GOMP_UNSETLOCK_ENTER:
+    case VG_USERREQ__GN_GOMP_CRITEND_ENTER:
+    case VG_USERREQ__GN_GOMP_CRITNAMEEND_ENTER:
+    case VG_USERREQ__GN_GOMP_ATOMICEND_ENTER:
+    case VG_USERREQ__GN_PTHREAD_UNLOCK_ENTER:
+        GN_(setInSyncCall)(tid);
+        break;
+    case VG_USERREQ__GN_GOMP_UNLOCK_LEAVE:
+    case VG_USERREQ__GN_GOMP_UNSETLOCK_LEAVE:
+    case VG_USERREQ__GN_GOMP_CRITEND_LEAVE:
+    case VG_USERREQ__GN_GOMP_CRITNAMEEND_LEAVE:
+    case VG_USERREQ__GN_GOMP_ATOMICEND_LEAVE:
+    case VG_USERREQ__GN_PTHREAD_UNLOCK_LEAVE:
+        GN_(resetInSyncCall)(tid);
+        if (GN_(EventGenerationEnabled))
+            GN_(flush_Sync)((UChar)SGLPRIM_SYNC_UNLOCK, (SyncID*)&args[1], 1);
+        break;
+
+    case VG_USERREQ__GN_GOMP_BARRIER_ENTER:
+    case VG_USERREQ__GN_GOMP_TEAMBARRIERWAIT_ENTER:
+    case VG_USERREQ__GN_GOMP_TEAMBARRIERWAITFINAL_ENTER:
+    case VG_USERREQ__GN_PTHREAD_BARRIER_ENTER:
+        /* log once the barrier is ENTERED and waiting and disable */
+        if (GN_(EventGenerationEnabled))
+            GN_(flush_Sync)((UChar)SGLPRIM_SYNC_BARRIER, (SyncID*)&args[1], 1);
+        GN_(setInSyncCall)(tid);
+        break;
+    case VG_USERREQ__GN_GOMP_BARRIER_LEAVE:
+    case VG_USERREQ__GN_GOMP_TEAMBARRIERWAIT_LEAVE:
+    case VG_USERREQ__GN_GOMP_TEAMBARRIERWAITFINAL_LEAVE:
+    case VG_USERREQ__GN_PTHREAD_BARRIER_LEAVE:
+        GN_(resetInSyncCall)(tid);
+        break;
+
+    case VG_USERREQ__GN_PTHREAD_CONDWAIT_ENTER:
+        GN_(setInSyncCall)(tid);
+        break;
+    case VG_USERREQ__GN_PTHREAD_CONDWAIT_LEAVE:
+        GN_(resetInSyncCall)(tid);
+        if (GN_(EventGenerationEnabled))
+            GN_(flush_Sync)((UChar)SGLPRIM_SYNC_CONDWAIT, (SyncID*)&args[1], 2);
+        break;
+
+    case VG_USERREQ__GN_PTHREAD_CONDSIG_ENTER:
+        GN_(setInSyncCall)(tid);
+        break;
+    case VG_USERREQ__GN_PTHREAD_CONDSIG_LEAVE:
+        GN_(resetInSyncCall)(tid);
+        if (GN_(EventGenerationEnabled))
+            GN_(flush_Sync)((UChar)SGLPRIM_SYNC_CONDSIG, (SyncID*)&args[1], 1);
+        break;
+
+    case VG_USERREQ__GN_PTHREAD_CONDBROAD_ENTER:
+        GN_(setInSyncCall)(tid);
+        break;
+    case VG_USERREQ__GN_PTHREAD_CONDBROAD_LEAVE:
+        GN_(resetInSyncCall)(tid);
+        if (GN_(EventGenerationEnabled))
+            GN_(flush_Sync)((UChar)SGLPRIM_SYNC_CONDBROAD, (SyncID*)&args[1], 1);
+        break;
+
+    case VG_USERREQ__GN_PTHREAD_SPINLOCK_ENTER:
+        GN_(setInSyncCall)(tid);
+        break;
+    case VG_USERREQ__GN_PTHREAD_SPINLOCK_LEAVE:
+        GN_(resetInSyncCall)(tid);
+        if (GN_(EventGenerationEnabled))
+            GN_(flush_Sync)((UChar)SGLPRIM_SYNC_SPINLOCK, (SyncID*)&args[1], 1);
+        break;
+
+    case VG_USERREQ__GN_PTHREAD_SPINUNLOCK_ENTER:
+        GN_(setInSyncCall)(tid);
+        break;
+    case VG_USERREQ__GN_PTHREAD_SPINUNLOCK_LEAVE:
+        GN_(resetInSyncCall)(tid);
+        if (GN_(EventGenerationEnabled))
+            GN_(flush_Sync)((UChar)SGLPRIM_SYNC_SPINUNLOCK, (SyncID*)&args[1], 1);
+        break;
+
+    default:
+        return False;
+    }
+
+    return True;
+}
diff --git a/gengrind/gn_crq.h b/gengrind/gn_crq.h
new file mode 100644
index 000000000..2cef337a2
--- /dev/null
+++ b/gengrind/gn_crq.h
@@ -0,0 +1,271 @@
+#ifndef GN_CRQ_H
+#define GN_CRQ_H
+
+#include "valgrind.h"
+#include "gn.h"
+
+Bool GN_(handleClientRequest)(ThreadId tid, UWord *args, UWord *ret);
+
+/* !! ABIWARNING !! ABIWARNING !! ABIWARNING !! ABIWARNING !!
+   This enum comprises an ABI exported by Valgrind to programs
+   which use client requests.  DO NOT CHANGE THE ORDER OF THESE
+   ENTRIES, NOR DELETE ANY -- add new ones at the end.
+ */
+
+typedef
+   enum {
+      VG_USERREQ__TOGGLE_COLLECT = VG_USERREQ_TOOL_BASE('G','G'),
+      VG_USERREQ__START_INSTRUMENTATION,
+      VG_USERREQ__STOP_INSTRUMENTATION,
+
+      VG_USERREQ__GN_PTHREAD_CREATE_ENTER,
+      VG_USERREQ__GN_PTHREAD_CREATE_LEAVE,
+      VG_USERREQ__GN_PTHREAD_JOIN_ENTER,
+      VG_USERREQ__GN_PTHREAD_JOIN_LEAVE,
+      VG_USERREQ__GN_PTHREAD_LOCK_ENTER,
+      VG_USERREQ__GN_PTHREAD_LOCK_LEAVE,
+      VG_USERREQ__GN_PTHREAD_UNLOCK_ENTER,
+      VG_USERREQ__GN_PTHREAD_UNLOCK_LEAVE,
+      VG_USERREQ__GN_PTHREAD_BARRIER_ENTER,
+      VG_USERREQ__GN_PTHREAD_BARRIER_LEAVE,
+      VG_USERREQ__GN_PTHREAD_CONDWAIT_ENTER,
+      VG_USERREQ__GN_PTHREAD_CONDWAIT_LEAVE,
+      VG_USERREQ__GN_PTHREAD_CONDSIG_ENTER,
+      VG_USERREQ__GN_PTHREAD_CONDSIG_LEAVE,
+      VG_USERREQ__GN_PTHREAD_CONDBROAD_ENTER,
+      VG_USERREQ__GN_PTHREAD_CONDBROAD_LEAVE,
+      VG_USERREQ__GN_PTHREAD_SPINLOCK_ENTER,
+      VG_USERREQ__GN_PTHREAD_SPINLOCK_LEAVE,
+      VG_USERREQ__GN_PTHREAD_SPINUNLOCK_ENTER,
+      VG_USERREQ__GN_PTHREAD_SPINUNLOCK_LEAVE,
+
+      VG_USERREQ__GN_GOMP_LOCK_ENTER,
+      VG_USERREQ__GN_GOMP_LOCK_LEAVE,
+      VG_USERREQ__GN_GOMP_UNLOCK_ENTER,
+      VG_USERREQ__GN_GOMP_UNLOCK_LEAVE,
+      VG_USERREQ__GN_GOMP_BARRIER_ENTER,
+      VG_USERREQ__GN_GOMP_BARRIER_LEAVE,
+      VG_USERREQ__GN_GOMP_ATOMICSTART_ENTER,
+      VG_USERREQ__GN_GOMP_ATOMICSTART_LEAVE,
+      VG_USERREQ__GN_GOMP_ATOMICEND_ENTER,
+      VG_USERREQ__GN_GOMP_ATOMICEND_LEAVE,
+      VG_USERREQ__GN_GOMP_CRITSTART_ENTER,
+      VG_USERREQ__GN_GOMP_CRITSTART_LEAVE,
+      VG_USERREQ__GN_GOMP_CRITEND_ENTER,
+      VG_USERREQ__GN_GOMP_CRITEND_LEAVE,
+      VG_USERREQ__GN_GOMP_CRITNAMESTART_ENTER,
+      VG_USERREQ__GN_GOMP_CRITNAMESTART_LEAVE,
+      VG_USERREQ__GN_GOMP_CRITNAMEEND_ENTER,
+      VG_USERREQ__GN_GOMP_CRITNAMEEND_LEAVE,
+      VG_USERREQ__GN_GOMP_SETLOCK_ENTER,
+      VG_USERREQ__GN_GOMP_SETLOCK_LEAVE,
+      VG_USERREQ__GN_GOMP_UNSETLOCK_ENTER,
+      VG_USERREQ__GN_GOMP_UNSETLOCK_LEAVE,
+      VG_USERREQ__GN_GOMP_TEAMBARRIERWAIT_ENTER,
+      VG_USERREQ__GN_GOMP_TEAMBARRIERWAIT_LEAVE,
+      VG_USERREQ__GN_GOMP_TEAMBARRIERWAITFINAL_ENTER,
+      VG_USERREQ__GN_GOMP_TEAMBARRIERWAITFINAL_LEAVE
+   } Vg_GengrindClientRequest;
+
+
+/* For future use */
+#define CALLGRIND_TOGGLE_COLLECT                                \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__TOGGLE_COLLECT,   \
+                                  0, 0, 0, 0, 0)
+/* For future use */
+#define CALLGRIND_START_INSTRUMENTATION                              \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__START_INSTRUMENTATION, \
+                                  0, 0, 0, 0, 0)
+
+/* For future use */
+#define CALLGRIND_STOP_INSTRUMENTATION                               \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__STOP_INSTRUMENTATION,  \
+                                  0, 0, 0, 0, 0)
+
+/*---------------------------------*/
+/*---  Synchronization capture  ---*/
+/*---------------------------------*/
+#define GN_PTHREAD_CREATE_ENTER(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_CREATE_ENTER,     \
+                                  thr, 0, 0, 0, 0)
+#define GN_PTHREAD_CREATE_LEAVE(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_CREATE_LEAVE,     \
+                                  thr, 0, 0, 0, 0)
+
+
+#define GN_PTHREAD_JOIN_ENTER(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_JOIN_ENTER,     \
+                                  thr, 0, 0, 0, 0)
+#define GN_PTHREAD_JOIN_LEAVE(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_JOIN_LEAVE,     \
+                                  thr, 0, 0, 0, 0)
+
+
+#define GN_PTHREAD_LOCK_ENTER(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_LOCK_ENTER,     \
+                                  mut, 0, 0, 0, 0)
+#define GN_PTHREAD_LOCK_LEAVE(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_LOCK_LEAVE,     \
+                                  mut, 0, 0, 0, 0)
+
+
+#define GN_PTHREAD_UNLOCK_ENTER(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_UNLOCK_ENTER,     \
+                                  mut, 0, 0, 0, 0)
+#define GN_PTHREAD_UNLOCK_LEAVE(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_UNLOCK_LEAVE,     \
+                                  mut, 0, 0, 0, 0)
+
+
+#define GN_PTHREAD_BARRIER_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_BARRIER_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define GN_PTHREAD_BARRIER_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_BARRIER_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+
+#define GN_PTHREAD_CONDWAIT_ENTER(cond, mtx) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_CONDWAIT_ENTER,     \
+                                  cond, mtx, 0, 0, 0)
+#define GN_PTHREAD_CONDWAIT_LEAVE(cond, mtx) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_CONDWAIT_LEAVE,     \
+                                  cond, mtx, 0, 0, 0)
+
+
+#define GN_PTHREAD_CONDSIG_ENTER(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_CONDSIG_ENTER,     \
+                                  cond, 0, 0, 0, 0)
+#define GN_PTHREAD_CONDSIG_LEAVE(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_CONDSIG_LEAVE,     \
+                                  cond, 0, 0, 0, 0)
+
+
+#define GN_PTHREAD_CONDBROAD_ENTER(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_CONDBROAD_ENTER,     \
+                                  cond, 0, 0, 0, 0)
+#define GN_PTHREAD_CONDBROAD_LEAVE(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_CONDBROAD_LEAVE,     \
+                                  cond, 0, 0, 0, 0)
+
+
+#define GN_PTHREAD_SPINLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_SPINLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define GN_PTHREAD_SPINLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_SPINLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define GN_PTHREAD_SPINUNLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_SPINUNLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define GN_PTHREAD_SPINUNLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTHREAD_SPINUNLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define GN_GOMP_LOCK_ENTER(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_LOCK_ENTER,     \
+                                  mutex, 0, 0, 0, 0)
+#define GN_GOMP_LOCK_LEAVE(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_LOCK_LEAVE,     \
+                                  mutex, 0, 0, 0, 0)
+
+
+#define GN_GOMP_UNLOCK_ENTER(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_UNLOCK_ENTER,     \
+                                  mutex, 0, 0, 0, 0)
+#define GN_GOMP_UNLOCK_LEAVE(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_UNLOCK_LEAVE,     \
+                                  mutex, 0, 0, 0, 0)
+
+
+#define GN_GOMP_BARRIER_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_BARRIER_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define GN_GOMP_BARRIER_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_BARRIER_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+
+#define GN_GOMP_ATOMICSTART_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_ATOMICSTART_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define GN_GOMP_ATOMICSTART_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_ATOMICSTART_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define GN_GOMP_ATOMICEND_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_ATOMICEND_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define GN_GOMP_ATOMICEND_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_ATOMICEND_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define GN_GOMP_CRITSTART_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_CRITSTART_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define GN_GOMP_CRITSTART_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_CRITSTART_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define GN_GOMP_CRITEND_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_CRITEND_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define GN_GOMP_CRITEND_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_CRITEND_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define GN_GOMP_CRITNAMESTART_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_CRITNAMESTART_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define GN_GOMP_CRITNAMESTART_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_CRITNAMESTART_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define GN_GOMP_CRITNAMEEND_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_CRITNAMEEND_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define GN_GOMP_CRITNAMEEND_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_CRITNAMEEND_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define GN_GOMP_SETLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_SETLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define GN_GOMP_SETLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_SETLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define GN_GOMP_UNSETLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_UNSETLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define GN_GOMP_UNSETLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_UNSETLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define GN_GOMP_TEAMBARRIERWAIT_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_TEAMBARRIERWAIT_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define GN_GOMP_TEAMBARRIERWAIT_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_TEAMBARRIERWAIT_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+
+#define GN_GOMP_TEAMBARRIERWAITFINAL_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_TEAMBARRIERWAITFINAL_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define GN_GOMP_TEAMBARRIERWAITFINAL_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_GOMP_TEAMBARRIERWAITFINAL_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+#endif
diff --git a/gengrind/gn_debug.c b/gengrind/gn_debug.c
new file mode 100644
index 000000000..9c5ca3df1
--- /dev/null
+++ b/gengrind/gn_debug.c
@@ -0,0 +1,81 @@
+#include "gn_debug.h"
+#include "gn_bb.h"
+
+static ULong bbWritten = 0;
+
+void GN_(printBBno)(void)
+{
+    if (bbWritten != GN_(bbExecutions)) {
+        /* only write BB once, if multiple debug calls are made */
+        bbWritten = GN_(bbExecutions);
+        VG_(printf)("BB# %llu\n", GN_(bbExecutions));
+    }
+}
+
+
+void GN_(printTabs)(UInt tabs)
+{
+    for (UInt i=0; i<tabs; ++i)
+        VG_(printf)("\t");
+}
+
+
+void GN_(printBBInfo)(BBInfo *bb)
+{
+    VG_(printf)("BBInfo %p\n", bb);
+    GN_(printTabs)(1);
+    VG_(printf)("Total Events: %d\n", bb->eventsTotal);
+}
+
+
+void GN_(sprint_SyncType)(HChar *str, SyncType type)
+{
+    switch(type) {
+    case SGLPRIM_SYNC_UNDEF:
+        VG_(sprintf)(str, "Undefined");
+        break;
+    case SGLPRIM_SYNC_CREATE:
+        VG_(sprintf)(str, "Spawn");
+        break;
+    case SGLPRIM_SYNC_EXIT:
+        VG_(sprintf)(str, "Exit");
+        break;
+    case SGLPRIM_SYNC_JOIN:
+        VG_(sprintf)(str, "Join");
+        break;
+    case SGLPRIM_SYNC_BARRIER:
+        VG_(sprintf)(str, "BarrieR");
+        break;
+    case SGLPRIM_SYNC_SYNC:
+        VG_(sprintf)(str, "Sync (generalized)");
+        break;
+    case SGLPRIM_SYNC_SWAP:
+        VG_(sprintf)(str, "Swap");
+        break;
+    case SGLPRIM_SYNC_LOCK:
+        VG_(sprintf)(str, "Lock");
+        break;
+    case SGLPRIM_SYNC_UNLOCK:
+        VG_(sprintf)(str, "Unlock");
+        break;
+    case SGLPRIM_SYNC_CONDWAIT:
+        VG_(sprintf)(str, "Conditional Wait");
+        break;
+    case SGLPRIM_SYNC_CONDSIG:
+        VG_(sprintf)(str, "Conditional Signal");
+        break;
+    case SGLPRIM_SYNC_CONDBROAD:
+        VG_(sprintf)(str, "Conditional Broadcast");
+        break;
+    case SGLPRIM_SYNC_SPINLOCK:
+        VG_(sprintf)(str, "SpinLock");
+        break;
+    case SGLPRIM_SYNC_SPINUNLOCK:
+        VG_(sprintf)(str, "SpinUnlock");
+        break;
+    default:
+        VG_(sprintf)(str, "INVALID");
+        tl_assert(0);
+        break;
+    }
+}
diff --git a/gengrind/gn_debug.h b/gengrind/gn_debug.h
new file mode 100644
index 000000000..820de608e
--- /dev/null
+++ b/gengrind/gn_debug.h
@@ -0,0 +1,39 @@
+#ifndef GN_DEBUG_H
+#define GN_DEBUG_H
+
+#include "gn.h"
+#include "gn_clo.h"
+
+void GN_(printTabs)(UInt tabs);
+void GN_(printBBno)(void);
+void GN_(sprint_SyncType)(HChar *str, SyncType type);
+
+//-------------------------------------------------------------------------------------------------
+/** Debug macros from Callgrind **/
+
+#if GN_ENABLE_DEBUG
+
+#define GN_DEBUGIF(x) \
+  if (UNLIKELY( (GN_(clo).verbose > x)))
+
+#define GN_DEBUG(x,format,args...)   \
+    GN_DEBUGIF(x) {                  \
+      GN_(printBBno)();	      \
+      VG_(printf)(format,##args);     \
+    }
+
+#define GN_ASSERT(cond)              \
+    if (UNLIKELY(!(cond))) {          \
+      GN_(printBBno)();	      \
+      tl_assert(cond);                \
+     }
+
+#else
+#define GN_DEBUGIF(x) if (0)
+#define GN_DEBUG(x...) {}
+#define GN_ASSERT(cond) tl_assert(cond);
+#endif
+
+
+
+#endif
diff --git a/gengrind/gn_events.c b/gengrind/gn_events.c
new file mode 100644
index 000000000..422d965e3
--- /dev/null
+++ b/gengrind/gn_events.c
@@ -0,0 +1,936 @@
+#include "gn.h"
+#include "gn_events.h"
+#include "gn_ipc.h"
+#include "gn_clo.h"
+#include "gn_callstack.h"
+#include "gn_threads.h"
+#include "gn_bb.h"
+#include "gn_debug.h"
+
+#define UNUSED_SYNC_DATA 0
+#define MAX_SYNC_DATA 2
+
+//-------------------------------------------------------------------------------------------------
+/* Events are added to a temporary buffer at instrumentation-time,
+ * in the order they are seen in the basic block.
+ * When an 'exit' IR stmt is found, all added events up to that point
+ * are flushed(instrumented) into the basic block, before the 'exit'.
+ */
+
+Bool GN_(EventGenerationEnabled);
+
+//-------------------------------------------------------------------------------------------------
+/** Global BB event tracking definitions **/
+GN_(EvVariant) GN_(EvBuffer)[GN_MAX_EVENTS_PER_BB];
+GnJumpKind GN_(EvJumps)[GN_MAX_JUMPS_PER_BB];
+
+
+//-------------------------------------------------------------------------------------------------
+/** Events added at instrumentation-time **/
+
+void GN_(add_TrackSyncs)(BBState *bbState)
+{
+    /* This checks if the current running thread was just swapped.
+     * Other sync events, like thread spawn/join/lock etc, are detected
+     * via thread library API wrapping, via Valgrind's client request mechanism */
+
+    IRExpr **argv = mkIRExprVec_0();
+    IRDirty *di = unsafeIRDirty_0_N(0, "checkSwitchThread",
+                                    VG_(fnptr_to_fnentry)(GN_(checkSwitchThread)), argv);
+    addStmtToIRSB(bbState->nbb, IRStmt_Dirty(di));
+}
+
+
+void GN_(add_TrackFns)(BBState *bbState)
+{
+    /* Track any change in the state of the running program that is checked
+     * at the beginning of a BB, e.g. a thread context change, function entry,
+     * et al
+     *
+     * This is expected to be instrumented at the beginning of the BB */
+
+    IRExpr **argv = mkIRExprVec_1(mkIRExpr_HWord((HWord)bbState->bbInfo));
+    IRDirty *di = unsafeIRDirty_0_N(1, "trackFns",
+                                    VG_(fnptr_to_fnentry)(GN_(trackFns)), argv);
+    addStmtToIRSB(bbState->nbb, IRStmt_Dirty(di));
+}
+
+
+void GN_(addEvent_Instr)(BBState *bbState, const IRStmt *st)
+{
+    GN_DEBUG(6, "+ addEvent_Instr\n");
+
+    GN_ASSERT(st->tag == Ist_IMark);
+    if (GN_(clo).gen_instr == False)
+        return;
+
+    Addr   cia   = st->Ist.IMark.addr + st->Ist.IMark.delta;
+    UInt   isize = st->Ist.IMark.len;
+
+    // If Vex fails to decode an instruction, the size will be zero.
+    // Pretend otherwise.
+    if (isize == 0) isize = VG_MIN_INSTR_SZB;
+
+    // Sanity-check size.
+    tl_assert( (VG_MIN_INSTR_SZB <= isize && isize <= VG_MAX_INSTR_SZB)
+               || VG_CLREQ_SZB == isize );
+
+    GN_ASSERT(bbState->eventsToFlush < GN_MAX_EVENTS_PER_BB);
+    GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+    ev->tag = GN_INSTR_EV;
+    ev->instr.type = SGLPRIM_CXT_INSTR;
+    ev->instr.id = cia;
+
+    bbState->eventsToFlush++;
+
+    GN_DEBUGIF(4) {
+        GN_(printTabs)(1);
+        VG_(printf)("Added Event: Instr - ");
+        VG_(printf)("Addr: %lu", cia);
+        VG_(printf)("\n");
+    }
+
+    GN_DEBUG(6, "- addEvent_Instr\n");
+}
+
+
+void GN_(addEvent_Compute)(BBState *bbState, const IRStmt *st)
+{
+    GN_DEBUG(6, "+ addEvent_Compute\n");
+
+    GN_ASSERT(st->tag == Ist_WrTmp);
+    if (GN_(clo).gen_comp == False)
+        return;
+
+    GN_ASSERT(bbState->eventsToFlush < GN_MAX_EVENTS_PER_BB);
+    GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+
+    ev->tag = GN_COMPUTE_EV;
+    ev->comp.type = SGLPRIM_COMP_TYPE_UNDEF;
+    ev->comp.arity = SGLPRIM_COMP_ARITY_UNDEF;
+    ev->comp.op = SGLPRIM_COMP_OP_UNDEF;
+    ev->comp.size = 0;
+
+    IRExpr* data = st->Ist.WrTmp.data;
+    IRType type = typeOfIRExpr(bbState->nbb->tyenv, data);
+    IRExprTag arity = data->tag;
+
+    if (type > Ity_INVALID) {
+        if (type < Ity_F16) {
+            ev->comp.type = SGLPRIM_COMP_IOP;
+        }
+        else if (type < Ity_V128) {
+            ev->comp.type = SGLPRIM_COMP_FLOP;
+        }
+        else {
+            //VG_(umsg)("Unsupported SIMD event encountered\n");
+            //ppIRStmt(bbState->st); VG_(printf)("\n");
+            /* SIMD */
+        }
+    }
+
+    switch (arity) {
+    case Iex_Unop:
+        ev->comp.arity = SGLPRIM_COMP_UNARY;
+        break;
+    case Iex_Binop:
+        ev->comp.arity = SGLPRIM_COMP_BINARY;
+        break;
+    case Iex_Triop:
+        ev->comp.arity = SGLPRIM_COMP_TERNARY;
+        break;
+    case Iex_Qop:
+        ev->comp.arity = SGLPRIM_COMP_QUARTERNARY;
+        break;
+    default:
+        tl_assert(0);
+        break;
+    }
+
+    /* TODO op mappings  */
+    /* TODO size mappings */
+
+    bbState->eventsToFlush++;
+
+    GN_DEBUGIF(4) {
+        GN_(printTabs)(1);
+        VG_(printf)("Added Event: Compute - ");
+        VG_(printf)("type: %d", ev->comp.type);
+        VG_(printf)("\n");
+    }
+
+    GN_DEBUG(6, "- addEvent_Compute\n");
+}
+
+
+static void gnSetEvent_Memory(GN_(EvVariant) *ev, Bool load, IRExpr *guard,
+                              IRExpr *aexpr, UInt size)
+{
+    ev->tag = GN_MEMORY_EV;
+    ev->mem.load = load;
+    ev->mem.guard = guard;
+    ev->mem.guarded = guard == NULL ? False : True;
+    ev->mem.aexpr = aexpr;
+    ev->mem.size = size;
+
+    GN_DEBUGIF(4) {
+        GN_(printTabs)(1);
+        VG_(printf)("Added Event: Memory - ");
+        ev->mem.load ? VG_(printf)("load, ") : VG_(printf)("store, ");
+        VG_(printf)("addr: %p", aexpr);
+        VG_(printf)("\n");
+    }
+}
+
+
+void GN_(addEvent_Memory_Load)(BBState *bbState, const IRStmt *st)
+{
+    GN_DEBUG(6, "+ addEvent_Memory_Load\n");
+
+    GN_ASSERT(st->tag == Ist_WrTmp);
+    if (GN_(clo).gen_mem == False)
+        return;
+
+    GN_ASSERT(bbState->eventsToFlush < GN_MAX_EVENTS_PER_BB);
+    GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+
+    IRExpr* data = st->Ist.WrTmp.data;
+    gnSetEvent_Memory(ev, True, NULL, data->Iex.Load.addr,
+                      sizeofIRType(data->Iex.Load.ty));
+    bbState->eventsToFlush++;
+
+    GN_DEBUG(6, "- addEvent_Memory_Load\n");
+}
+
+
+void GN_(addEvent_Memory_Store)(BBState *bbState, const IRStmt *st)
+{
+    GN_DEBUG(6, "+ addEvent_Memory_Store\n");
+
+    if (GN_(clo).gen_mem == False)
+        return;
+
+    GN_ASSERT(bbState->eventsToFlush < GN_MAX_EVENTS_PER_BB);
+    GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+
+    IRExpr* data = st->Ist.Store.data;
+    gnSetEvent_Memory(ev, False, NULL, st->Ist.Store.addr,
+                      sizeofIRType(typeOfIRExpr(bbState->nbb->tyenv, data)));
+    bbState->eventsToFlush++;
+
+    GN_DEBUG(6, "- addEvent_Memory_Store\n");
+}
+
+
+void GN_(addEvent_Memory_Guarded_Load)(BBState *bbState, const IRStmt *st)
+{
+    GN_DEBUG(6, "+ addEvent_Memory_Guarded_Load\n");
+
+    GN_ASSERT(st->tag == Ist_LoadG);
+    if (GN_(clo).gen_mem == False)
+        return;
+
+    IRLoadG *lg = st->Ist.LoadG.details;
+    IRType res, arg;
+    typeOfIRLoadGOp(lg->cvt, &res, &arg);
+
+    GN_ASSERT(bbState->eventsToFlush < GN_MAX_EVENTS_PER_BB);
+    GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+
+    // We'll only capture the addr if the load happens,
+    // otherwise ignore the memory event
+    gnSetEvent_Memory(ev, True, lg->guard, lg->addr,
+                      sizeofIRType(arg));
+    bbState->eventsToFlush++;
+
+    GN_DEBUG(6, "- addEvent_Memory_Guarded_Load\n");
+}
+
+
+void GN_(addEvent_Memory_Guarded_Store)(BBState *bbState, const IRStmt *st)
+{
+    GN_DEBUG(6, "+ addEvent_Memory_Guarded_Store\n");
+
+    GN_ASSERT(st->tag == Ist_StoreG);
+    if (GN_(clo).gen_mem == False)
+        return;
+
+    IRStoreG *sg = st->Ist.StoreG.details;
+
+    GN_ASSERT(bbState->eventsToFlush < GN_MAX_EVENTS_PER_BB);
+    GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+
+    gnSetEvent_Memory(ev, False, sg->guard, sg->addr,
+                      sizeofIRType(typeOfIRExpr(bbState->nbb->tyenv, sg->data)));
+    bbState->eventsToFlush++;
+
+    GN_DEBUG(6, "- addEvent_Memory_Guarded_Store\n");
+}
+
+
+void GN_(addEvent_Dirty)(BBState *bbState, const IRStmt *st)
+{
+    /* Only memory events are generated for a Dirty IR,
+     * since that's all the info VEX provides.
+     *
+     * Dirty IR stmts are rare and for special cases,
+     * so any compute events would most likely be insignificant */
+
+    GN_DEBUG(6, "+ addEvent_Dirty\n");
+
+    GN_ASSERT(st->tag == Ist_Dirty);
+    if (GN_(clo).gen_mem == False)
+        return;
+
+    IRDirty *di = st->Ist.Dirty.details;
+
+    if (di->guard != NULL) {
+        GN_ASSERT(isIRAtom(di->guard));
+
+        IRConst *t = IRConst_U1(True);
+        Bool isGuarded = (di->guard->tag != Iex_Const ||
+                          !eqIRConst(di->guard->Iex.Const.con, t));
+
+        if (isGuarded) {
+            /* if it's guarded by anything but 'Const(True)', then return early */
+            VG_(umsg)("Unsupported Guarded Dirty Call encountered\n");
+            ppIRStmt(st); VG_(printf)("\n");
+            ppIRExpr(di->guard); VG_(printf)("\n");
+            isGuarded = True;
+            return;
+        }
+    }
+
+    if (di->mFx != Ifx_None) {
+        tl_assert(di->mAddr != NULL);
+        tl_assert(di->mSize != 0);
+        UInt datasize = di->mSize;
+        if (di->mFx == Ifx_Read || di->mFx == Ifx_Modify) {
+            GN_ASSERT(bbState->eventsToFlush < GN_MAX_EVENTS_PER_BB);
+            GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+            gnSetEvent_Memory(ev, True, NULL, di->mAddr, datasize);
+            bbState->eventsToFlush++;
+        }
+        if (di->mFx == Ifx_Write || di->mFx == Ifx_Modify)
+            GN_ASSERT(bbState->eventsToFlush < GN_MAX_EVENTS_PER_BB);
+            GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+            gnSetEvent_Memory(ev, False, NULL, di->mAddr, datasize);
+            bbState->eventsToFlush++;
+    }
+    else {
+        tl_assert(di->mAddr == NULL);
+        tl_assert(di->mSize == 0);
+    }
+
+    GN_DEBUGIF(4) {
+        GN_(printTabs)(1);
+        VG_(printf)("Added Event: Dirty Call\n");
+    }
+
+    GN_DEBUG(6, "- addEvent_Dirty\n");
+}
+
+
+void GN_(addEvent_CAS)(BBState *bbState, const IRStmt *st)
+{
+    /* Follow Callgrind's analysis of this event
+     * as a read, followed by a write */
+
+    // TODO send a special event?
+
+    GN_DEBUG(6, "+ addEvent_CAS\n");
+
+    GN_ASSERT(st->tag == Ist_CAS);
+    if (GN_(clo).gen_mem == False)
+        return;
+
+    IRCAS *cas = st->Ist.CAS.details;
+    UInt datasize = sizeofIRType(typeOfIRExpr(bbState->nbb->tyenv, cas->dataLo));
+    if (cas->dataHi != NULL)
+        datasize *= 2; /* since this is a doubleword-cas */
+
+    GN_ASSERT(bbState->eventsToFlush < GN_MAX_EVENTS_PER_BB);
+    GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+
+    gnSetEvent_Memory(ev, True, NULL, cas->addr, datasize);
+    bbState->eventsToFlush++;
+
+    ev = GN_(EvBuffer) + bbState->eventsToFlush;
+    gnSetEvent_Memory(ev, False, NULL, cas->addr, datasize);
+
+    bbState->eventsToFlush++;
+
+    GN_DEBUGIF(4) {
+        GN_(printTabs)(1);
+        VG_(printf)("Added Event: CAS\n");
+    }
+
+    GN_DEBUG(6, "- addEvent_CAS\n");
+}
+
+
+void GN_(addEvent_LLSC)(BBState *bbState, const IRStmt *st)
+{
+    // TODO(someday) send a special event?
+    //
+    GN_DEBUG(6, "+ addEvent_LLSC\n");
+
+    GN_ASSERT(st->tag == Ist_LLSC);
+    if (GN_(clo).gen_mem == False)
+        return;
+
+    if (st->Ist.LLSC.storedata == NULL) {
+        /* LL */
+        UInt datasize =  sizeofIRType(typeOfIRTemp(bbState->nbb->tyenv, st->Ist.LLSC.result));
+        GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+        gnSetEvent_Memory(ev, True, NULL, st->Ist.LLSC.addr, datasize);
+        bbState->eventsToFlush++;
+    }
+    else {
+        /* SC */
+        UInt datasize =  sizeofIRType(typeOfIRExpr(bbState->nbb->tyenv, st->Ist.LLSC.storedata));
+        GN_(EvVariant) *ev = GN_(EvBuffer) + bbState->eventsToFlush;
+        gnSetEvent_Memory(ev, False, NULL, st->Ist.LLSC.addr, datasize);
+        bbState->eventsToFlush++;
+    }
+
+    GN_DEBUGIF(4) {
+        GN_(printTabs)(1);
+        VG_(printf)("Added Event: LL/SC\n");
+    }
+
+    GN_DEBUG(6, "- addEvent_LLSC\n");
+}
+
+
+static void gnAddExit(BBState *bbState, IRJumpKind jk)
+{
+    GnJumpKind jmp;
+    switch (jk) {
+        /* N.B. empirically it looks like Ijk_Call/Ret are rarely,
+         * if ever found, at least in amd64 */
+    case Ijk_Boring:
+        jmp = jk_Jump;
+        break;
+    case Ijk_Call:
+        jmp = jk_Call;
+        break;
+    case Ijk_Ret:
+        jmp = jk_Return;
+        break;
+    default:
+        /* other types may be e.g. client requests, signals, et al
+         * and are not recorded in the BB metadata */
+        jmp = jk_Other;
+        break;
+    }
+
+    /* Hold a running list of jumps for this BB (during instrumentation-time)
+     * in a temporary global buffer before assigning to the BB metadata at the
+     * end of the BB instrumentation */
+    GN_(EvJumps)[bbState->bbInfo->jmpCount++] = jmp;
+
+    GN_DEBUGIF(4) {
+        GN_(printTabs)(1);
+        VG_(printf)("Added Event: Exit - %s:%d\n",
+                    GN_(getJumpStr)(jmp, False), bbState->bbInfo->jmpCount);
+    }
+}
+
+
+void GN_(addEvent_Exit)(BBState *bbState, const IRStmt *st)
+{
+    GN_DEBUG(6, "+ addEvent_Exit\n");
+
+    GN_ASSERT(st->tag == Ist_Exit);
+
+    if (GN_(clo).gen_fn == False)
+        return;
+
+    gnAddExit(bbState, st->Ist.Exit.jk);
+
+    GN_DEBUG(6, "- addEvent_Exit\n");
+}
+
+
+static void addEvent_BBEnd_jmps(BBState *bbState)
+{
+    GN_ASSERT(GN_(clo).gen_fn == True);
+
+    gnAddExit(bbState, bbState->nbb->jumpkind);
+
+    /* all exits */
+    GN_ASSERT(bbState->bbInfo != NULL && bbState->bbInfo->jmps == NULL);
+    bbState->bbInfo->jmps = VG_(malloc)("gn.events.jmps.1",
+                                        bbState->bbInfo->jmpCount * sizeof(GnJumpKind));
+
+    /* update the BBInfo now that we know about all exits */
+    for (UInt i=0; i<bbState->bbInfo->jmpCount; ++i)
+        bbState->bbInfo->jmps[i] = GN_(EvJumps)[i];
+}
+
+
+void GN_(addEvent_BBEnd)(BBState *bbState)
+{
+    /* Update BB metadata with any outstanding state gathered
+     * during instrumentation phase */
+
+    if (GN_(clo).gen_fn == True)
+        addEvent_BBEnd_jmps(bbState);
+}
+
+
+//-------------------------------------------------------------------------------------------------
+/** Instrumentation for events **/
+
+static void gnReserveEventsInBuffer(IRSB *nbb, IRType tyW, UInt eventsToFlush)
+{
+    /* check if there's enough space in the event buffer */
+
+    /* tmp1 <- GN_(currEv) */
+    IRTemp currBufPtrTmp = newIRTemp(nbb->tyenv, tyW);
+    IRExpr *currBufPtr = mkIRExpr_HWord((HWord)&GN_(currEv));
+    addStmtToIRSB(nbb,
+                  IRStmt_WrTmp(currBufPtrTmp,
+                               IRExpr_Load(ENDNESS, tyW,
+                                           currBufPtr)));
+
+    /* tmp2 <- GN_(endEv) */
+    IRTemp endBufPtrTmp = newIRTemp(nbb->tyenv, tyW);
+    IRExpr *endBufPtr = mkIRExpr_HWord((HWord)&GN_(endEv));
+    addStmtToIRSB(nbb,
+                  IRStmt_WrTmp(endBufPtrTmp,
+                               IRExpr_Load(ENDNESS, tyW,
+                                           endBufPtr)));
+
+    /* tmp3 <- size of events */
+    UInt eventsBytes = eventsToFlush * sizeof(SglEvVariant);
+    IRTemp eventsTmp = newIRTemp(nbb->tyenv, tyW);
+    addStmtToIRSB(nbb,
+                  IRStmt_WrTmp(eventsTmp,
+                               IRExpr_Const(IRConst_U64(eventsBytes))));
+
+    /* tmp4 = tmp1 + tmp3 */
+    IRTemp neededTmp = newIRTemp(nbb->tyenv, tyW);
+    IRExpr *calcNeeded = IRExpr_Binop(IOP_ADD_PTR,
+                                      IRExpr_RdTmp(currBufPtrTmp),
+                                      IRExpr_RdTmp(eventsTmp));
+    addStmtToIRSB(nbb, IRStmt_WrTmp(neededTmp, calcNeeded));
+
+    /* tmp5 = tmp2 < tmp4 */
+    IRTemp cmpTmp = newIRTemp(nbb->tyenv, Ity_I1);
+    IRExpr *cmp = IRExpr_Binop(IOP_CMPLT_PTR,
+                               IRExpr_RdTmp(endBufPtrTmp),
+                               IRExpr_RdTmp(neededTmp));
+    addStmtToIRSB(nbb, IRStmt_WrTmp(cmpTmp, cmp));
+
+    /* request new buffer if we ran out of events */
+    IRDirty *di = emptyIRDirty();
+    di->cee = mkIRCallee(0, "ipc.setNextBuffer",
+                         VG_(fnptr_to_fnentry)(GN_(flushCurrAndSetNextBuffer)));
+    di->guard = IRExpr_RdTmp(cmpTmp);
+    di->args = mkIRExprVec_0();
+    di->tmp = IRTemp_INVALID;
+    di->nFxState = 0;
+    di->mFx = Ifx_None;
+    di->mAddr = NULL;
+    di->mSize = 0;
+    addStmtToIRSB(nbb, IRStmt_Dirty(di));
+}
+
+
+static IRExpr* gnIRConst(IRType type, UInt val)
+{
+    switch (type) {
+    case Ity_I8:
+        return IRExpr_Const(IRConst_U8(val));
+        break;
+    case Ity_I16:
+        return IRExpr_Const(IRConst_U16(val));
+        break;
+    case Ity_I32:
+        return IRExpr_Const(IRConst_U32(val));
+        break;
+    case Ity_I64:
+        return IRExpr_Const(IRConst_U64(val));
+        break;
+    default:
+        tl_assert(0);
+    }
+}
+
+
+static IRTemp incrSlot(IRSB *bb, IRTemp oldSlotTmp, IRExpr *slotSize, IRType tyW)
+{
+    /* increment event slot
+     * VEX IR is SSA, so we need to get a new temporary */
+    IRTemp slotTmp = newIRTemp(bb->tyenv, tyW);
+    addStmtToIRSB(bb,
+                  IRStmt_WrTmp(slotTmp,
+                               IRExpr_Binop(IOP_ADD_PTR,
+                                            IRExpr_RdTmp(oldSlotTmp), slotSize)));
+    return slotTmp;
+}
+
+
+#define GN_STORE_CONST_TO_OFFSET(bb, slot, val, type, member) {\
+        IRTemp tmp = newIRTemp(bb->tyenv, IRTYPE_PTR);\
+        IRExpr *offsetExpr = mkIRExpr_HWord((HWord)offsetof(type, member));\
+        addStmtToIRSB(bb,\
+                      IRStmt_WrTmp(tmp,\
+                                   IRExpr_Binop(IOP_ADD_PTR,\
+                                                IRExpr_RdTmp(slot), offsetExpr)));\
+        IRType itype = integerIRTypeOfSize(MEMBER_SIZE(type, member));\
+        addStmtToIRSB(bb,\
+                      IRStmt_Store(ENDNESS,\
+                                   IRExpr_RdTmp(tmp),\
+                                   gnIRConst(itype, val)));\
+    }
+
+#define GN_STORE_EXPR_TO_OFFSET(bb, slot, expr, type, member) {\
+        IRTemp tmp = newIRTemp(bb->tyenv, IRTYPE_PTR);\
+        IRExpr *offsetExpr = mkIRExpr_HWord((HWord)offsetof(type, member));\
+        addStmtToIRSB(bb,\
+                      IRStmt_WrTmp(tmp,\
+                                   IRExpr_Binop(IOP_ADD_PTR,\
+                                                IRExpr_RdTmp(slot), offsetExpr)));\
+        addStmtToIRSB(bb,\
+                      IRStmt_Store(ENDNESS,\
+                                   IRExpr_RdTmp(tmp),\
+                                   expr));\
+    }
+
+static IRTemp gnInstrumentEvent_Instr(IRSB *bb, GN_(InstrEvent) *ev,
+                                      IRTemp slot, IRExpr *slotSize, IRType tyW)
+{
+    /* slot.tag <- instr tag */
+    GN_STORE_CONST_TO_OFFSET(bb, slot, SGL_CXT_TAG, SglEvVariant, tag);
+
+    /* slot.cxt.type <- instr */
+    GN_STORE_CONST_TO_OFFSET(bb, slot, SGLPRIM_CXT_INSTR, SglEvVariant, cxt.type);
+
+    /* slot.cxt.id <- iaddr */
+    GN_STORE_CONST_TO_OFFSET(bb, slot, ev->id, SglEvVariant, cxt.id);
+
+    return incrSlot(bb, slot, slotSize, tyW);
+}
+
+
+static IRTemp gnInstrumentEvent_Compute(IRSB *bb, GN_(ComputeEvent) *ev,
+                                        IRTemp slot, IRExpr *slotSize, IRType tyW)
+{
+    /* slot.tag <- comp tag */
+    GN_STORE_CONST_TO_OFFSET(bb, slot, SGL_COMP_TAG, SglEvVariant, tag);
+
+    /* slot.comp.type <- iop/flop/simd */
+    GN_STORE_CONST_TO_OFFSET(bb, slot, ev->type, SglEvVariant, comp.type);
+
+    /* slot.comp.arity <- comp arity */
+    GN_STORE_CONST_TO_OFFSET(bb, slot, ev->arity, SglEvVariant, comp.arity);
+
+    return incrSlot(bb, slot, slotSize, tyW);
+}
+
+
+static IRTemp gnInstrumentEvent_Memory(IRSB *bb, GN_(MemoryEvent) *ev,
+                                       IRTemp slot, IRExpr *slotSize, IRType tyW)
+{
+    /* slot.tag <- mem tag */
+    GN_STORE_CONST_TO_OFFSET(bb, slot, SGL_MEM_TAG, SglEvVariant, tag);
+
+    /* slot.mem.type <- load/store */
+    GN_STORE_CONST_TO_OFFSET(bb, slot, ev->load ? SGLPRIM_MEM_LOAD : SGLPRIM_MEM_STORE,
+                             SglEvVariant, mem.type);
+
+    /* slot.mem.size <- access size */
+    GN_STORE_CONST_TO_OFFSET(bb, slot, ev->size, SglEvVariant, mem.size);
+
+    /* slot.mem.addr <- aexpr */
+    GN_STORE_EXPR_TO_OFFSET(bb, slot, ev->aexpr, SglEvVariant, mem.begin_addr);
+
+    IRTemp newSlot;
+
+    if (ev->guarded == True) {
+        /* only increment the slot if (guard == True) */
+        newSlot = newIRTemp(bb->tyenv, tyW);
+        IRExpr *ite = IRExpr_ITE(ev->guard,
+                                 IRExpr_Binop(IOP_ADD_PTR, IRExpr_RdTmp(slot), slotSize),
+                                 IRExpr_RdTmp(slot));
+        addStmtToIRSB(bb,
+                      IRStmt_WrTmp(newSlot, ite));
+    }
+    else {
+        newSlot = incrSlot(bb, slot, slotSize, tyW);
+    }
+
+    return newSlot;
+}
+
+
+static void gnInstrument_JmpsPassed(IRSB *bb, Int jmp)
+{
+    /* GN_(lastJmpsPassed) = jmp */
+    GN_ASSERT(sizeof(GN_(lastJmpsPassed)) == 4);
+    IRExpr *jmpExpr = IRExpr_Const(IRConst_U32(jmp));
+    IRExpr *addr = mkIRExpr_HWord((HWord)&GN_(lastJmpsPassed));
+    addStmtToIRSB(bb,
+                  IRStmt_Store(ENDNESS, addr, jmpExpr));
+}
+
+
+static void gnInstrument_skipIfEventGenDisabled(IRSB *bb, IRConst *dst, IRJumpKind ijk)
+{
+    if (!(GN_(clo).gen_sync == True ||
+          GN_(clo).start_collect_func != NULL ||
+          GN_(clo).stop_collect_func != NULL))
+        return;
+
+    GN_DEBUGIF(6) {
+        HChar jstr[16];
+        switch(ijk) {
+        case Ijk_Ret:
+            VG_(sprintf)(jstr, "Return");
+            break;
+        case Ijk_Call:
+            VG_(sprintf)(jstr, "Call");
+            break;
+        case Ijk_Boring:
+            VG_(sprintf)(jstr, "Boring");
+            break;
+        default:
+            VG_(sprintf)(jstr, "Other");
+        }
+        VG_(printf)("SkipEventGen: Inserting Conditional %s to 0x%llX\n", jstr, dst->Ico.U64);
+    }
+
+    /* if event generation is disabled, then jump to the exit instruction */
+
+    /* assumes that a Bool is 8 bits */
+    IRTemp enabledTmp = newIRTemp(bb->tyenv, Ity_I8);
+    IRExpr *enabledLoad = IRExpr_Load(ENDNESS, Ity_I8,
+                                mkIRExpr_HWord((HWord)&GN_(EventGenerationEnabled)));
+    IRTemp disabledTmp = newIRTemp(bb->tyenv, Ity_I1);
+
+    /* tmp1 <- EventGenerationEnabled */
+    addStmtToIRSB(bb,
+                  IRStmt_WrTmp(enabledTmp, enabledLoad));
+
+    /* tmp2 <- (tmp1 == 0) */
+    addStmtToIRSB(bb,
+                  IRStmt_WrTmp(disabledTmp,
+                               IRExpr_Binop(Iop_CmpEQ8,
+                                            IRExpr_RdTmp(enabledTmp),
+                                            IRExpr_Const(IRConst_U8(0)))));
+
+    /* if (tmp2) then jump to next instr */
+    addStmtToIRSB(bb,
+                  IRStmt_Exit(IRExpr_RdTmp(disabledTmp),
+                              ijk, dst, OFFB_RIP));
+}
+
+
+static void gnInstrument_EventCapture(IRSB *nbb, IRType tyW, UInt eventsToFlush)
+{
+    /* make sure there's enough space in the current buffer,
+     * or get a new buffer */
+    gnReserveEventsInBuffer(nbb, tyW, eventsToFlush);
+
+    /* Initialize the event slot temporary
+     * tmp <- current event slot */
+    IRTemp slotTmp = newIRTemp(nbb->tyenv, tyW);
+    IRExpr *slotPtr = mkIRExpr_HWord((HWord)&GN_(currEv));
+    addStmtToIRSB(nbb,
+                  IRStmt_WrTmp(slotTmp,
+                               IRExpr_Load(ENDNESS, tyW,
+                                           slotPtr)));
+
+    /* now we have enough event slots in the buffer,
+     * load in the buffer pointer and flush events to buffer */
+    IRExpr *slotSize = mkIRExpr_HWord((HWord)sizeof(SglEvVariant));
+    for (UInt i=0; i<eventsToFlush; ++i) {
+        /* fill in event specific attributes and get the next slot
+         * ML: We compute the next slot in each event instrumentation
+         * because the slot does not always increment by 1, such as
+         * in a conditional memory event */
+        switch(GN_(EvBuffer)[i].tag) {
+        case GN_INSTR_EV:
+            slotTmp = gnInstrumentEvent_Instr(nbb, &GN_(EvBuffer)[i].instr,
+                                              slotTmp, slotSize, tyW);
+            break;
+        case GN_COMPUTE_EV:
+            slotTmp = gnInstrumentEvent_Compute(nbb, &GN_(EvBuffer)[i].comp,
+                                                slotTmp, slotSize, tyW);
+            break;
+        case GN_MEMORY_EV:
+            slotTmp = gnInstrumentEvent_Memory(nbb, &GN_(EvBuffer)[i].mem,
+                                               slotTmp, slotSize, tyW);
+            break;
+        default:
+            tl_assert(0);
+            break;
+        }
+    }
+
+    /* store the new buffer length */
+    addStmtToIRSB(nbb,
+                  IRStmt_Store(ENDNESS,
+                               slotPtr, IRExpr_RdTmp(slotTmp)));
+
+    /* update the buffer used var */
+    GN_ASSERT(sizeof(*GN_(usedEv)) == sizeofIRType(tyW));
+    IRTemp usedPtrTmp = newIRTemp(nbb->tyenv, tyW);
+    IRExpr *usedPtr = mkIRExpr_HWord((HWord)&GN_(usedEv));
+    addStmtToIRSB(nbb,
+                  IRStmt_WrTmp(usedPtrTmp,
+                               IRExpr_Load(ENDNESS, tyW,
+                                           usedPtr)));
+    IRTemp usedTmp = newIRTemp(nbb->tyenv, tyW);
+    addStmtToIRSB(nbb,
+                  IRStmt_WrTmp(usedTmp,
+                               IRExpr_Load(ENDNESS, tyW,
+                                           IRExpr_RdTmp(usedPtrTmp))));
+    IRTemp newUsedTmp = newIRTemp(nbb->tyenv, tyW);
+    IRExpr *eventsAdded = mkIRExpr_HWord((HWord)eventsToFlush);
+    addStmtToIRSB(nbb,
+                  IRStmt_WrTmp(newUsedTmp,
+                               IRExpr_Binop(IOP_ADD_PTR,
+                                            IRExpr_RdTmp(usedTmp), eventsAdded)));
+    addStmtToIRSB(nbb,
+                  IRStmt_Store(ENDNESS,
+                               IRExpr_RdTmp(usedPtrTmp),
+                               IRExpr_RdTmp(newUsedTmp)));
+}
+
+
+void GN_(flushEvents)(BBState *bbState, Int prev_flush, GN_(Flush) flushType)
+{
+    GN_DEBUGIF(2) {
+        GN_(printTabs)(1);
+        VG_(printf)("Flushing Events\n");
+    }
+
+    IRSB *const nbb = bbState->nbb;
+    IRSB *const obb = bbState->obb;
+
+    Int flush_from = prev_flush + 1;
+    Int flush_to;
+    IRConst *skip_dst;
+    IRJumpKind skip_jk;
+
+    switch(flushType.tag) {
+    case GN_FLUSH_EXIT_ST:
+        flush_to = flushType.exit_imark_idx;
+        skip_dst = IRCONST_PTR(obb->stmts[flushType.exit_imark_idx]->Ist.IMark.addr);
+        skip_jk = Ijk_Boring;
+        break;
+    case GN_FLUSH_BB_END:
+        flush_to = obb->stmts_used;
+        GN_ASSERT(obb->next->tag == Iex_Const);
+        skip_dst = IRCONST_PTR(obb->next->Iex.Const.con->Ico.U64);
+
+        /* ideally we would use the jumpkind from the end of the basic block
+         * (obb->jumpkind), but VEX doesn't allow us to insert exits with
+         * Ijk_Call/Ret, so we unconditionally set it to Ijk_Boring and let our
+         * callstack handler determine whether it was a Call or Return */
+        skip_jk = Ijk_Boring;
+        break;
+    default:
+        tl_assert(0);
+    }
+
+    if (flush_from == flush_to) {
+        /* Should only happen when the last statement in the BB is an exit.
+         * In that case there is no need to flush again because there are no
+         * captured events between the last side-exit and the last unconditional
+         * exit */
+        return;
+    }
+
+    /* Perform the instrumentation for the event capture flush.
+     *
+     * - insert IR from obb[begin] to obb[end] into nbb
+     * - insert conditional jump over event-capture instrumentation to skip_to
+     * - insert event capture instrumentation
+     */
+
+    for (Int i = flush_from; i < flush_to; ++i)
+        addStmtToIRSB(nbb, obb->stmts[i]);
+    gnInstrument_skipIfEventGenDisabled(nbb, skip_dst, skip_jk);
+    gnInstrument_EventCapture(nbb, bbState->hWordTy, bbState->eventsToFlush);
+
+    /* add the exit after instrumentation */
+    if (flushType.tag == GN_FLUSH_EXIT_ST)
+        addStmtToIRSB(nbb, obb->stmts[flush_to]);
+
+    /* any extra client instrumentation
+     * (anything not being sent to the event analysis frontend) */
+    if (GN_(clo).gen_fn == True)
+        gnInstrument_JmpsPassed(nbb, bbState->jmpsPassed); // set which jump this flush is at
+    bbState->jmpsPassed++;
+
+    /* reset events for next flush */
+    if (GN_(clo).bbinfo_needed == True)
+        bbState->bbInfo->eventsTotal += bbState->eventsToFlush;
+    bbState->eventsToFlush = 0;
+}
+
+
+void GN_(flush_Sync)(SyncType type, SyncID *data, UInt args)
+{
+    /* Synchronization events are flushed immediately and not queued in a buffer.
+     * Synchronization events are not generated in BB instrumentation.
+     * Instead, they are  generated at either a context switch,
+     * or a thread API call/return. */
+
+    GN_ASSERT(GN_(currEv) < GN_(endEv));
+
+    /* add event */
+    GN_ASSERT(args <= MAX_SYNC_DATA && args > 0);
+    SglEvVariant *slot = GN_(currEv);
+    slot->tag = SGL_SYNC_TAG;
+    slot->sync.type = type;
+
+    UInt i=0;
+    for (; i<args; ++i)
+        slot->sync.data[i] = data[i];
+    for (; i<MAX_SYNC_DATA; ++i)
+        slot->sync.data[i] = UNUSED_SYNC_DATA;
+
+    /* increment event slot */
+    ++GN_(currEv);
+    ++*GN_(usedEv);
+    if (GN_(currEv) == GN_(endEv))
+        GN_(flushCurrAndSetNextBuffer)();
+
+    GN_DEBUGIF(6) {
+        HChar str[16];
+        GN_(sprint_SyncType)(str, type);
+        VG_(printf)("SyncEvent: %s; SyncData0: 0x%lu; SyncData1: 0x%lu\n",
+                    str, data[0], data[1]);
+    }
+}
+
+
+void GN_(flush_FnEnter)(const HChar *fnname)
+{
+    GN_DEBUG(6, "Fn Enter: %s\n", fnname);
+}
+
+
+void GN_(flush_FnExit)(const HChar *fnname)
+{
+    GN_DEBUG(6, "Fn Exit : %s\n", fnname);
+}
+
+
+void GN_(updateEventGeneration)(void)
+{
+    if (GN_(afterStartFunc) == True &&
+            GN_(afterEndFunc) == False &&
+            GN_(isInSyncCall)() == False) {
+        GN_(EventGenerationEnabled) = True;
+    }
+    else {
+        GN_(EventGenerationEnabled) = False;
+    }
+}
diff --git a/gengrind/gn_events.h b/gengrind/gn_events.h
new file mode 100644
index 000000000..8b5a2d39f
--- /dev/null
+++ b/gengrind/gn_events.h
@@ -0,0 +1,102 @@
+#ifndef GN_EVENTS_H
+#define GN_EVENTS_H
+
+#include "gn.h"
+#include "gn_callstack.h"
+#include "gn_jumps.h"
+
+//-------------------------------------------------------------------------------------------------
+/** Events infrastructure **/
+
+typedef struct GN_(_EvVariant) GN_(EvVariant);
+typedef struct GN_(_MemoryEvent) GN_(MemoryEvent);
+typedef SglCxtEv GN_(InstrEvent);
+typedef SglCompEv GN_(ComputeEvent);
+typedef GnJumpKind GN_(JKEvent);
+typedef struct GN_(Flush) GN_(Flush);
+
+
+enum GN_(EvTag) {
+    GN_UNDEF_EV,
+    GN_MEMORY_EV,
+    GN_COMPUTE_EV,
+    GN_INSTR_EV,
+    GN_JK_EV,
+};
+
+
+struct GN_(_MemoryEvent) {
+    IRExpr *aexpr;
+    IRExpr *guard;
+    /* TODO the alt LoadG value is ignored for now */
+    UInt size;
+    Bool guarded;
+    /* guarded == False implies guard == NULL, and vice-versa */
+    Bool load;
+};
+
+
+struct GN_(_EvVariant) {
+    enum GN_(EvTag) tag;
+    union {
+        GN_(MemoryEvent) mem;
+        GN_(ComputeEvent) comp;
+        GN_(InstrEvent) instr;
+        GN_(JKEvent) jk;
+    };
+};
+
+
+#define GN_MAX_EVENTS_PER_BB (1024)
+extern GN_(EvVariant) GN_(EvBuffer)[GN_MAX_EVENTS_PER_BB];
+/* A smaller buffer local to Valgrind
+ * Used to track per-bb events */
+
+#define GN_MAX_JUMPS_PER_BB (64)
+extern UInt GN_(EvJumpsInBB);
+extern GnJumpKind GN_(EvJumps)[GN_MAX_JUMPS_PER_BB];
+
+extern Bool GN_(EventGenerationEnabled);
+
+
+enum GN_(FlushTag) {
+    GN_FLUSH_EXIT_ST,
+    GN_FLUSH_BB_END,
+};
+
+struct GN_(Flush) {
+    enum GN_(FlushTag) tag;
+
+    Int exit_imark_idx;
+    Int exit_stmt_idx;
+    /* only valid if flush is on an Ist_Exit */
+};
+
+
+//-------------------------------------------------------------------------------------------------
+void GN_(add_TrackSyncs)(BBState *bbState);
+void GN_(flush_Sync)(SyncType type, SyncID *data, UInt args);
+
+void GN_(add_TrackFns)(BBState *bbState);
+void GN_(flush_FnExit)(const HChar *fnname);
+void GN_(flush_FnEnter)(const HChar *fnname);
+
+void GN_(addEvent_Instr)(BBState *bbState, const IRStmt *st);
+void GN_(addEvent_Compute)(BBState *bbState, const IRStmt *st);
+void GN_(addEvent_Memory_Load)(BBState *bbState, const IRStmt *st);
+void GN_(addEvent_Memory_Store)(BBState *bbState, const IRStmt *st);
+void GN_(addEvent_CAS)(BBState *bbState, const IRStmt *st);
+void GN_(addEvent_LLSC)(BBState *bbState, const IRStmt *st);
+void GN_(addEvent_Dirty)(BBState *bbState, const IRStmt *st);
+void GN_(addEvent_Exit)(BBState *bbState, const IRStmt *st);
+void GN_(addEvent_BBEnd)(BBState *bbState);
+
+void GN_(flushEvents)(BBState *bbState, Int flush_from, GN_(Flush) flushType);
+
+void GN_(updateEventGeneration)(void);
+
+void GN_(addEvent_Memory_Guarded_Load)(BBState *bbState, const IRStmt *st);
+void GN_(addEvent_Memory_Guarded_Store)(BBState *bbState, const IRStmt *st);
+/* unsupported events */
+
+#endif
diff --git a/gengrind/gn_fn.c b/gengrind/gn_fn.c
new file mode 100644
index 000000000..864a82794
--- /dev/null
+++ b/gengrind/gn_fn.c
@@ -0,0 +1,495 @@
+/* Taken from Callgrind */
+
+#include "gn_clo.h"
+#include "gn_fn.h"
+#include "gn_bb.h"
+#include "gn_debug.h"
+
+//-------------------------------------------------------------------------------------------------
+/** Static variable definitions **/
+static ULong uniqueObjs;
+static ULong uniqueFiles;
+static ULong uniqueFns;
+static ObjNode* allObjs[N_OBJ_ENTRIES];
+
+static const HChar *anonname = "???";
+static Addr runtimeResolveAddr = 0;
+static int runtimeResolveLength = 0;
+static const HChar *runtimeResolveName = "_dl_runtime_resolve";
+/* N.B. see searchRuntimeResolve() */
+
+
+struct chunk_t { int start, len; };
+struct pattern
+{
+	// a code pattern is a list of tuples (start offset, length)
+    const HChar* name;
+    int len;
+    struct chunk_t chunk[];
+};
+
+
+
+//-------------------------------------------------------------------------------------------------
+/** Helper function definitions **/
+
+
+static inline Addr bbAddr(BBInfo *bb) { return bb->offset + bb->obj->offset; }
+
+
+#define HASH_CONSTANT (256)
+static UInt strHash(const HChar *str, UInt tableSize)
+{
+    int hash = 0;
+    for (; *str; str++)
+        hash = (HASH_CONSTANT * hash + *str) % tableSize;
+    return hash;
+}
+
+
+__attribute__((unused))    // Possibly;  depends on the platform.
+static Bool checkCode(ObjNode* obj, UChar code[], struct pattern* pat)
+{
+	/* Scan for a pattern in the code of an ELF object.
+	 * If found, return true and set runtime_resolve_{addr,length} */
+	Bool found;
+	Addr addr, end;
+	int chunk, start, len;
+
+	/* first chunk of pattern should always start at offset 0 and
+	 * have at least 3 bytes */
+	GN_ASSERT((pat->chunk[0].start == 0) && (pat->chunk[0].len >2));
+
+	end = obj->start + obj->size - pat->len;
+	addr = obj->start;
+	while(addr < end) {
+		found = (VG_(memcmp)( (void*)addr, code, pat->chunk[0].len) == 0);
+
+		if (found) {
+			chunk = 1;
+			while(1) {
+				start = pat->chunk[chunk].start;
+				len   = pat->chunk[chunk].len;
+				if (len == 0) break;
+
+				GN_ASSERT(len >2);
+
+				if (VG_(memcmp)( (void*)(addr+start), code+start, len) != 0) {
+					found = False;
+					break;
+				}
+				chunk++;
+			}
+
+			if (found) {
+				if (VG_(clo_verbosity) > 1)
+					VG_(message)(Vg_DebugMsg, "Found runtime_resolve (%s): "
+								 "%s +%#lx=%#lx, length %d\n",
+								 pat->name, obj->name + obj->last_slash_pos,
+								 addr - obj->start, addr, pat->len);
+
+				runtimeResolveAddr   = addr;
+				runtimeResolveLength = pat->len;
+				return True;
+			}
+		}
+		addr++;
+	}
+	return False;
+}
+
+
+static Bool searchRuntimeResolve(ObjNode* obj)
+{
+	/* _ld_runtime_resolve, located in ld.so, needs special handling:
+	 * The jump at end into the resolved function should not be
+	 * represented as a call (as usually done in callgrind with jumps),
+	 * but as a return + call. Otherwise, the repeated existence of
+	 * _ld_runtime_resolve in call chains will lead to huge cycles,
+	 * making the profile almost worthless.
+	 *
+	 * If ld.so is stripped, the symbol will not appear. But as this
+	 * function is handcrafted assembler, we search for it.
+	 *
+	 * We stop if the ELF object name does not seem to be the runtime linker
+	 */
+#if defined(VGP_x86_linux)
+	static UChar code[] = {
+		/* 0*/ 0x50, 0x51, 0x52, 0x8b, 0x54, 0x24, 0x10, 0x8b,
+		/* 8*/ 0x44, 0x24, 0x0c, 0xe8, 0x70, 0x01, 0x00, 0x00,
+		/*16*/ 0x5a, 0x59, 0x87, 0x04, 0x24, 0xc2, 0x08, 0x00 };
+	/* Check ranges [0-11] and [16-23] ([12-15] is an absolute address) */
+	static struct pattern pat = {
+		"x86-def", 24, {{ 0,12 }, { 16,8 }, { 24,0}} };
+
+	/* Pattern for glibc-2.8 on OpenSuse11.0 */
+	static UChar code_28[] = {
+		/* 0*/ 0x50, 0x51, 0x52, 0x8b, 0x54, 0x24, 0x10, 0x8b,
+		/* 8*/ 0x44, 0x24, 0x0c, 0xe8, 0x70, 0x01, 0x00, 0x00,
+		/*16*/ 0x5a, 0x8b, 0x0c, 0x24, 0x89, 0x04, 0x24, 0x8b,
+		/*24*/ 0x44, 0x24, 0x04, 0xc2, 0x0c, 0x00 };
+	static struct pattern pat_28 = {
+		"x86-glibc2.8", 30, {{ 0,12 }, { 16,14 }, { 30,0}} };
+
+	if (VG_(strncmp)(obj->name, "/lib/ld", 7) != 0) return False;
+	if (checkCode(obj, code, &pat)) return True;
+	if (checkCode(obj, code_28, &pat_28)) return True;
+	return False;
+#endif
+
+#if defined(VGP_ppc32_linux)
+	static UChar code[] = {
+		/* 0*/ 0x94, 0x21, 0xff, 0xc0, 0x90, 0x01, 0x00, 0x0c,
+		/* 8*/ 0x90, 0x61, 0x00, 0x10, 0x90, 0x81, 0x00, 0x14,
+		/*16*/ 0x7d, 0x83, 0x63, 0x78, 0x90, 0xa1, 0x00, 0x18,
+		/*24*/ 0x7d, 0x64, 0x5b, 0x78, 0x90, 0xc1, 0x00, 0x1c,
+		/*32*/ 0x7c, 0x08, 0x02, 0xa6, 0x90, 0xe1, 0x00, 0x20,
+		/*40*/ 0x90, 0x01, 0x00, 0x30, 0x91, 0x01, 0x00, 0x24,
+		/*48*/ 0x7c, 0x00, 0x00, 0x26, 0x91, 0x21, 0x00, 0x28,
+		/*56*/ 0x91, 0x41, 0x00, 0x2c, 0x90, 0x01, 0x00, 0x08,
+		/*64*/ 0x48, 0x00, 0x02, 0x91, 0x7c, 0x69, 0x03, 0xa6, /* at 64: bl aff0 <fixup> */
+		/*72*/ 0x80, 0x01, 0x00, 0x30, 0x81, 0x41, 0x00, 0x2c,
+		/*80*/ 0x81, 0x21, 0x00, 0x28, 0x7c, 0x08, 0x03, 0xa6,
+		/*88*/ 0x81, 0x01, 0x00, 0x24, 0x80, 0x01, 0x00, 0x08,
+		/*96*/ 0x80, 0xe1, 0x00, 0x20, 0x80, 0xc1, 0x00, 0x1c,
+		/*104*/0x7c, 0x0f, 0xf1, 0x20, 0x80, 0xa1, 0x00, 0x18,
+		/*112*/0x80, 0x81, 0x00, 0x14, 0x80, 0x61, 0x00, 0x10,
+		/*120*/0x80, 0x01, 0x00, 0x0c, 0x38, 0x21, 0x00, 0x40,
+		/*128*/0x4e, 0x80, 0x04, 0x20 };
+	static struct pattern pat = {
+		"ppc32-def", 132, {{ 0,65 }, { 68,64 }, { 132,0 }} };
+
+	if (VG_(strncmp)(obj->name, "/lib/ld", 7) != 0) return False;
+	return checkCode(obj, code, &pat);
+#endif
+
+#if defined(VGP_amd64_linux)
+	static UChar code[] = {
+		/* 0*/ 0x48, 0x83, 0xec, 0x38, 0x48, 0x89, 0x04, 0x24,
+		/* 8*/ 0x48, 0x89, 0x4c, 0x24, 0x08, 0x48, 0x89, 0x54, 0x24, 0x10,
+		/*18*/ 0x48, 0x89, 0x74, 0x24, 0x18, 0x48, 0x89, 0x7c, 0x24, 0x20,
+		/*28*/ 0x4c, 0x89, 0x44, 0x24, 0x28, 0x4c, 0x89, 0x4c, 0x24, 0x30,
+		/*38*/ 0x48, 0x8b, 0x74, 0x24, 0x40, 0x49, 0x89, 0xf3,
+		/*46*/ 0x4c, 0x01, 0xde, 0x4c, 0x01, 0xde, 0x48, 0xc1, 0xe6, 0x03,
+		/*56*/ 0x48, 0x8b, 0x7c, 0x24, 0x38, 0xe8, 0xee, 0x01, 0x00, 0x00,
+		/*66*/ 0x49, 0x89, 0xc3, 0x4c, 0x8b, 0x4c, 0x24, 0x30,
+		/*74*/ 0x4c, 0x8b, 0x44, 0x24, 0x28, 0x48, 0x8b, 0x7c, 0x24, 0x20,
+		/*84*/ 0x48, 0x8b, 0x74, 0x24, 0x18, 0x48, 0x8b, 0x54, 0x24, 0x10,
+		/*94*/ 0x48, 0x8b, 0x4c, 0x24, 0x08, 0x48, 0x8b, 0x04, 0x24,
+		/*103*/0x48, 0x83, 0xc4, 0x48, 0x41, 0xff, 0xe3 };
+	static struct pattern pat = {
+		"amd64-def", 110, {{ 0,62 }, { 66,44 }, { 110,0 }} };
+
+	if ((VG_(strncmp)(obj->name, "/lib/ld", 7) != 0) &&
+		(VG_(strncmp)(obj->name, "/lib64/ld", 9) != 0)) return False;
+	return checkCode(obj, code, &pat);
+#endif
+
+	/* For other platforms, no patterns known */
+	return False;
+}
+
+
+static ObjNode* newObjNode(DebugInfo *di, ObjNode *next)
+{
+	ObjNode *obj = VG_(malloc)("gn.file.newobjnode.1", sizeof(ObjNode));
+	obj->name = di ? VG_(strdup)("gn.file.newobjnode.2", VG_(DebugInfo_get_filename)(di))
+		: anonname;
+
+	for (UInt i=0; i<N_FILE_ENTRIES; i++) {
+		obj->files[i] = NULL;
+	}
+
+	obj->number = uniqueObjs++;
+	obj->start  = di ? VG_(DebugInfo_get_text_avma)(di) : 0;
+	obj->size   = di ? VG_(DebugInfo_get_text_size)(di) : 0;
+	obj->offset = di ? VG_(DebugInfo_get_text_bias)(di) : 0;
+	obj->next   = next;
+
+	obj->last_slash_pos = 0;
+	for (UInt i=0; obj->name[i] != '\0'; i++) {
+		if (obj->name[i] == '/')
+			obj->last_slash_pos = i+1;
+	}
+
+	if (runtimeResolveAddr == 0)
+		searchRuntimeResolve(obj);
+
+	return obj;
+}
+
+
+static inline FileNode* newFileNode(const HChar *filename, ObjNode *const obj,
+                                    FileNode *const next)
+{
+    FileNode *file = VG_(malloc)("gn.file.newfilenode.1", sizeof(FileNode));
+    file->name = VG_(strdup)("gn.file.newfilenode.2", filename);
+    for (UInt i=0; i<N_FN_ENTRIES; i++)
+        file->fns[i] = NULL;
+
+    file->number = uniqueFiles++;
+    file->obj    = obj;
+    file->next   = next;
+
+    return file;
+}
+
+
+static inline FnNode* newFnNode(const HChar *fnname, FileNode *const file,
+                                FnNode *const next)
+{
+    FnNode *fn = VG_(malloc)("gn.file.newfnnode.1", sizeof(FnNode));
+    fn->name = VG_(strdup)("gn.file.newfnnode.2", fnname);
+
+    fn->number   = uniqueFns++;
+    fn->file     = file;
+    fn->next     = next;
+
+    // TODO(soon) are these needed?
+    fn->dump_before    = False;
+    fn->dump_after     = False;
+    fn->zero_before    = False;
+    fn->toggle_collect = False;
+    fn->skip           = False;
+    fn->pop_on_jump    = False;
+    fn->is_malloc      = False;
+    fn->is_free        = False;
+    fn->is_realloc     = False;
+
+    fn->group               = 0;
+
+    return fn;
+}
+
+
+static Bool GN_(getDebugInfo)(/*IN*/  Addr instr,
+                              /*OUT*/ const HChar **dirname,
+                              /*OUT*/ const HChar **filename,
+                              /*OUT*/ const HChar **fnname,
+                              /*Optional OUT*/ DebugInfo **di)
+{
+    Bool result = True;
+    UInt lineno;
+
+    if (di)
+        *di = VG_(find_DebugInfo)(instr);
+
+    Bool foundFn = VG_(get_fnname)(instr, fnname);
+    Bool foundFileLine = VG_(get_filename_linenum)(instr, filename, dirname, &lineno);
+
+    if (!foundFn && !foundFileLine) {
+        *filename = anonname;
+        *fnname = anonname;
+        result = False;
+    }
+    else if (!foundFn && foundFileLine) {
+        *fnname = anonname;
+    }
+    else if (foundFn && !foundFileLine) {
+        *filename = anonname;
+    }
+
+    return result;
+}
+
+
+static inline const HChar* getUnknownFnName(const BBInfo *bb)
+{
+    // just use the address if no name found
+    static HChar buf[32];
+    GN_ASSERT(sizeof(Addr) == 8);
+    int p = VG_(sprintf)(buf, "%#016lx", (UWord)(bb->offset));
+    VG_(sprintf)(buf+p, "%s",
+                 (bb->sectKind == Vg_SectData) ? "[Data]" :
+                 (bb->sectKind == Vg_SectBSS)  ? " [BSS]" :
+                 (bb->sectKind == Vg_SectGOT)  ? " [GOT]" :
+                 (bb->sectKind == Vg_SectPLT)  ? " [PLT]" : "");
+    return buf;
+}
+
+
+static inline FnNode* getFnNodeInFile(FileNode *const file, const HChar *fnname)
+{
+    UInt fnnamehash = strHash(fnname, N_FN_ENTRIES);
+    FnNode *fn = file->fns[fnnamehash];
+
+    while (fn) {
+        if (VG_(strcmp)(fnname, fn->name) == 0)
+            break;
+        fn = fn->next;
+    }
+
+    if (fn == NULL) {
+        fn = newFnNode(fnname, file, file->fns[fnnamehash]);
+        file->fns[fnnamehash] = fn;
+    }
+
+    return fn;
+}
+
+
+static inline FnNode* getFnNodeInSeg(DebugInfo *const di,
+                                     const HChar *dirname,
+                                     const HChar *filename,
+                                     const HChar *fnname)
+{
+    ObjNode *obj = GN_(getObjNode)(di);
+    FileNode *file = GN_(getFileNode)(obj, dirname, filename);
+    FnNode *fn = getFnNodeInFile(file, fnname);
+
+    return fn;
+}
+
+
+//-------------------------------------------------------------------------------------------------
+/** File function definitions - External **/
+
+FileNode* GN_(getFileNode)(ObjNode *const obj,
+                           const HChar *dirname,
+                           const HChar *filename)
+{
+    /* absolute path */
+    HChar abspath[VG_(strlen)(dirname) + 1 + VG_(strlen)(filename) + 1];
+    VG_(strcpy)(abspath, dirname);
+    if (filename[0] != '\0')
+        VG_(strcat)(abspath, "/");
+    VG_(strcat)(abspath, filename);
+
+    /* hash */
+    UInt filenamehash = strHash(filename, N_FILE_ENTRIES);
+    FileNode *file = obj->files[filenamehash];
+
+    while (file) {
+        if (VG_(strcmp)(filename, file->name) == 0)
+            break;
+        file = file->next;
+    }
+
+    if (file == NULL) {
+        file = newFileNode(filename, obj, obj->files[filenamehash]);
+        obj->files[filenamehash] = file;
+    }
+
+    return file;
+}
+
+
+ObjNode* GN_(getObjNode)(DebugInfo *di)
+{
+    ObjNode *obj;
+    UInt namehash;
+    const HChar *name;
+
+    name = di ? VG_(DebugInfo_get_filename)(di) : anonname;
+    namehash = strHash(name, N_OBJ_ENTRIES);
+    obj = allObjs[namehash];
+
+    while (obj) {
+        if (VG_(strcmp)(name, obj->name) == 0)
+            break;
+        obj = obj->next;
+    }
+
+    if (obj == NULL) {
+        obj = newObjNode(di, allObjs[namehash]);
+        allObjs[namehash] = obj;
+    }
+
+    return obj;
+}
+
+
+ObjNode* GN_(getObjNodeByAddr)(Addr addr)
+{
+    ObjNode *obj;
+    DebugInfo *di;
+    PtrdiffT offset;
+
+    di = VG_(find_DebugInfo)(addr);
+    obj = GN_(getObjNode)(di);
+
+	// check if object was remapped
+	offset = di ? VG_(DebugInfo_get_text_bias)(di) : 0;
+	if (obj->offset != offset) {
+		Addr start = di ? VG_(DebugInfo_get_text_avma)(di) : 0;
+		GN_ASSERT(obj->size == (di ? VG_(DebugInfo_get_text_size)(di) : 0));
+		GN_ASSERT((PtrdiffT)(obj->start - start) == obj->offset - offset);
+		obj->offset = offset;
+		obj->start = start;
+	}
+
+	return obj;
+}
+
+
+FnNode* GN_(getFnNode)(BBInfo *bb)
+{
+    /* Return if already cached
+     * This should be the common path */
+    if (bb->fn)
+        return bb->fn;
+
+    /* Else, lookup function info in binary */
+
+    const HChar *fnname, *filename, *dirname;
+    DebugInfo *di;
+
+    /* Find debug info for function  */
+    GN_(getDebugInfo)(bbAddr(bb), &dirname, &filename, &fnname, &di);
+
+    if (VG_(strcmp)(fnname, anonname) == 0)
+        fnname = getUnknownFnName(bb); // name correction
+    else if (VG_(get_fnname_if_entry)(bbAddr(bb), &fnname))
+        bb->isFnEntry = True;
+
+    /* HACK for correct _exit:
+     * _exit is redirected to VG_(__libc_freeres_wrapper) by valgrind,
+     * so we rename it back again :-) */
+    static BBInfo *exitBB = NULL;
+    if ((VG_(strcmp)(fnname, "vgPlain___libc_freeres_wrapper") == 0) && (exitBB != NULL))
+        GN_(getDebugInfo)(bbAddr(exitBB), &dirname, &filename, &fnname, &di);
+    if ((VG_(strcmp)(fnname, "_exit") == 0) && (exitBB == NULL))
+        exitBB = bb;
+
+    /* Special handling for dynamic library resolution function */
+    if (runtimeResolveAddr &&
+        (bbAddr(bb) >= runtimeResolveAddr) &&
+        (bbAddr(bb) < runtimeResolveAddr + runtimeResolveLength)) {
+        fnname = runtimeResolveName;
+    }
+
+    /* Look up function metadata node */
+    FnNode *fn = getFnNodeInSeg(di, dirname, filename, fnname);
+
+    /* Last initialization step requiring BBInfo */
+    if (fn->initialized == False) {
+
+        if (bb->sectKind == Vg_SectPLT)
+            fn->skip = GN_(clo).skip_plt;
+
+        if (VG_(strcmp)(fn->name, runtimeResolveName) == 0)
+            fn->pop_on_jump = True;
+
+        fn->is_malloc  = (VG_(strcmp)(fn->name, "malloc") == 0);
+        fn->is_realloc = (VG_(strcmp)(fn->name, "realloc") == 0);
+        fn->is_free    = (VG_(strcmp)(fn->name, "free") == 0);
+
+        /* TODO(soon) update fn config (dump before/after, toggle collect, et al) */
+
+        fn->initialized = True;
+    }
+
+    bb->fn = fn;
+    return fn;
+}
+
+
+void GN_(initFn)()
+{
+    uniqueObjs = 0;
+    uniqueFiles = 0;
+    uniqueFns = 0;
+
+    for (UInt i=0; i<N_OBJ_ENTRIES; ++i)
+        allObjs[i] = NULL;
+}
diff --git a/gengrind/gn_fn.h b/gengrind/gn_fn.h
new file mode 100644
index 000000000..03f2425ff
--- /dev/null
+++ b/gengrind/gn_fn.h
@@ -0,0 +1,83 @@
+/* Helpers to get info for functions from the binary,
+ * such as scanning ELF sections and debug info */
+
+#ifndef GN_FILE_H
+#define GN_FILE_H
+
+#include "gn.h"
+
+
+//-------------------------------------------------------------------------------------------------
+/** File type definitions **/
+
+/* Quite arbitrary fixed hash sizes */
+#define   N_OBJ_ENTRIES         47
+#define  N_FILE_ENTRIES         53
+#define    N_FN_ENTRIES         87
+
+struct _FileNode {
+   HChar*     name;
+   FnNode*   fns[N_FN_ENTRIES];
+   UInt       number;
+   ObjNode*  obj;
+   FileNode* next;
+};
+
+
+struct _ObjNode {
+    /* If an object is dlopened multiple times, we hope that <name> is unique;
+     * <start> and <offset> can change with each dlopen, and <start> is
+     * zero when object is unmapped (possible at dump time).  */
+
+   const HChar* name;
+   UInt       last_slash_pos;
+
+   Addr       start;  /* Start address of text segment mapping */
+   SizeT      size;   /* Length of mapping */
+   PtrdiffT   offset; /* Offset between symbol address and file offset */
+
+   FileNode* files[N_FILE_ENTRIES];
+   UInt       number;
+   ObjNode*  next;
+};
+
+
+struct _FnNode {
+    /* the <number> of fn_node, file_node and obj_node are for compressed dumping
+     * and a index into the dump boolean table and fn_info_table */
+
+    HChar*    name;
+    UInt      number;
+    FileNode* file;     /* reverse mapping for 2nd hash */
+    FnNode*   next;
+
+    Bool initialized    : 1;
+    Bool dump_before    : 1;
+    Bool dump_after     : 1;
+    Bool zero_before    : 1;
+    Bool toggle_collect : 1;
+    Bool skip           : 1; // currently unused, see Callgrind
+    Bool pop_on_jump    : 1; // currently unused, see Callgrind
+
+    Bool is_malloc      : 1;
+    Bool is_realloc     : 1;
+    Bool is_free        : 1;
+
+    Int  group;
+    Int  separate_callers;
+    Int  separate_recursions;
+    /* TODO(someday) These are not used right now
+     * see Callgrind */
+};
+
+
+//-------------------------------------------------------------------------------------------------
+/** File function declarations **/
+
+FnNode* GN_(getFnNode)(BBInfo *bb);
+ObjNode* GN_(getObjNode)(DebugInfo *di);
+ObjNode* GN_(getObjNodeByAddr)(Addr addr);
+FileNode* GN_(getFileNode)(ObjNode *const obj, const HChar *dirname, const HChar *filename);
+void GN_(initFn)(void);
+
+#endif
diff --git a/gengrind/gn_ipc.c b/gengrind/gn_ipc.c
new file mode 100644
index 000000000..616ce6eea
--- /dev/null
+++ b/gengrind/gn_ipc.c
@@ -0,0 +1,350 @@
+#include "gn_ipc.h"
+#include "gn_clo.h"
+#include "coregrind/pub_core_libcfile.h"
+#include "coregrind/pub_core_aspacemgr.h"
+#include "coregrind/pub_core_syscall.h"
+#include "pub_tool_basics.h"
+#include "pub_tool_vki.h"       // errnum, vki_timespec
+#include "pub_tool_vkiscnums.h" // __NR_nanosleep
+
+static Bool initialized = False;
+static Int gnEmptyFd;
+static Int gnFullFd;
+static Sigil2DBISharedData* gnShmem;
+
+SglEvVariant *GN_(currEv);
+SglEvVariant *GN_(endEv);
+size_t *GN_(usedEv);
+/* IPC channel */
+
+static UInt          gnNextIdx;
+static UInt          gnCurrIdx;
+static EventBuffer   *gnCurrEvBuf;
+//static SglEvVariant  *currEvSlot;
+//static NameBuffer    *currNameBuf;
+//static char*         currNameSlot;
+/* cached IPC state */
+
+
+static Bool isFull[SIGIL2_IPC_BUFFERS];
+/* track available buffers */
+
+
+//static inline void set_next_buffer(void)
+//{
+//    /* try the next buffer, circular */
+//    ++curr_idx;
+//    if (curr_idx == SIGIL2_IPC_BUFFERS)
+//        curr_idx = 0;
+//
+//    /* if the next buffer is full,
+//     * wait until Sigil2 communicates that it's free */
+//    if (is_full[curr_idx])
+//    {
+//        UInt buf_idx;
+//        Int res = VG_(read)(gnEmptyFd, &buf_idx, sizeof(buf_idx));
+//        if (res != sizeof(buf_idx))
+//        {
+//            VG_(umsg)("error VG_(read)\n");
+//            VG_(umsg)("error reading from Sigrind fifo\n");
+//            VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+//            VG_(exit)(1);
+//        }
+//
+//        tl_assert(buf_idx < SIGIL2_IPC_BUFFERS);
+//        tl_assert(buf_idx == curr_idx);
+//        curr_idx = buf_idx;
+//        is_full[curr_idx] = False;
+//    }
+//
+//    set_and_init_buffer(curr_idx);
+//}
+//
+//
+//static inline Bool is_events_full(void)
+//{
+//    return curr_ev_buf->used == SIGIL2_EVENTS_BUFFER_SIZE;
+//}
+//
+//
+//static inline Bool is_names_full(UInt size)
+//{
+//    return (curr_name_buf->used + size) > SIGIL2_EVENTS_BUFFER_SIZE;
+//}
+//
+//
+//SglEvVariant* GN_(acq_event_slot)()
+//{
+//    tl_assert(initialized == True);
+//
+//    if (is_events_full())
+//    {
+//        flush_to_sigil2();
+//        set_next_buffer();
+//    }
+//
+//    curr_ev_buf->used++;
+//    return curr_ev_slot++;
+//}
+//
+//
+//EventNameSlotTuple GN_(acq_event_name_slot)(UInt size)
+//{
+//    tl_assert(initialized == True);
+//
+//    if (is_events_full() || is_names_full(size))
+//    {
+//        flush_to_sigil2();
+//        set_next_buffer();
+//    }
+//
+//    EventNameSlotTuple tuple = {curr_ev_slot, curr_name_slot, curr_name_buf->used};
+//    curr_ev_buf->used   += 1;
+//    curr_ev_slot        += 1;
+//    curr_name_buf->used += size;
+//    curr_name_slot      += size;
+//
+//    return tuple;
+//}
+
+
+//-------------------------------------------------------------------------------------------------
+/** Initialization/Termination **/
+
+static int openFifo(const HChar *fifo_path, int flags)
+{
+    tl_assert(initialized == False);
+
+    int tries = 0;
+    const int max_tries = 4;
+    int fd = VG_(fd_open)(fifo_path, flags, 0600);
+    while (fd < 0) {
+        if (++tries < max_tries) {
+#if defined(VGO_linux) && defined(VGA_amd64)
+            /* TODO any serious implications in Valgrind of calling syscalls directly?
+             * MDL20170220 The "VG_(syscall)" wrappers don't look like they do much
+             * else besides doing platform specific setup.
+             * In our case, we only accommodate x86_64 or aarch64. */
+            struct vki_timespec req;
+            req.tv_sec = 0;
+            req.tv_nsec = 500000000;
+            /* wait some time before trying to connect,
+             * giving Sigil2 time to bring up IPC */
+            VG_(do_syscall2)(__NR_nanosleep, (UWord)&req, 0);
+#else
+#error "Only linux is supported"
+#endif
+            fd = VG_(fd_open)(fifo_path, flags, 0600);
+        }
+        else {
+            VG_(umsg)("FIFO for Sigrind failed\n");
+            VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+            VG_(exit) (1);
+        }
+    }
+
+    return fd;
+}
+
+
+static Sigil2DBISharedData* openShmem(const HChar *gnShmem_path, int flags)
+{
+    tl_assert(initialized == False);
+
+    int shared_mem_fd = VG_(fd_open)(gnShmem_path, flags, 0600);
+    if (shared_mem_fd < 0) {
+        VG_(umsg)("Cannot open shared_mem file %s\n", gnShmem_path);
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+
+    SysRes res = VG_(am_shared_mmap_file_float_valgrind)(sizeof(Sigil2DBISharedData),
+                                                         VKI_PROT_READ|VKI_PROT_WRITE,
+                                                         shared_mem_fd, (Off64T)0);
+    if (sr_isError(res)) {
+        VG_(umsg)("error %lu %s\n", sr_Err(res), VG_(strerror)(sr_Err(res)));
+        VG_(umsg)("error VG_(am_shared_mmap_file_float_valgrind) %s\n", gnShmem_path);
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+
+    Addr addr_shared = sr_Res (res);
+    VG_(close)(shared_mem_fd);
+
+    return (Sigil2DBISharedData*) addr_shared;
+}
+
+
+void GN_(initIPC)()
+{
+    tl_assert(initialized == False);
+
+    if (GN_(clo).standalone_test == True) {
+        gnShmem = VG_(malloc)("gn.test.buffer", sizeof(*gnShmem));
+
+        gnNextIdx = 0;
+        gnCurrEvBuf = gnShmem->eventBuffers + gnNextIdx;
+        gnCurrEvBuf->used = 0;
+        GN_(currEv) = gnCurrEvBuf->events + gnCurrEvBuf->used;
+        GN_(usedEv) = &gnCurrEvBuf->used;
+
+        /* ensure events is an array, not a pointer */
+        tl_assert(sizeof(gnCurrEvBuf->events) != sizeof(gnCurrEvBuf->events[0]));
+        GN_(endEv) = gnCurrEvBuf->events + sizeof(gnCurrEvBuf->events)/sizeof(gnCurrEvBuf->events[0]);
+        for (UInt i=0; i<SIGIL2_IPC_BUFFERS; ++i)
+            isFull[i] = False;
+
+        ++gnNextIdx;
+
+        initialized = True;
+        return;
+    }
+
+    if (GN_(clo).ipc_dir == NULL) {
+        VG_(fmsg)("No --ipc-dir argument found, shutting down...\n");
+        VG_(exit)(1);
+    }
+
+    Int ipcDirLen = VG_(strlen)(GN_(clo).ipc_dir);
+    Int filenameLen;
+
+    //len is strlen + null + other chars (/ and -0)
+    filenameLen = ipcDirLen + VG_(strlen)(SIGIL2_IPC_SHMEM_BASENAME) + 4;
+    HChar gnShmemPath[filenameLen];
+    VG_(snprintf)(gnShmemPath, filenameLen, "%s/%s-0", GN_(clo).ipc_dir, SIGIL2_IPC_SHMEM_BASENAME);
+
+    filenameLen = ipcDirLen + VG_(strlen)(SIGIL2_IPC_EMPTYFIFO_BASENAME) + 4;
+    HChar emptyfifoPath[filenameLen];
+    VG_(snprintf)(emptyfifoPath, filenameLen, "%s/%s-0", GN_(clo).ipc_dir, SIGIL2_IPC_EMPTYFIFO_BASENAME);
+
+    filenameLen = ipcDirLen + VG_(strlen)(SIGIL2_IPC_FULLFIFO_BASENAME) + 4;
+    HChar fullfifoPath[filenameLen];
+    VG_(snprintf)(fullfifoPath, filenameLen, "%s/%s-0", GN_(clo).ipc_dir, SIGIL2_IPC_FULLFIFO_BASENAME);
+
+    gnEmptyFd = openFifo(emptyfifoPath, VKI_O_RDONLY);
+    gnFullFd  = openFifo(fullfifoPath, VKI_O_WRONLY);
+    gnShmem   = openShmem(gnShmemPath, VKI_O_RDWR);
+
+    /* initialize cached IPC state */
+    GN_(currEv) = NULL;
+    GN_(endEv) = NULL;
+    gnCurrIdx = 0;
+    gnNextIdx = 0;
+    GN_(setNextBuffer)();
+    for (UInt i=0; i<SIGIL2_IPC_BUFFERS; ++i)
+        isFull[i] = False;
+
+    initialized = True;
+}
+
+
+void GN_(termIPC)(void)
+{
+    tl_assert(initialized == True);
+
+    if (GN_(clo).standalone_test == True) {
+        VG_(free)(gnShmem);
+        gnShmem = NULL;
+        gnCurrEvBuf = NULL;
+        GN_(currEv) = NULL;
+        GN_(usedEv) = NULL;
+        GN_(endEv) = NULL;
+        return;
+    }
+
+
+    /* Flush the remaining buffer ... */
+    GN_(flushCurrBuffer)();
+
+    /* ... and send finish sequence */
+    UInt finished = SIGIL2_IPC_FINISHED;
+    if (VG_(write)(gnFullFd, &finished, sizeof(finished)) != sizeof(finished)) {
+        VG_(umsg)("error VG_(write)\n");
+        VG_(umsg)("error writing to Sigrind fifo\n");
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+
+    /* wait until Sigrind disconnects */
+    while (VG_(read)(gnEmptyFd, &finished, sizeof(finished)) > 0);
+
+    VG_(close)(gnEmptyFd);
+    VG_(close)(gnFullFd);
+}
+
+
+void GN_(flushCurrBuffer)(void)
+{
+    /* Mark that the buffer is being flushed,
+     * and tell Sigil2 the buffer is ready to consume */
+    isFull[gnCurrIdx] = True;
+    Int res = VG_(write)(gnFullFd, &gnCurrIdx, sizeof(gnCurrIdx));
+    if (res != sizeof(gnCurrIdx)) {
+        VG_(umsg)("error VG_(write)\n");
+        VG_(umsg)("error writing to Sigrind fifo\n");
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+}
+
+
+void GN_(setNextBuffer)(void)
+{
+    /* try the next buffer, circular */
+    if (gnNextIdx == SIGIL2_IPC_BUFFERS)
+        gnNextIdx = 0;
+
+    /* if the next buffer is full,
+     * wait until Sigil2 communicates that it's free */
+    if (isFull[gnNextIdx]) {
+        UInt bufIdx;
+        Int res = VG_(read)(gnEmptyFd, &bufIdx, sizeof(bufIdx));
+        if (res != sizeof(bufIdx)) {
+            VG_(umsg)("error VG_(read)\n");
+            VG_(umsg)("error reading from Sigrind fifo\n");
+            VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+            VG_(exit)(1);
+        }
+
+        tl_assert(bufIdx < SIGIL2_IPC_BUFFERS);
+        tl_assert(bufIdx == gnNextIdx);
+        isFull[gnNextIdx] = False;
+    }
+
+    gnCurrEvBuf = gnShmem->eventBuffers + gnNextIdx;
+    gnCurrEvBuf->used = 0;
+    GN_(currEv) = gnCurrEvBuf->events + gnCurrEvBuf->used;
+    GN_(usedEv) = &gnCurrEvBuf->used;
+
+    /* ensure events is an array, not a pointer */
+    tl_assert(sizeof(gnCurrEvBuf->events) != sizeof(gnCurrEvBuf->events[0]));
+    GN_(endEv) = gnCurrEvBuf->events + sizeof(gnCurrEvBuf->events)/sizeof(gnCurrEvBuf->events[0]);
+
+    //currNameBuf = gnShmem->nameBuffers + currIdx;
+    //currNameBuf->used = 0;
+    //currNameSlot = curr_name_buf->names + curr_name_buf->used;
+
+    gnCurrIdx = gnNextIdx;
+    ++gnNextIdx;
+}
+
+
+void GN_(flushCurrAndSetNextBuffer)(void)
+{
+    if (GN_(clo).standalone_test == True) {
+        gnCurrEvBuf = gnShmem->eventBuffers + gnNextIdx;
+        gnCurrEvBuf->used = 0;
+        GN_(currEv) = gnCurrEvBuf->events + gnCurrEvBuf->used;
+        GN_(usedEv) = &gnCurrEvBuf->used;
+
+        /* ensure events is an array, not a pointer */
+        tl_assert(sizeof(gnCurrEvBuf->events) != sizeof(gnCurrEvBuf->events[0]));
+        GN_(endEv) = gnCurrEvBuf->events + sizeof(gnCurrEvBuf->events)/sizeof(gnCurrEvBuf->events[0]);
+
+        ++gnNextIdx;
+    }
+    else {
+        GN_(flushCurrBuffer)();
+        GN_(setNextBuffer)();
+    }
+}
diff --git a/gengrind/gn_ipc.h b/gengrind/gn_ipc.h
new file mode 100644
index 000000000..a278774b4
--- /dev/null
+++ b/gengrind/gn_ipc.h
@@ -0,0 +1,43 @@
+#ifndef GN_IPC_H
+#define GN_IPC_H
+
+#include "Frontends/CommonShmemIPC.h"
+#include "gn.h"
+
+/* An implementation of interprocess communication with the Sigil2 frontend.
+ * IPC includes initialization, termination, shared memory buffer writes, and
+ * synchronization via named pipes */
+
+typedef struct EventNameSlotTuple
+{
+    SglEvVariant*  event_slot;
+    char*          name_slot;
+    UInt           name_idx;
+} EventNameSlotTuple;
+
+void GN_(initIPC)(void);
+void GN_(termIPC)(void);
+
+void GN_(setNextBuffer)(void);
+void GN_(flushCurrBuffer)(void);
+void GN_(flushCurrAndSetNextBuffer)(void);
+
+// TODO delete
+//SglEvVariant* GN_(acq_event_slot)(void);
+/* Get a buffer slot to add an event */
+//EventNameSlotTuple GN_(acq_event_name_slot)(UInt size);
+/* Get a buffer slot to add an event (probably a context event)
+ * and a name slot to add a name with it (like a function name) */
+
+extern SglEvVariant *GN_(currEv);
+extern SglEvVariant *GN_(endEv);
+extern size_t *GN_(usedEv);
+/* Points to current location in event buffer,
+ * to generate events to,
+ * and the end position in the buffer
+ * 
+ * A null pointer means no buffer has been acquired,
+ * currEv == endEv means the buffer is full
+ */
+
+#endif
diff --git a/gengrind/gn_jumps.c b/gengrind/gn_jumps.c
new file mode 100644
index 000000000..0f62f855a
--- /dev/null
+++ b/gengrind/gn_jumps.c
@@ -0,0 +1,160 @@
+#include "gn_jumps.h"
+#include "gn_bb.h"
+#include "gn_debug.h"
+
+
+UInt GN_(lastJmpsPassed);
+static JumpTable allJns;
+
+
+//-------------------------------------------------------------------------------------------------
+/** Helper function definitions **/
+
+static inline UInt jnHashIdx(BBInfo *from, UInt jmp, BBInfo *to, UInt capacity)
+{
+    return (UInt) ( (UWord)from + 7 * (UWord)to + 13 * jmp) % capacity;
+}
+
+
+#define N_JN_INITIAL_ENTRIES (4437)
+static void initJumpTable(JumpTable *jt)
+{
+    GN_ASSERT(jt != NULL);
+
+    jt->capacity = N_JN_INITIAL_ENTRIES;
+    jt->entries = 0;
+    jt->table = VG_(malloc)("gn.jumps.initjt.1", jt->capacity * sizeof(JumpNode*));
+    jt->spontaneous = NULL;
+
+    for (UInt i=0; i<jt->capacity; ++i)
+        jt->table[i] = NULL;
+}
+
+
+static void resizeJumpTable(JumpTable *jt)
+{
+    UInt newcap = 2 * jt->capacity + 3;
+    JumpNode **newtable = VG_(malloc)("gn.jumps.resize.1", newcap * sizeof(JumpNode*));
+
+    for (UInt i=0; i<newcap; ++i)
+        newtable[i] = NULL;
+
+    for (UInt i=0; i<jt->capacity; ++i) {
+
+        JumpNode *jn = jt->table[i];
+        if (jn == NULL)
+            continue;
+        while (jn != NULL) {
+            JumpNode *next = jn->next_hash;
+            UInt newidx = jnHashIdx(jn->from, jn->jmp, jn->to, newcap);
+            jn->next_hash = newtable[newidx];
+            newtable[newidx] = jn;
+            jn = next;
+        }
+    }
+
+    VG_(free)(jt->table);
+    jt->table = newtable;
+    jt->capacity = newcap;
+}
+
+
+static JumpNode* newJumpNode(BBInfo *from, UInt jmp, BBInfo *to)
+{
+    JumpNode *jn;
+
+    allJns.entries++;
+
+    /* Use Callgrind's hash implementation
+     * resize if filled more than 80% */
+    if (10 * allJns.entries / allJns.capacity > 8)
+        resizeJumpTable(&allJns);
+
+    jn = VG_(malloc)("gn.jumps.newjn.1", sizeof(JumpNode));
+
+    jn->from = from;
+    jn->jmp = jmp;
+    jn->to = to;
+    jn->jmpkind = jk_Call; // ML: why is this?
+
+    jn->next_from = allJns.spontaneous;
+    allJns.spontaneous = jn;
+
+    UInt idx = jnHashIdx(from, jmp, to, allJns.capacity);
+    jn->next_hash = allJns.table[idx];
+    allJns.table[idx] = jn;
+
+    return jn;
+}
+
+
+//-------------------------------------------------------------------------------------------------
+/** External function definitions **/
+
+JumpNode* GN_(getJumpNode)(BBInfo *from, UInt jmp, BBInfo *to)
+{
+    JumpNode *jn;
+
+    if ((jn = to->lruToJmp) && (jn->from == from) && (jn->jmp = jmp)) {
+        GN_ASSERT(jn->to == to);
+    }
+    else if ((jn = from->lruFromJmp) && (jn->to == to) && (jn->jmp == jmp)) {
+        GN_ASSERT(jn->from == from);
+    }
+    else {
+        UInt idx = jnHashIdx(from, jmp, to, allJns.capacity);
+        jn = allJns.table[idx];
+
+        while(jn != NULL) {
+            if ((jn->from == from) && (jn->jmp == jmp) && (jn->to == to))
+                break;
+            jn = jn->next_hash;
+        }
+
+        if (jn == NULL)
+            jn = newJumpNode(from, jmp, to);
+
+        from->lruFromJmp = jn;
+        to->lruToJmp = jn;
+    }
+
+    return jn;
+}
+
+
+void GN_(initJumpTable)()
+{
+    initJumpTable(&allJns);
+}
+
+
+static HChar jmpstr[32];
+const HChar* GN_(getJumpStr)(GnJumpKind jk, Bool isConditional)
+{
+    if (isConditional == True)
+        VG_(sprintf)(jmpstr, "Cond-");
+    else
+        jmpstr[0] = '\0';
+
+    switch(jk) {
+    case jk_None:
+        VG_(sprintf)(jmpstr + VG_(strlen)(jmpstr), "Fall-through");
+        break;
+    case jk_Call:
+        VG_(sprintf)(jmpstr + VG_(strlen)(jmpstr), "Call");
+        break;
+    case jk_Return:
+        VG_(sprintf)(jmpstr + VG_(strlen)(jmpstr), "Return");
+        break;
+    case jk_Jump:
+        VG_(sprintf)(jmpstr + VG_(strlen)(jmpstr), "Jump");
+        break;
+    case jk_Other:
+        VG_(sprintf)(jmpstr + VG_(strlen)(jmpstr), "Other");
+        break;
+    default:
+        tl_assert(0);
+    }
+
+    return jmpstr;
+}
diff --git a/gengrind/gn_jumps.h b/gengrind/gn_jumps.h
new file mode 100644
index 000000000..37df270e1
--- /dev/null
+++ b/gengrind/gn_jumps.h
@@ -0,0 +1,60 @@
+#ifndef GN_JUMPS_H
+#define GN_JUMPS_H
+
+#include "gn.h"
+
+
+//-------------------------------------------------------------------------------------------------
+/** Jump related type definitions **/
+
+enum _GnJumpKind {
+    /* The types of control flow changes that can happen between
+     * execution of two BBs in a thread.
+     */
+
+    jk_None = 0,   /* no explicit change by a guest instruction */
+    jk_Jump,       /* regular jump */
+    jk_Call,
+    jk_Return,
+    jk_CondJump,   /* conditional jump taken (only used as JumpNode type) */
+    jk_Other,
+};
+
+
+struct _JumpNode {
+    /* JmpCall node
+     * for subroutine call (from->bb->jmp_addr => to->bb->addr)
+     *
+     * Each BB has at most one CALL instruction. The list of JumpNode
+     * from this call is a pointer to the list head (stored in BB_node),
+     * and <next_from> in the JumpNode struct.
+     *
+     * For fast lookup, JumpNodes are reachable with a hash table, keyed by
+     * the (from_bb_node,to) pair. <next_hash> is used for the JumpNode chain
+     * of one hash table entry.
+     */ 
+
+    GnJumpKind jmpkind;  /* jk_Call, jk_Jump, jk_CondJump */
+    JumpNode* next_hash; /* for hash entry chain */
+    JumpNode* next_from; /* next JumpNode from a BB */
+    BBInfo *from, *to;   /* call arc from/to this BB */
+    UInt jmp;            /* jump no. in source */
+};
+
+
+struct _JumpTable {
+    UInt capacity;
+    UInt entries;
+    JumpNode **table;
+    JumpNode *spontaneous;
+};
+
+
+//-------------------------------------------------------------------------------------------------
+/** Jump related function definitions **/
+
+JumpNode* GN_(getJumpNode)(BBInfo *from, UInt jmp, BBInfo *to);
+void GN_(initJumpTable)(void);
+const HChar* GN_(getJumpStr)(GnJumpKind jk, Bool isConditional);
+
+#endif
diff --git a/gengrind/gn_main.c b/gengrind/gn_main.c
new file mode 100644
index 000000000..7c70f7fe1
--- /dev/null
+++ b/gengrind/gn_main.c
@@ -0,0 +1,258 @@
+
+/*--------------------------------------------------------------------*/
+/*--- Gengrind: The event generation Valgrind tool.      gn_main.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Gengrind, the event generation Valgrind tool
+
+   Copyright (C) 2017-2018 Michael Lui
+      mike.d.lui@gmail.com
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+/* N.B. much of the function tracking code throughout is based on
+ * Callgrind's implementation.
+ * Most modifications to the Callgrind base are removing unused
+ * functionality or stylistic changes. */
+
+#include "gn.h"
+#include "gn_ipc.h"
+#include "gn_clo.h"
+#include "gn_bb.h"
+#include "gn_events.h"
+#include "gn_threads.h"
+#include "gn_crq.h"
+#include "gn_callstack.h"
+#include "gn_jumps.h"
+#include "gn_debug.h"
+
+
+static void gn_post_clo_init(void)
+{
+    if (GN_(clo).gen_fn) {
+        GN_(clo).bbinfo_needed = True;
+    }
+
+    GN_(initIPC)();
+    GN_(initializeThreadState)();
+
+    if (GN_(clo).gen_fn == True) {
+        GN_(initBB)();
+        GN_(initCallStack)();
+        GN_(initJumpTable)();
+        GN_(lastJmpsPassed) = 0;
+    }
+
+    /* start generating events right away or not */
+    if (GN_(clo).start_collect_func == NULL)
+        GN_(afterStartFunc) = True;
+    else
+        GN_(afterStartFunc) = False;
+}
+
+
+static
+IRSB* gn_instrument ( VgCallbackClosure* closure,
+                      IRSB* obb,
+                      const VexGuestLayout* layout,
+                      const VexGuestExtents* vge,
+                      const VexArchInfo* archinfo_host,
+                      IRType gWordTy, IRType hWordTy )
+{
+    /* Each uninstrumented basic block (OBB) is passed to this function.
+     * An instrumented basic block (NBB) is generated from OBB and then passed
+     * back to Valgrind to use for execution.
+     *
+     * Each VEX IR statement (see libvex_ir.h) in the OBB is iterated upon,
+     * and metadata is temporarily stored, e.g. memory addresses.
+     * After each iteration, the VEX IR stmt is placed in the NBB.
+     *
+     * Upon reaching a possible Exit or the end of the OBB, all the temporary
+     * metadata captured up to that point is flushed before adding that VEX IR
+     * stmt.
+     * Flushing happens by inserting additional IR to write all the captured
+     * metadata to a buffer, and then resetting the metadata.
+     * Resetting metadata should take place during a flush.
+     *
+     * Some metadata needs to be available before any other VEX IR is inserted
+     * into the NBB. In this case, instrumentation will be inserted before
+     * iteration of the OBB.
+     */
+
+    if (gWordTy != hWordTy) {
+        // We don't currently support this case.
+        // From 'pub_tool_tooliface.h' 20171026:
+        // "They [gWordTy/hWordTy] will by [be] either Ity_I32 or Ity_I64.
+        // So far we have never built a cross-architecture
+        // Valgrind so they should always be the same."
+        VG_(tool_panic)("host/guest word size mismatch");
+    }
+
+    // No instrumentation if it is switched off
+    if (!GN_(clo).enable_instrumentation) {
+        GN_DEBUG(5, "instrument(BB %#lx) [Instrumentation OFF]\n",
+                  (Addr)closure->readdr);
+        return obb;
+    }
+
+    BBState bbState;
+    Int i = GN_(initBBState)(&bbState, obb, hWordTy);
+
+    GN_DEBUG(3, "+ instrument(BB %#lx)(%d)\n", (Addr)closure->readdr, bbState.bbInfo->uid);
+
+    // pre-BB instrumentation
+    {
+        if (GN_(clo).gen_fn == True)
+            GN_(add_TrackFns)(&bbState); // Function call/return tracking
+
+        if (GN_(clo).gen_sync == True)
+            GN_(add_TrackSyncs)(&bbState); // Need to setup thread context instrumentation
+    }
+
+    // BB instrumentation
+    Int curr_instr_idx = -1;
+    Int prev_flushed_idx = -1;
+
+    for (/*use current i*/; i < obb->stmts_used; ++i) {
+        const IRStmt *st = obb->stmts[i];
+        GN_ASSERT(isFlatIRStmt(st));
+
+        switch (st->tag) {
+        case Ist_NoOp:
+        case Ist_AbiHint:
+        case Ist_Put:
+        case Ist_PutI:
+        case Ist_MBE:
+            break;
+        case Ist_IMark:
+            curr_instr_idx = i;
+            GN_(addEvent_Instr)(&bbState, st);
+            break;
+        case Ist_WrTmp:
+            switch (st->Ist.WrTmp.data->tag) {
+            case Iex_Load:
+                GN_(addEvent_Memory_Load)(&bbState, st);
+                break;
+            case Iex_Unop:
+            case Iex_Binop:
+            case Iex_Triop:
+            case Iex_Qop:
+                GN_(addEvent_Compute)(&bbState, st);
+                break;
+            default:
+                break;
+            }
+            break;
+        case Ist_Store:
+            GN_(addEvent_Memory_Store)(&bbState, st);
+            break;
+        case Ist_StoreG:
+            GN_(addEvent_Memory_Guarded_Store)(&bbState, st);
+            break;
+        case Ist_LoadG:
+            GN_(addEvent_Memory_Guarded_Load)(&bbState, st);
+            break;
+        case Ist_Dirty:
+            GN_(addEvent_Dirty)(&bbState, st);
+            break;
+        case Ist_CAS:
+            GN_(addEvent_CAS)(&bbState, st);
+            break;
+        case Ist_LLSC:
+            GN_(addEvent_LLSC)(&bbState, st);
+            break;
+        case Ist_Exit:
+            GN_(addEvent_Exit)(&bbState, st);
+
+            /* gather instrumentation before any basic block exits */
+            GN_(Flush) f = {GN_FLUSH_EXIT_ST, curr_instr_idx, i};
+            GN_(flushEvents)(&bbState, prev_flushed_idx, f);
+            prev_flushed_idx = i;
+            break;
+        default:
+            tl_assert(0);
+            break;
+        } //end switch
+
+        GN_DEBUGIF(5) {
+            VG_(printf)("   pass  ");
+            ppIRStmt(st);
+            VG_(printf)("\n");
+        }
+    } //end foreach statement
+
+    // post-BB instrumentation
+    {
+        GN_(Flush) f = {GN_FLUSH_BB_END, -1, -1};
+        GN_(flushEvents)(&bbState, prev_flushed_idx, f);
+        GN_(addEvent_BBEnd)(&bbState);
+    }
+
+    return bbState.nbb;
+}
+
+static void gn_fini(Int exitcode)
+{
+    GN_(termIPC)();
+
+    finishCallstack();
+}
+
+static void gn_pre_clo_init(void)
+{
+    VG_(details_name)            ("Gengrind");
+    VG_(details_version)         (NULL);
+    VG_(details_description)     ("an event generation Valgrind tool");
+    VG_(details_copyright_author)(
+        "Copyright (C) 2017-2018, and GNU GPL'd, by Michael Lui.");
+    VG_(details_bug_reports_to)  (VG_BUGS_TO);
+
+    VG_(details_avg_translation_sizeB) (500);
+
+    /* Following example set by Callgrind, to make analysis easier */
+    VG_(clo_vex_control).iropt_unroll_thresh = 0;   // cannot be overriden.
+    VG_(clo_vex_control).guest_chase_thresh = 0;    // cannot be overriden.
+
+    VG_(basic_tool_funcs)        (gn_post_clo_init,
+                                  gn_instrument,
+                                  gn_fini);
+
+    VG_(needs_command_line_options)(GN_(processCmdLineOption),
+                                    NULL, NULL);
+
+    /* Track when a new VG-level thread is created or destroyed
+     * This is needed, for example, to generate unique-id's for each thread */
+    VG_(track_pre_thread_ll_create)(GN_(preVGThreadCreate));
+    VG_(track_pre_thread_ll_exit)(GN_(preVGThreadExit));
+
+    VG_(needs_client_requests)(GN_(handleClientRequest));
+    //VG_(track_start_client_code)();
+
+    VG_(track_pre_deliver_signal)(GN_(preDeliverSignal));
+    VG_(track_post_deliver_signal)(GN_(postDeliverSignal));
+
+    GN_(setCloDefaults)();
+}
+
+VG_DETERMINE_INTERFACE_VERSION(gn_pre_clo_init)
+
+/*--------------------------------------------------------------------*/
+/*--- end                                                          ---*/
+/*--------------------------------------------------------------------*/
diff --git a/gengrind/gn_sync.h b/gengrind/gn_sync.h
new file mode 100644
index 000000000..ff1d33d44
--- /dev/null
+++ b/gengrind/gn_sync.h
@@ -0,0 +1,57 @@
+#ifndef GN_SYNC_INTERCEPTS_H
+#define GN_SYNC_INTERCEPTS_H
+
+#include "valgrind.h"
+
+typedef enum _Vg_GnTClientRequest Vg_GnTClientRequest;
+enum _Vg_GnTClientRequest {
+    VG_USERREQ__GN_INIT = VG_USERREQ_TOOL_BASE('G', 'N'),
+    VG_USERREQ__GN_DISABLE_EVENTS,
+    VG_USERREQ__GN_ENABLE_EVENTS,
+
+    //---------------------------------------------------------------------------------------------
+    /** pthread client requests **/
+    VG_USERREQ__GN_PTH_CREATE_PRE,
+    VG_USERREQ__GN_PTH_CREATE_POST,
+    VG_USERREQ__GN_PTH_JOIN_PRE,
+    VG_USERREQ__GN_PTH_JOIN_POST,
+
+    VG_USERREQ__GN_PTH_MUTEX_LOCK_PRE,
+    VG_USERREQ__GN_PTH_MUTEX_LOCK_POST,
+
+    VG_USERREQ__GN_PTH_MUTEX_UNLOCK_PRE,
+    VG_USERREQ__GN_PTH_MUTEX_UNLOCK_POST,
+
+    VG_USERREQ__GN_PTH_BARRIER_WAIT_PRE,
+    VG_USERREQ__GN_PTH_BARRIER_WAIT_POST,
+
+    VG_USERREQ__GN_PTH_COND_WAIT_PRE,
+    VG_USERREQ__GN_PTH_COND_WAIT_POST,
+
+    VG_USERREQ__GN_PTH_COND_SIGNAL_PRE,
+    VG_USERREQ__GN_PTH_COND_SIGNAL_POST,
+
+    VG_USERREQ__GN_PTH_COND_BROADCAST_PRE,
+    VG_USERREQ__GN_PTH_COND_BROADCAST_POST,
+
+    VG_USERREQ__GN_PTH_SPIN_LOCK_PRE,
+    VG_USERREQ__GN_PTH_SPIN_LOCK_POST,
+
+    VG_USERREQ__GN_PTH_SPIN_UNLOCK_PRE,
+    VG_USERREQ__GN_PTH_SPIN_UNLOCK_POST,
+};
+
+/* Disable event generation.
+ * This tool is only interested in generating events specific to a workload.
+ * When inside specific functions, such as pthread API calls,
+ * compute and memory events are specific to a synchronization implementation,
+ * and not the respective workload, so disable events until the function
+ * returns.*/
+#define GN_DISABLE_EVENTS()                                        \
+    VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_DISABLE_EVENTS, \
+                                    0, 0, 0, 0, 0)
+#define GN_ENABLE_EVENTS()                                         \
+    VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_ENABLE_EVENTS,  \
+                                    0, 0, 0, 0, 0)
+
+#endif
diff --git a/gengrind/gn_sync_intercepts.c b/gengrind/gn_sync_intercepts.c
new file mode 100644
index 000000000..10b18b283
--- /dev/null
+++ b/gengrind/gn_sync_intercepts.c
@@ -0,0 +1,57 @@
+
+//-------------------------------------------------------------------------------------------------
+/** Using Helgrind as an example **/
+
+#include "pub_tool_basics.h"
+#include "pub_tool_redir.h"
+#include "pub_tool_clreq.h"
+#include "gn_sync.h"
+
+#include <pthread.h>
+#include <stdio.h>
+
+#if defined(VGO_linux)
+#define PTH_FUNC(ret_ty, f, args...) \
+   ret_ty I_WRAP_SONAME_FNNAME_ZZ(VG_Z_LIBPTHREAD_SONAME,f)(args); \
+   ret_ty I_WRAP_SONAME_FNNAME_ZZ(VG_Z_LIBPTHREAD_SONAME,f)(args)
+#endif
+
+
+#define TRACE_PTH_FNS 0
+
+//-------------------------------------------------------------------------------------------------
+/** pthread_create **/
+/* ensure this has its own frame, so as to make it more distinguishable
+   in suppressions */
+    __attribute__((noinline))
+static int pthread_create_WRK(pthread_t *thread, const pthread_attr_t *attr,
+                              void *(*start) (void *), void *arg)
+{
+    int    ret;
+    OrigFn fn;
+
+    VALGRIND_GET_ORIG_FN(fn);
+
+    GN_DISABLE_EVENTS();
+    VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTH_CREATE_PRE,
+                                    0, 0, 0, 0, 0);
+    CALL_FN_W_WWWW(ret, fn, thread, attr, start, arg);
+    VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__GN_PTH_CREATE_POST,
+                                    0, 0, 0, 0, 0);
+    GN_ENABLE_EVENTS();
+
+    if (TRACE_PTH_FNS) {
+        fprintf(stderr, " :: pth_create -> %d >>\n", ret);
+    }
+    return ret;
+}
+#if defined(VGO_linux)
+   PTH_FUNC(int, pthreadZucreateZAZa, // pthread_create@*
+                 pthread_t *thread, const pthread_attr_t *attr,
+                 void *(*start) (void *), void *arg) {
+      return pthread_create_WRK(thread, attr, start, arg);
+   }
+#endif
+
+//-------------------------------------------------------------------
+/** pthread_join **/
diff --git a/gengrind/gn_threads.c b/gengrind/gn_threads.c
new file mode 100644
index 000000000..5f60d4fb9
--- /dev/null
+++ b/gengrind/gn_threads.c
@@ -0,0 +1,178 @@
+#include "gn_threads.h"
+#include "gn_events.h"
+#include "gn_ipc.h"
+#include "gn_debug.h"
+
+#define GN_MAX_THREADS 500
+// max threads 'running' at a given time
+// same as MAX_THREADS_DEFAULT
+
+static ThreadId threadIdCounter;
+static ThreadId threadIdMap[GN_MAX_THREADS];
+static ThreadState threadStateTable[GN_MAX_THREADS];
+static ThreadId spawnerThread;
+static Bool isInSyncCall;
+
+ThreadId GN_(lastTid); // ThreadID of the last BB
+ThreadId GN_(currentTid);
+
+//-------------------------------------------------------------------------------------------------
+/** Helper function definitions **/
+
+static inline ThreadId getUTID(ThreadId tid)
+{
+    GN_ASSERT(threadIdMap[tid] != VG_INVALID_THREADID);
+    return threadIdMap[tid];
+}
+
+static void switchThread(ThreadId tid)
+{
+    ThreadId utid = getUTID(tid);
+    if (GN_(currentTid) == utid)
+        return;
+
+    /* save current state */
+    threadStateTable[GN_(currentTid)].lastJmpsPassed = GN_(lastJmpsPassed);
+    threadStateTable[GN_(currentTid)].isInSyncCall = isInSyncCall;
+    threadStateTable[GN_(currentTid)].eventGenerationEnabled = GN_(EventGenerationEnabled);
+
+    /* restore previous state */
+    GN_(lastJmpsPassed) = threadStateTable[utid].lastJmpsPassed;
+    GN_(currentTid) = utid;
+    isInSyncCall = threadStateTable[utid].isInSyncCall;
+    GN_(EventGenerationEnabled) = threadStateTable[GN_(currentTid)].eventGenerationEnabled;
+}
+
+
+//-------------------------------------------------------------------------------------------------
+/** External function definitions **/
+
+void GN_(preVGThreadCreate)(ThreadId parent, ThreadId child)
+{
+    if (GN_(clo).gen_sync == False)
+        return;
+
+    /* Create a new mapping from child TID -> reported TID
+     *
+     * This mapping is required because VG reuses exited thread ids,
+     * and each new thread should not have its stats conflated with
+     * previous threads with the same ID
+     * 
+     * VG thread ids are expected to count up, starting from 1 */
+    GN_ASSERT(threadIdCounter <= GN_MAX_UNIQUE_THREADS);
+    threadIdMap[child] = ++threadIdCounter; // expects first thread to be "1"
+
+    /* The last thread to make a thread spawn call (pthread_create)
+     * should be saved.
+     * Now send the thread spawn event with the child unique id */
+    GN_ASSERT(spawnerThread == parent);
+    GN_ASSERT(GN_(lastTid) == parent);
+
+    SyncID spawnData;
+    spawnData = threadIdMap[child];
+
+    GN_(flush_Sync)((UChar)SGLPRIM_SYNC_CREATE, &spawnData, 1);
+}
+
+
+void GN_(preVGThreadExit)(ThreadId quitTid)
+{
+    if (GN_(clo).gen_sync == False)
+        return;
+
+    GN_ASSERT(threadIdCounter > quitTid);
+
+    SyncID exitData;
+    exitData = threadIdMap[quitTid];
+
+    /* TODO(soonish)
+     * MDL20180304 is sending the manner of exit (e.g. join) enough,
+     * or do we walso send a SGLPRIM_SYNC_EXIT? */
+}
+
+
+void GN_(initializeThreadState)(void)
+{
+    /* initialize Thread IDs */
+    threadIdCounter = 0;
+
+    for (UInt i=0; i<GN_MAX_THREADS; ++i)
+        threadIdMap[i] = VG_INVALID_THREADID;
+
+    spawnerThread = VG_INVALID_THREADID;
+    GN_(lastTid) = VG_INVALID_THREADID;
+
+    /* initialize per-thread state variables */
+    for (UInt i=0; i<GN_MAX_THREADS; ++i) {
+        threadStateTable[i].isInSyncCall = False;
+        threadStateTable[i].eventGenerationEnabled = False;
+    }
+    isInSyncCall = False;
+}
+
+
+void GN_(preDeliverSignal)(ThreadId tid, Int sigNum,
+                           __attribute__((unused)) Bool altStack)
+{
+    switchThread(tid);
+    GN_DEBUG(1, "Pre Signal %d", sigNum);
+}
+
+
+void GN_(postDeliverSignal)(ThreadId tid, Int sigNum)
+{
+    switchThread(tid);
+    GN_DEBUG(1, "Post Signal %d", sigNum);
+}
+
+
+void GN_(checkSwitchThread)(void)
+{
+    /* expected to be called at beginning of BB */
+
+    ThreadId tid = VG_(get_running_tid)();
+    if (GN_(lastTid) != tid) {
+
+        GN_ASSERT(GN_(currEv) < GN_(endEv));
+
+        /* add event */
+        SglEvVariant *slot = GN_(currEv);
+        slot->tag = SGL_SYNC_TAG;
+        slot->sync.type = SGLPRIM_SYNC_SWAP;
+        slot->sync.data[0] = threadIdMap[tid];
+
+        /* increment event slot */
+        ++GN_(currEv);
+        ++*GN_(usedEv);
+        if (GN_(currEv) == GN_(endEv))
+            GN_(flushCurrAndSetNextBuffer)();
+
+        /* swap contexts */
+        switchThread(tid);
+
+        GN_(lastTid) = tid;
+    }
+}
+
+void GN_(setInSyncCall)(ThreadId tid)
+{
+    if (GN_(currentTid) != tid)
+        switchThread(tid);
+    isInSyncCall = True;
+    GN_(updateEventGeneration)();
+}
+
+
+void GN_(resetInSyncCall)(ThreadId tid)
+{
+    if (GN_(currentTid) != tid)
+        switchThread(tid);
+    isInSyncCall = False;
+    GN_(updateEventGeneration)();
+}
+
+
+Bool GN_(isInSyncCall)(void)
+{
+    return isInSyncCall;
+}
diff --git a/gengrind/gn_threads.h b/gengrind/gn_threads.h
new file mode 100644
index 000000000..4cd1ea7ef
--- /dev/null
+++ b/gengrind/gn_threads.h
@@ -0,0 +1,40 @@
+#ifndef GN_THREADS_H
+#define GN_THREADS_H
+
+#include "gn.h"
+#include "pub_tool_threadstate.h"
+
+#define GN_MAX_UNIQUE_THREADS 4096
+// total threads created over span of target application
+
+extern ThreadId GN_(lastTid);
+extern ThreadId GN_(currentTid);
+
+//-------------------------------------------------------------------------------------------------
+/** Callstack-tracking type definitions **/
+
+typedef struct _ThreadState ThreadState;
+struct _ThreadState {
+    UInt lastJmpsPassed;
+    Bool isInSyncCall;
+    Bool eventGenerationEnabled;
+};
+
+
+//-------------------------------------------------------------------------------------------------
+/** Thread-tracking declarations **/
+
+void GN_(preVGThreadCreate)(ThreadId parent, ThreadId child);
+void GN_(preVGThreadExit)(ThreadId quitTid);
+void GN_(initializeThreadState)(void);
+
+void GN_(preDeliverSignal)(ThreadId tid, Int sigNum, Bool altStack);
+void GN_(postDeliverSignal)(ThreadId tid, Int sigNum);
+
+void GN_(checkSwitchThread)(void);
+
+Bool GN_(isInSyncCall)(void);
+void GN_(setInSyncCall)(ThreadId tid);
+void GN_(resetInSyncCall)(ThreadId tid);
+
+#endif
diff --git a/gengrind/tests/Makefile.am b/gengrind/tests/Makefile.am
new file mode 100644
index 000000000..972acc3ce
--- /dev/null
+++ b/gengrind/tests/Makefile.am
@@ -0,0 +1,5 @@
+
+include $(top_srcdir)/Makefile.tool-tests.am
+
+SUBDIRS = .
+DIST_SUBDIRS = .
diff --git a/sigrind/.ycm_extra_conf.py b/sigrind/.ycm_extra_conf.py
new file mode 100644
index 000000000..a9d50f65f
--- /dev/null
+++ b/sigrind/.ycm_extra_conf.py
@@ -0,0 +1,177 @@
+# This file is NOT licensed under the GPLv3, which is the license for the rest
+# of YouCompleteMe.
+#
+# Here's the license text for this file:
+#
+# This is free and unencumbered software released into the public domain.
+#
+# Anyone is free to copy, modify, publish, use, compile, sell, or
+# distribute this software, either in source code form or as a compiled
+# binary, for any purpose, commercial or non-commercial, and by any
+# means.
+#
+# In jurisdictions that recognize copyright laws, the author or authors
+# of this software dedicate any and all copyright interest in the
+# software to the public domain. We make this dedication for the benefit
+# of the public at large and to the detriment of our heirs and
+# successors. We intend this dedication to be an overt act of
+# relinquishment in perpetuity of all present and future rights to this
+# software under copyright law.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+# IN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR
+# OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,
+# ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
+# OTHER DEALINGS IN THE SOFTWARE.
+#
+# For more information, please refer to <http://unlicense.org/>
+
+import os
+import ycm_core
+
+# These are the compilation flags that will be used in case there's no
+# compilation database set (by default, one is not set).
+# CHANGE THIS LIST OF FLAGS. YES, THIS IS THE DROID YOU HAVE BEEN LOOKING FOR.
+flags = [
+'-Wall',
+'-Wextra',
+#'-Werror',
+#'-Wc++98-compat',
+#'-Wno-long-long',
+#'-Wno-variadic-macros',
+#'-fexceptions',
+#'-DNDEBUG',
+# THIS IS IMPORTANT! Without a "-std=<something>" flag, clang won't know which
+# language to use when compiling headers. So it will guess. Badly. So C++
+# headers will be compiled as C headers. You don't want that so ALWAYS specify
+# a "-std=<something>".
+# For a C project, you would set this to something like 'c99' instead of
+# 'c++11'.
+'-std=c99',
+# ...and the same thing goes for the magic -x option which specifies the
+# language that the files to be compiled are written in. This is mostly
+# relevant for c++ headers.
+# For a C project, you would set this to 'c' instead of 'c++'.
+'-x',
+'c',
+'-DVGO_linux',
+'-DVGA_amd64',
+'-isystem',
+'../BoostParts',
+'-isystem',
+# This path will only work on OS X, but extra paths that don't exist are not
+# harmful
+'/System/Library/Frameworks/Python.framework/Headers',
+'-isystem',
+'../llvm/include',
+'-isystem',
+'../llvm/tools/clang/include',
+'-I','.',
+'-I','..',
+'-I','../include',
+'-I','../coregrind',
+'-I','../VEX/pub',
+'-I','../VEX/priv',
+'-I','../inst/include/valgrind',
+'-I','../../',
+'-I','../../../../',
+'-I','../../../../../'
+]
+
+
+# Set this to the absolute path to the folder (NOT the file!) containing the
+# compile_commands.json file to use that instead of 'flags'. See here for
+# more details: http://clang.llvm.org/docs/JSONCompilationDatabase.html
+#
+# You can get CMake to generate this file for you by adding:
+#   set( CMAKE_EXPORT_COMPILE_COMMANDS 1 )
+# to your CMakeLists.txt file.
+#
+# Most projects will NOT need to set this to anything; you can just change the
+# 'flags' list of compilation flags. Notice that YCM itself uses that approach.
+compilation_database_folder = ''
+
+if os.path.exists( compilation_database_folder ):
+  database = ycm_core.CompilationDatabase( compilation_database_folder )
+else:
+  database = None
+
+SOURCE_EXTENSIONS = [ '.cpp', '.cxx', '.cc', '.c', '.m', '.mm' ]
+
+def DirectoryOfThisScript():
+  return os.path.dirname( os.path.abspath( __file__ ) )
+
+def MakeRelativePathsInFlagsAbsolute( flags, working_directory ):
+  if not working_directory:
+    return list( flags )
+  new_flags = []
+  make_next_absolute = False
+  path_flags = [ '-isystem', '-I', '-iquote', '--sysroot=' ]
+  for flag in flags:
+    new_flag = flag
+
+    if make_next_absolute:
+      make_next_absolute = False
+      if not flag.startswith( '/' ):
+        new_flag = os.path.join( working_directory, flag )
+
+    for path_flag in path_flags:
+      if flag == path_flag:
+        make_next_absolute = True
+        break
+
+      if flag.startswith( path_flag ):
+        path = flag[ len( path_flag ): ]
+        new_flag = path_flag + os.path.join( working_directory, path )
+        break
+
+    if new_flag:
+      new_flags.append( new_flag )
+  return new_flags
+
+
+def IsHeaderFile( filename ):
+  extension = os.path.splitext( filename )[ 1 ]
+  return extension in [ '.h', '.hxx', '.hpp', '.hh' ]
+
+
+def GetCompilationInfoForFile( filename ):
+  # The compilation_commands.json file generated by CMake does not have entries
+  # for header files. So we do our best by asking the db for flags for a
+  # corresponding source file, if any. If one exists, the flags for that file
+  # should be good enough.
+  if IsHeaderFile( filename ):
+    basename = os.path.splitext( filename )[ 0 ]
+    for extension in SOURCE_EXTENSIONS:
+      replacement_file = basename + extension
+      if os.path.exists( replacement_file ):
+        compilation_info = database.GetCompilationInfoForFile(
+          replacement_file )
+        if compilation_info.compiler_flags_:
+          return compilation_info
+    return None
+  return database.GetCompilationInfoForFile( filename )
+
+
+def FlagsForFile( filename, **kwargs ):
+  if database:
+    # Bear in mind that compilation_info.compiler_flags_ does NOT return a
+    # python list, but a "list-like" StringVec object
+    compilation_info = GetCompilationInfoForFile( filename )
+    if not compilation_info:
+      return None
+
+    final_flags = MakeRelativePathsInFlagsAbsolute(
+      compilation_info.compiler_flags_,
+      compilation_info.compiler_working_dir_ )
+
+  else:
+    relative_to = DirectoryOfThisScript()
+    final_flags = MakeRelativePathsInFlagsAbsolute( flags, relative_to )
+
+  return {
+    'flags': final_flags,
+    'do_cache': True
+  }
diff --git a/sigrind/Makefile.am b/sigrind/Makefile.am
new file mode 100644
index 000000000..08953ce5f
--- /dev/null
+++ b/sigrind/Makefile.am
@@ -0,0 +1,83 @@
+include $(top_srcdir)/Makefile.tool.am
+
+EXTRA_DIST =  
+
+#----------------------------------------------------------------------------
+# Headers, etc
+#----------------------------------------------------------------------------
+
+pkginclude_HEADERS = callgrind.h
+
+bin_SCRIPTS = 
+
+noinst_HEADERS = \
+	events.h \
+	global.h \
+	log_events.h
+
+
+#----------------------------------------------------------------------------
+# sigrind-<platform>
+#----------------------------------------------------------------------------
+
+noinst_PROGRAMS  = sigrind-@VGCONF_ARCH_PRI@-@VGCONF_OS@
+if VGCONF_HAVE_PLATFORM_SEC
+noinst_PROGRAMS += sigrind-@VGCONF_ARCH_SEC@-@VGCONF_OS@
+endif
+
+SIGRIND_SOURCES_COMMON = \
+	bb.c \
+	bbcc.c \
+	callstack.c \
+	clo.c \
+	context.c \
+	debug.c \
+	events.c \
+	fn.c \
+	jumps.c \
+	threads.c \
+	log_events.c \
+	sigil2_ipc.c \
+	sg_main.c 
+
+SIGRIND_CFLAGS_COMMON =  -I$(top_srcdir)/../../.. -I$(top_srcdir)/include -I$(top_srcdir)/VEX/pub
+
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_SOURCES      = \
+	$(SIGRIND_SOURCES_COMMON)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CPPFLAGS     = \
+	$(AM_CPPFLAGS_@VGCONF_PLATFORM_PRI_CAPS@)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CFLAGS       = \
+	$(AM_CFLAGS_@VGCONF_PLATFORM_PRI_CAPS@) $(SIGRIND_CFLAGS_COMMON)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_DEPENDENCIES = \
+	$(TOOL_DEPENDENCIES_@VGCONF_PLATFORM_PRI_CAPS@)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDADD        = \
+	$(TOOL_LDADD_@VGCONF_PLATFORM_PRI_CAPS@)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDFLAGS      = \
+	$(TOOL_LDFLAGS_@VGCONF_PLATFORM_PRI_CAPS@)
+sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LINK = \
+	$(top_builddir)/coregrind/link_tool_exe_@VGCONF_OS@ \
+	@VALT_LOAD_ADDRESS_PRI@ \
+	$(LINK) \
+	$(sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_CFLAGS) \
+	$(sigrind_@VGCONF_ARCH_PRI@_@VGCONF_OS@_LDFLAGS)
+
+if VGCONF_HAVE_PLATFORM_SEC
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_SOURCES      = \
+	$(SIGRIND_SOURCES_COMMON)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CPPFLAGS     = \
+	$(AM_CPPFLAGS_@VGCONF_PLATFORM_SEC_CAPS@)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CFLAGS       = \
+	$(AM_CFLAGS_@VGCONF_PLATFORM_SEC_CAPS@) $(SIGRIND_CFLAGS_COMMON)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_DEPENDENCIES = \
+	$(TOOL_DEPENDENCIES_@VGCONF_PLATFORM_SEC_CAPS@)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDADD        = \
+	$(TOOL_LDADD_@VGCONF_PLATFORM_SEC_CAPS@)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDFLAGS      = \
+	$(TOOL_LDFLAGS_@VGCONF_PLATFORM_SEC_CAPS@)
+sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LINK = \
+	$(top_builddir)/coregrind/link_tool_exe_@VGCONF_OS@ \
+	@VALT_LOAD_ADDRESS_SEC@ \
+	$(LINK) \
+	$(sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_CFLAGS) \
+	$(sigrind_@VGCONF_ARCH_SEC@_@VGCONF_OS@_LDFLAGS)
+endif
diff --git a/sigrind/bb.c b/sigrind/bb.c
new file mode 100644
index 000000000..ceea5b969
--- /dev/null
+++ b/sigrind/bb.c
@@ -0,0 +1,345 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                         bb.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+/*------------------------------------------------------------*/
+/*--- Basic block (BB) operations                          ---*/
+/*------------------------------------------------------------*/
+
+/* BB hash, resizable */
+bb_hash bbs;
+
+void CLG_(init_bb_hash)()
+{
+   Int i;
+
+   bbs.size    = 8437;
+   bbs.entries = 0;
+   bbs.table = (BB**) CLG_MALLOC("cl.bb.ibh.1",
+                                 bbs.size * sizeof(BB*));
+
+   for (i = 0; i < bbs.size; i++) bbs.table[i] = NULL;
+}
+
+bb_hash* CLG_(get_bb_hash)()
+{
+  return &bbs;
+}
+
+/* The hash stores BBs according to
+ * - ELF object (is 0 for code in anonymous mapping)
+ * - BB base as object file offset
+ */
+static __inline__
+UInt bb_hash_idx(obj_node* obj, PtrdiffT offset, UInt size)
+{
+  return (((Addr)obj) + offset) % size;
+}
+
+/* double size of bb table  */
+static
+void resize_bb_table(void)
+{
+    Int i, new_size, conflicts1 = 0, conflicts2 = 0;
+    BB **new_table, *curr, *next;
+    UInt new_idx;
+
+    new_size  = 2* bbs.size +3;
+    new_table = (BB**) CLG_MALLOC("cl.bb.rbt.1",
+                                  new_size * sizeof(BB*));
+ 
+    for (i = 0; i < new_size; i++)
+      new_table[i] = NULL;
+ 
+    for (i = 0; i < bbs.size; i++) {
+	if (bbs.table[i] == NULL) continue;
+ 
+	curr = bbs.table[i];
+	while (NULL != curr) {
+	    next = curr->next;
+
+	    new_idx = bb_hash_idx(curr->obj, curr->offset, new_size);
+
+	    curr->next = new_table[new_idx];
+	    new_table[new_idx] = curr;
+	    if (curr->next) {
+		conflicts1++;
+		if (curr->next->next)
+		    conflicts2++;
+	    }
+
+	    curr = next;
+	}
+    }
+
+    VG_(free)(bbs.table);
+
+
+    CLG_DEBUG(0, "Resize BB Hash: %u => %d (entries %u, conflicts %d/%d)\n",
+	     bbs.size, new_size,
+	     bbs.entries, conflicts1, conflicts2);
+
+    bbs.size  = new_size;
+    bbs.table = new_table;
+    CLG_(stat).bb_hash_resizes++;
+}
+
+
+/**
+ * Allocate new BB structure (including space for event type list)
+ * Not initialized:
+ * - instr_len, cost_count, instr[]
+ */
+static BB* new_bb(obj_node* obj, PtrdiffT offset,
+		  UInt instr_count, UInt cjmp_count, Bool cjmp_inverted)
+{
+   BB* bb;
+   UInt idx, size;
+
+   /* check fill degree of bb hash table and resize if needed (>80%) */
+   bbs.entries++;
+   if (10 * bbs.entries / bbs.size > 8)
+       resize_bb_table();
+
+   size = sizeof(BB) + instr_count * sizeof(InstrInfo)
+                     + (cjmp_count+1) * sizeof(CJmpInfo);
+   bb = (BB*) CLG_MALLOC("cl.bb.nb.1", size);
+   VG_(memset)(bb, 0, size);
+
+   bb->obj        = obj;
+   bb->offset     = offset;
+   
+   bb->instr_count = instr_count;
+   bb->cjmp_count  = cjmp_count;
+   bb->cjmp_inverted = cjmp_inverted;
+   bb->jmp         = (CJmpInfo*) &(bb->instr[instr_count]);
+   bb->instr_len   = 0;
+   bb->cost_count  = 0;
+   bb->sect_kind   = VG_(DebugInfo_sect_kind)(NULL, offset + obj->offset);
+   bb->fn          = 0;
+   bb->line        = 0;
+   bb->is_entry    = 0;
+   bb->bbcc_list   = 0;
+   bb->last_bbcc   = 0;
+
+   /* insert into BB hash table */
+   idx = bb_hash_idx(obj, offset, bbs.size);
+   bb->next = bbs.table[idx];
+   bbs.table[idx] = bb;
+
+   CLG_(stat).distinct_bbs++;
+
+#if CLG_ENABLE_DEBUG
+   CLG_DEBUGIF(3) {
+     VG_(printf)("  new_bb (instr %u, jmps %u, inv %s) [now %d]: ",
+		 instr_count, cjmp_count,
+		 cjmp_inverted ? "yes":"no",
+		 CLG_(stat).distinct_bbs);
+      CLG_(print_bb)(0, bb);
+      VG_(printf)("\n");
+   }
+#endif
+
+   CLG_(get_fn_node)(bb);
+
+   return bb;
+}
+
+
+/* get the BB structure for a BB start address */
+static __inline__
+BB* lookup_bb(obj_node* obj, PtrdiffT offset)
+{
+    BB* bb;
+    Int idx;
+
+    idx = bb_hash_idx(obj, offset, bbs.size);
+    bb = bbs.table[idx];
+
+    while(bb) {
+      if ((bb->obj == obj) && (bb->offset == offset)) break;
+      bb = bb->next;
+    }
+
+    CLG_DEBUG(5, "  lookup_bb (Obj %s, off %#lx): %p\n",
+              obj->name, (UWord)offset, bb);
+    return bb;
+}
+
+static __inline__
+obj_node* obj_of_address(Addr addr)
+{
+  obj_node* obj;
+  DebugInfo* di;
+  PtrdiffT offset;
+
+  di = VG_(find_DebugInfo)(addr);
+  obj = CLG_(get_obj_node)( di );
+
+  /* Update symbol offset in object if remapped */
+  /* FIXME (or at least check this) 2008 Feb 19: 'offset' is
+     only correct for text symbols, not for data symbols */
+  offset = di ? VG_(DebugInfo_get_text_bias)(di):0;
+  if (obj->offset != offset) {
+      Addr start = di ? VG_(DebugInfo_get_text_avma)(di) : 0;
+
+      CLG_DEBUG(0, "Mapping changed for '%s': %#lx -> %#lx\n",
+		obj->name, obj->start, start);
+
+      /* Size should be the same, and offset diff == start diff */
+      CLG_ASSERT( obj->size == (di ? VG_(DebugInfo_get_text_size)(di) : 0) );
+      CLG_ASSERT( obj->start - start == obj->offset - offset );
+      obj->offset = offset;
+      obj->start = start;
+  }
+
+  return obj;
+}
+
+/* Get the BB structure for a BB start address.
+ * If the BB has to be created, the IRBB is needed to
+ * compute the event type list for costs, and seen_before is
+ * set to False. Otherwise, seen_before is set to True.
+ *
+ * BBs are never discarded. There are 2 cases where this function
+ * is called from CLG_(instrument)() and a BB already exists:
+ * - The instrumented version was removed from Valgrinds TT cache
+ * - The ELF object of the BB was unmapped and mapped again.
+ *   This involves a possibly different address, but is handled by
+ *   looking up a BB keyed by (obj_node, file offset).
+ *
+ * bbIn==0 is possible for artificial BB without real code.
+ * Such a BB is created when returning to an unknown function.
+ */
+BB* CLG_(get_bb)(Addr addr, IRSB* bbIn, /*OUT*/ Bool *seen_before)
+{
+  BB*   bb;
+  obj_node* obj;
+  UInt n_instrs, n_jmps;
+  Bool cjmp_inverted = False;
+
+  CLG_DEBUG(5, "+ get_bb(BB %#lx)\n", addr);
+
+  obj = obj_of_address(addr);
+  bb = lookup_bb(obj, addr - obj->offset);
+
+  n_instrs = 0;
+  n_jmps = 0;
+  CLG_(collectBlockInfo)(bbIn, &n_instrs, &n_jmps, &cjmp_inverted);
+
+  *seen_before = bb ? True : False;
+  if (*seen_before) {
+    if (bb->instr_count != n_instrs) {
+      VG_(message)(Vg_DebugMsg, 
+		   "ERROR: BB Retranslation Mismatch at BB %#lx\n", addr);
+      VG_(message)(Vg_DebugMsg,
+		   "  new: Obj %s, Off %#lx, BBOff %#lx, Instrs %u\n",
+		   obj->name, (UWord)obj->offset,
+		   addr - obj->offset, n_instrs);
+      VG_(message)(Vg_DebugMsg,
+		   "  old: Obj %s, Off %#lx, BBOff %#lx, Instrs %u\n",
+		   bb->obj->name, (UWord)bb->obj->offset,
+		   (UWord)bb->offset, bb->instr_count);
+      CLG_ASSERT(bb->instr_count == n_instrs );
+    }
+    CLG_ASSERT(bb->cjmp_count == n_jmps );
+    CLG_(stat).bb_retranslations++;
+
+    CLG_DEBUG(5, "- get_bb(BB %#lx): seen before.\n", addr);
+    return bb;
+  }
+
+  bb = new_bb(obj, addr - obj->offset, n_instrs, n_jmps, cjmp_inverted);
+
+  CLG_DEBUG(5, "- get_bb(BB %#lx)\n", addr);
+
+  return bb;
+}
+
+/* Delete the BB info for the bb with unredirected entry-point
+   address 'addr'. */
+void CLG_(delete_bb)(Addr addr)
+{
+    BB  *bb, *bp;
+    Int idx, size;
+
+    obj_node* obj = obj_of_address(addr);
+    PtrdiffT offset = addr - obj->offset;
+
+    idx = bb_hash_idx(obj, offset, bbs.size);
+    bb = bbs.table[idx];
+
+    /* bb points at the current bb under consideration, and bp is the
+       one before. */
+    bp = NULL;
+    while(bb) {
+      if ((bb->obj == obj) && (bb->offset == offset)) break;
+      bp = bb;
+      bb = bb->next;
+    }
+
+    if (bb == NULL) {
+	CLG_DEBUG(3, "  delete_bb (Obj %s, off %#lx): NOT FOUND\n",
+		  obj->name, (UWord)offset);
+
+	/* we didn't find it.
+	 * this happens when callgrinds instrumentation mode
+	 * was off at BB translation time, ie. no BB was created.
+	 */
+	return;
+    }
+
+    /* unlink it from hash table */
+
+    if (bp == NULL) {
+       /* we found the first one in the list. */
+       tl_assert(bb == bbs.table[idx]);
+       bbs.table[idx] = bb->next;
+    } else {
+       tl_assert(bb != bbs.table[idx]);
+       bp->next = bb->next;
+    }
+
+    CLG_DEBUG(3, "  delete_bb (Obj %s, off %#lx): %p, BBCC head: %p\n",
+	      obj->name, (UWord)offset, bb, bb->bbcc_list);
+
+    if (bb->bbcc_list == 0) {
+	/* can be safely deleted */
+
+	/* Fill the block up with junk and then free it, so we will
+	   hopefully get a segfault if it is used again by mistake. */
+	size = sizeof(BB)
+	    + bb->instr_count * sizeof(InstrInfo)
+	    + (bb->cjmp_count+1) * sizeof(CJmpInfo);
+	VG_(memset)( bb, 0xAA, size );
+	CLG_FREE(bb);
+	return;
+    }
+    CLG_DEBUG(3, "  delete_bb: BB in use, can not free!\n");
+}
diff --git a/sigrind/bbcc.c b/sigrind/bbcc.c
new file mode 100644
index 000000000..01e30531d
--- /dev/null
+++ b/sigrind/bbcc.c
@@ -0,0 +1,872 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                       bbcc.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+#include "pub_tool_threadstate.h"
+
+/*------------------------------------------------------------*/
+/*--- BBCC operations                                      ---*/
+/*------------------------------------------------------------*/
+
+#define N_BBCC_INITIAL_ENTRIES  10437
+
+/* BBCC table (key is BB/Context), per thread, resizable */
+bbcc_hash current_bbccs;
+
+void CLG_(init_bbcc_hash)(bbcc_hash* bbccs)
+{
+   Int i;
+
+   CLG_ASSERT(bbccs != 0);
+
+   bbccs->size    = N_BBCC_INITIAL_ENTRIES;
+   bbccs->entries = 0;
+   bbccs->table = (BBCC**) CLG_MALLOC("cl.bbcc.ibh.1",
+                                      bbccs->size * sizeof(BBCC*));
+
+   for (i = 0; i < bbccs->size; i++) bbccs->table[i] = NULL;
+}
+
+void CLG_(copy_current_bbcc_hash)(bbcc_hash* dst)
+{
+  CLG_ASSERT(dst != 0);
+
+  dst->size    = current_bbccs.size;
+  dst->entries = current_bbccs.entries;
+  dst->table   = current_bbccs.table;
+}
+
+bbcc_hash* CLG_(get_current_bbcc_hash)()
+{
+  return &current_bbccs;
+}
+
+void CLG_(set_current_bbcc_hash)(bbcc_hash* h)
+{
+  CLG_ASSERT(h != 0);
+
+  current_bbccs.size    = h->size;
+  current_bbccs.entries = h->entries;
+  current_bbccs.table   = h->table;
+}
+
+/*
+ * Zero all costs of a BBCC
+ */
+void CLG_(zero_bbcc)(BBCC* bbcc)
+{
+  Int i;
+
+  CLG_ASSERT(bbcc->cxt != 0);
+  CLG_DEBUG(1, "  zero_bbcc: BB %#lx, Cxt %u "
+	   "(fn '%s', rec %u)\n", 
+	   bb_addr(bbcc->bb),
+	   bbcc->cxt->base_number + bbcc->rec_index,
+	   bbcc->cxt->fn[0]->name,
+	   bbcc->rec_index);
+
+  if ((bbcc->ecounter_sum ==0) &&
+      (bbcc->ret_counter ==0)) return;
+
+  for(i=0;i <= bbcc->bb->cjmp_count;i++) {
+    bbcc->jmp[i].ecounter = 0;
+  }
+  bbcc->ecounter_sum = 0;
+  bbcc->ret_counter = 0;
+}
+
+
+
+void CLG_(forall_bbccs)(void (*func)(BBCC*))
+{
+  BBCC *bbcc, *bbcc2;
+  int i, j;
+	
+  for (i = 0; i < current_bbccs.size; i++) {
+    if ((bbcc=current_bbccs.table[i]) == NULL) continue;
+    while (bbcc) {
+      /* every bbcc should have a rec_array */
+      CLG_ASSERT(bbcc->rec_array != 0);
+
+      for(j=0;j<bbcc->cxt->fn[0]->separate_recursions;j++) {
+	if ((bbcc2 = bbcc->rec_array[j]) == 0) continue;
+
+	(*func)(bbcc2);
+      }
+      bbcc = bbcc->next;
+    }
+  }
+}
+
+
+/* All BBCCs for recursion level 0 are inserted into a
+ * thread specific hash table with key
+ * - address of BB structure (unique, as never freed)
+ * - current context (includes caller chain)
+ * BBCCs for other recursion levels are in bbcc->rec_array.
+ *
+ * The hash is used in setup_bb(), i.e. to find the cost
+ * counters to be changed in the execution of a BB.
+ */
+
+static __inline__
+UInt bbcc_hash_idx(BB* bb, Context* cxt, UInt size)
+{
+   CLG_ASSERT(bb != 0);
+   CLG_ASSERT(cxt != 0);
+
+   return ((Addr)bb + (Addr)cxt) % size;
+}
+ 
+
+/* Lookup for a BBCC in hash.
+ */ 
+static
+BBCC* lookup_bbcc(BB* bb, Context* cxt)
+{
+   BBCC* bbcc = bb->last_bbcc;
+   UInt  idx;
+
+   /* check LRU */
+   if (bbcc->cxt == cxt) {
+       if (!CLG_(clo).separate_threads) {
+	   /* if we don't dump threads separate, tid doesn't have to match */
+	   return bbcc;
+       }
+       if (bbcc->tid == CLG_(current_tid)) return bbcc;
+   }
+
+   CLG_(stat).bbcc_lru_misses++;
+
+   idx = bbcc_hash_idx(bb, cxt, current_bbccs.size);
+   bbcc = current_bbccs.table[idx];
+   while (bbcc &&
+	  (bb      != bbcc->bb ||
+	   cxt     != bbcc->cxt)) {
+       bbcc = bbcc->next;
+   }
+   
+   CLG_DEBUG(2,"  lookup_bbcc(BB %#lx, Cxt %u, fn '%s'): %p (tid %u)\n",
+	    bb_addr(bb), cxt->base_number, cxt->fn[0]->name, 
+	    bbcc, bbcc ? bbcc->tid : 0);
+
+   CLG_DEBUGIF(2)
+     if (bbcc) CLG_(print_bbcc)(-2,bbcc);
+
+   return bbcc;
+}
+
+
+/* double size of hash table 1 (addr->BBCC) */
+static void resize_bbcc_hash(void)
+{
+    Int i, new_size, conflicts1 = 0, conflicts2 = 0;
+    BBCC** new_table;
+    UInt new_idx;
+    BBCC *curr_BBCC, *next_BBCC;
+
+    new_size = 2*current_bbccs.size+3;
+    new_table = (BBCC**) CLG_MALLOC("cl.bbcc.rbh.1",
+                                    new_size * sizeof(BBCC*));
+ 
+    for (i = 0; i < new_size; i++)
+      new_table[i] = NULL;
+ 
+    for (i = 0; i < current_bbccs.size; i++) {
+	if (current_bbccs.table[i] == NULL) continue;
+ 
+	curr_BBCC = current_bbccs.table[i];
+	while (NULL != curr_BBCC) {
+	    next_BBCC = curr_BBCC->next;
+
+	    new_idx = bbcc_hash_idx(curr_BBCC->bb,
+				    curr_BBCC->cxt,
+				    new_size);
+
+	    curr_BBCC->next = new_table[new_idx];
+	    new_table[new_idx] = curr_BBCC;
+	    if (curr_BBCC->next) {
+		conflicts1++;
+		if (curr_BBCC->next->next)
+		    conflicts2++;
+	    }
+
+	    curr_BBCC = next_BBCC;
+	}
+    }
+
+    VG_(free)(current_bbccs.table);
+
+
+    CLG_DEBUG(0,"Resize BBCC Hash: %u => %d (entries %u, conflicts %d/%d)\n",
+	     current_bbccs.size, new_size,
+	     current_bbccs.entries, conflicts1, conflicts2);
+
+    current_bbccs.size = new_size;
+    current_bbccs.table = new_table;
+    CLG_(stat).bbcc_hash_resizes++;
+}
+
+
+static __inline
+BBCC** new_recursion(int size)
+{
+    BBCC** bbccs;
+    int i;
+
+    bbccs = (BBCC**) CLG_MALLOC("cl.bbcc.nr.1", sizeof(BBCC*) * size);
+    for(i=0;i<size;i++)
+	bbccs[i] = 0;
+
+    CLG_DEBUG(3,"  new_recursion(size %d): %p\n", size, bbccs);
+
+    return bbccs;
+}
+  
+
+/*
+ * Allocate a new BBCC
+ *
+ * Uninitialized:
+ * cxt, rec_index, rec_array, next_bbcc, next1, next2
+ */
+static __inline__ 
+BBCC* new_bbcc(BB* bb)
+{
+   BBCC* bbcc;
+   Int i;
+
+   /* We need cjmp_count+1 JmpData structs:
+    * the last is for the unconditional jump/call/ret at end of BB
+    */
+   bbcc = (BBCC*)CLG_MALLOC("cl.bbcc.nb.1",
+			    sizeof(BBCC) +
+			    (bb->cjmp_count+1) * sizeof(JmpData));
+   bbcc->bb  = bb;
+   bbcc->tid = CLG_(current_tid);
+
+   bbcc->ret_counter = 0;
+   bbcc->skipped = 0;
+   for(i=0; i<=bb->cjmp_count; i++) {
+       bbcc->jmp[i].ecounter = 0;
+       bbcc->jmp[i].jcc_list = 0;
+   }
+   bbcc->ecounter_sum = 0;
+
+   /* Init pointer caches (LRU) */
+   bbcc->lru_next_bbcc = 0;
+   bbcc->lru_from_jcc  = 0;
+   bbcc->lru_to_jcc  = 0;
+   
+   CLG_(stat).distinct_bbccs++;
+
+   CLG_DEBUG(3, "  new_bbcc(BB %#lx): %p (now %d)\n",
+	    bb_addr(bb), bbcc, CLG_(stat).distinct_bbccs);
+
+   return bbcc;
+}
+
+
+/**
+ * Inserts a new BBCC into hashes.
+ * BBCC specific items must be set as this is used for the hash
+ * keys:
+ *  fn     : current function
+ *  tid    : current thread ID
+ *  from   : position where current function is called from
+ *
+ * Recursion level doesn't need to be set as this is not included
+ * in the hash key: Only BBCCs with rec level 0 are in hashes.
+ */
+static
+void insert_bbcc_into_hash(BBCC* bbcc)
+{
+    UInt idx;
+    
+    CLG_ASSERT(bbcc->cxt != 0);
+
+    CLG_DEBUG(3,"+ insert_bbcc_into_hash(BB %#lx, fn '%s')\n",
+	     bb_addr(bbcc->bb), bbcc->cxt->fn[0]->name);
+
+    /* check fill degree of hash and resize if needed (>90%) */
+    current_bbccs.entries++;
+    if (100 * current_bbccs.entries / current_bbccs.size > 90)
+	resize_bbcc_hash();
+
+    idx = bbcc_hash_idx(bbcc->bb, bbcc->cxt, current_bbccs.size);
+    bbcc->next = current_bbccs.table[idx];
+    current_bbccs.table[idx] = bbcc;
+
+    CLG_DEBUG(3,"- insert_bbcc_into_hash: %u entries\n",
+	     current_bbccs.entries);
+}
+
+/* String is returned in a dynamically allocated buffer. Caller is
+   responsible for free'ing it. */
+static HChar* mangled_cxt(const Context* cxt, Int rec_index)
+{
+    Int i, p;
+
+    if (!cxt) return VG_(strdup)("cl.bbcc.mcxt", "(no context)");
+
+    /* Overestimate the number of bytes we need to hold the string. */
+    SizeT need = 20;   // rec_index + nul-terminator
+    for (i = 0; i < cxt->size; ++i)
+       need += VG_(strlen)(cxt->fn[i]->name) + 1;   // 1 for leading '
+
+    HChar *mangled = CLG_MALLOC("cl.bbcc.mcxt", need);
+    p = VG_(sprintf)(mangled, "%s", cxt->fn[0]->name);
+    if (rec_index >0)
+	p += VG_(sprintf)(mangled+p, "'%d", rec_index +1);
+    for(i=1;i<cxt->size;i++)
+	p += VG_(sprintf)(mangled+p, "'%s", cxt->fn[i]->name);
+
+    return mangled;
+}
+
+
+/* Create a new BBCC as a copy of an existing one,
+ * but with costs set to 0 and jcc chains empty.
+ *
+ * This is needed when a BB is executed in another context than
+ * the one at instrumentation time of the BB.
+ *
+ * Use cases:
+ *  rec_index == 0: clone from a BBCC with differing tid/cxt
+ *                  and insert into hashes
+ *  rec_index >0  : clone from a BBCC with same tid/cxt and rec_index 0
+ *                  don't insert into hashes
+ */
+static BBCC* clone_bbcc(BBCC* orig, Context* cxt, Int rec_index)
+{
+    BBCC* bbcc;
+
+    CLG_DEBUG(3,"+ clone_bbcc(BB %#lx, rec %d, fn %s)\n",
+	     bb_addr(orig->bb), rec_index, cxt->fn[0]->name);
+
+    bbcc = new_bbcc(orig->bb);
+
+    if (rec_index == 0) {
+
+      /* hash insertion is only allowed if tid or cxt is different */
+      CLG_ASSERT((orig->tid != CLG_(current_tid)) ||
+		(orig->cxt != cxt));
+
+      bbcc->rec_index = 0;
+      bbcc->cxt = cxt;
+      bbcc->rec_array = new_recursion(cxt->fn[0]->separate_recursions);
+      bbcc->rec_array[0] = bbcc;
+
+      insert_bbcc_into_hash(bbcc);
+    }
+    else {
+      if (CLG_(clo).separate_threads)
+	CLG_ASSERT(orig->tid == CLG_(current_tid));
+
+      CLG_ASSERT(orig->cxt == cxt);
+      CLG_ASSERT(orig->rec_array);
+      CLG_ASSERT(cxt->fn[0]->separate_recursions > rec_index);
+      CLG_ASSERT(orig->rec_array[rec_index] ==0);
+
+      /* new BBCC will only have differing recursion level */
+      bbcc->rec_index = rec_index;
+      bbcc->cxt = cxt;
+      bbcc->rec_array = orig->rec_array;
+      bbcc->rec_array[rec_index] = bbcc;
+    }
+
+    /* update list of BBCCs for same BB */
+    bbcc->next_bbcc = orig->bb->bbcc_list;
+    orig->bb->bbcc_list = bbcc;
+
+
+    CLG_DEBUGIF(3)
+      CLG_(print_bbcc)(-2, bbcc);
+
+    HChar *mangled_orig = mangled_cxt(orig->cxt, orig->rec_index);
+    HChar *mangled_bbcc = mangled_cxt(bbcc->cxt, bbcc->rec_index);
+    CLG_DEBUG(2,"- clone_BBCC(%p, %d) for BB %#lx\n"
+		"   orig %s\n"
+		"   new  %s\n",
+	     orig, rec_index, bb_addr(orig->bb),
+             mangled_orig,
+             mangled_bbcc);
+    CLG_FREE(mangled_orig);
+    CLG_FREE(mangled_bbcc);
+
+    CLG_(stat).bbcc_clones++;
+ 
+    return bbcc;
+};
+
+
+
+/* Get a pointer to the cost centre structure for given basic block
+ * address. If created, the BBCC is inserted into the BBCC hash.
+ * Also sets BB_seen_before by reference.
+ *
+ */ 
+BBCC* CLG_(get_bbcc)(BB* bb)
+{
+   BBCC* bbcc;
+
+   CLG_DEBUG(3, "+ get_bbcc(BB %#lx)\n", bb_addr(bb));
+
+   bbcc = bb->bbcc_list;
+
+   if (!bbcc) {
+     bbcc = new_bbcc(bb);
+
+     /* initialize BBCC */
+     bbcc->cxt       = 0;
+     bbcc->rec_array = 0;
+     bbcc->rec_index = 0;
+
+     bbcc->next_bbcc = bb->bbcc_list;
+     bb->bbcc_list = bbcc;
+     bb->last_bbcc = bbcc;
+
+     CLG_DEBUGIF(3)
+       CLG_(print_bbcc)(-2, bbcc);
+   }
+
+   CLG_DEBUG(3, "- get_bbcc(BB %#lx): BBCC %p\n",
+		bb_addr(bb), bbcc);
+
+   return bbcc;
+}
+
+
+/* Callgrind manages its own call stack for each thread.
+ * When leaving a function, a underflow can happen when
+ * Callgrind's tracing was switched on in the middle of
+ * a run, i.e. when Callgrind was not able to trace the
+ * call instruction.
+ * This function tries to reconstruct the original call.
+ * As we know the return address (the address following
+ * the CALL instruction), we can detect the function
+ * we return back to, but the original call site is unknown.
+ * We suppose a call site at return address - 1.
+ * (TODO: other heuristic: lookup info of instrumented BBs).
+ */
+static void handleUnderflow(BB* bb)
+{
+  /* RET at top of call stack */
+  BBCC* source_bbcc;
+  BB* source_bb;
+  Bool seen_before;
+  fn_node* caller;
+  int fn_number;
+  unsigned *pactive;
+
+  CLG_DEBUG(1,"  Callstack underflow !\n");
+
+  /* we emulate an old call from the function we return to
+   * by using (<return address> -1) */
+  source_bb = CLG_(get_bb)(bb_addr(bb)-1, 0, &seen_before);
+  source_bbcc = CLG_(get_bbcc)(source_bb);
+
+  /* seen_before can be true if RET from a signal handler */
+  if (!seen_before) {
+    source_bbcc->ecounter_sum = CLG_(current_state).collect ? 1 : 0;
+  }
+  else if (CLG_(current_state).collect)
+    source_bbcc->ecounter_sum++;
+  
+  /* Force a new top context, will be set active by push_cxt() */
+  CLG_(current_fn_stack).top--;
+  CLG_(current_state).cxt = 0;
+  caller = CLG_(get_fn_node)(bb);
+  CLG_(push_cxt)( caller );
+
+  if (!seen_before) {
+    /* set rec array for source BBCC: this is at rec level 1 */
+    source_bbcc->rec_array = new_recursion(caller->separate_recursions);
+    source_bbcc->rec_array[0] = source_bbcc;
+
+    CLG_ASSERT(source_bbcc->cxt == 0);
+    source_bbcc->cxt = CLG_(current_state).cxt;
+    insert_bbcc_into_hash(source_bbcc);
+  }
+  CLG_ASSERT(CLG_(current_state).bbcc);
+
+  /* correct active counts */
+  fn_number = CLG_(current_state).bbcc->cxt->fn[0]->number;
+  pactive = CLG_(get_fn_entry)(fn_number);
+  (*pactive)--;
+
+  /* This assertion is not correct for reentrant
+   * signal handlers */
+  /* CLG_ASSERT(*pactive == 0); */
+
+  CLG_(current_state).nonskipped = 0; /* we didn't skip this function */
+  /* back to current context */
+  CLG_(push_cxt)( CLG_(current_state).bbcc->cxt->fn[0] );
+  CLG_(push_call_stack)(source_bbcc, 0, CLG_(current_state).bbcc,
+		       (Addr)-1, False);
+}
+
+
+/*
+ * Helper function called at start of each instrumented BB to setup
+ * pointer to costs for current thread/context/recursion level
+ */
+
+VG_REGPARM(1)
+void CLG_(setup_bbcc)(BB* bb)
+{
+  BBCC *bbcc, *last_bbcc;
+  Bool  call_emulation = False, delayed_push = False, skip = False;
+  Addr sp;
+  BB* last_bb;
+  ThreadId tid;
+  ClgJumpKind jmpkind;
+  Bool isConditionalJump;
+  Int passed = 0, csp;
+  Bool ret_without_call = False;
+  Int popcount_on_return = 1;
+
+  CLG_DEBUG(3,"+ setup_bbcc(BB %#lx)\n", bb_addr(bb));
+
+  /* This is needed because thread switches can not reliable be tracked
+   * with callback CLG_(run_thread) only: we have otherwise no way to get
+   * the thread ID after a signal handler returns.
+   * This could be removed again if that bug is fixed in Valgrind.
+   * This is in the hot path but hopefully not to costly.
+   */
+  tid = VG_(get_running_tid)();
+#if 1
+  /* CLG_(switch_thread) is a no-op when tid is equal to CLG_(current_tid).
+   * As this is on the hot path, we only call CLG_(switch_thread)(tid)
+   * if tid differs from the CLG_(current_tid).
+   */
+  if (UNLIKELY(tid != CLG_(current_tid)))
+  {
+     CLG_(switch_thread)(tid);
+  }
+#else
+  CLG_ASSERT(VG_(get_running_tid)() == CLG_(current_tid));
+#endif
+
+  sp = VG_(get_SP)(tid);
+  last_bbcc = CLG_(current_state).bbcc;
+  last_bb = last_bbcc ? last_bbcc->bb : 0;
+
+  if (last_bb) {
+      passed = CLG_(current_state).jmps_passed;
+      CLG_ASSERT(passed <= last_bb->cjmp_count);
+      jmpkind = last_bb->jmp[passed].jmpkind;
+      isConditionalJump = (passed < last_bb->cjmp_count);
+
+      CLG_DEBUGIF(4) {
+      }
+  }
+  else {
+      jmpkind = jk_None;
+      isConditionalJump = False;
+  }
+
+  /* Manipulate JmpKind if needed, only using BB specific info */
+
+  csp = CLG_(current_call_stack).sp;
+
+  /* A return not matching the top call in our callstack is a jump */
+  if ( (jmpkind == jk_Return) && (csp >0)) {
+      Int csp_up = csp-1;      
+      call_entry* top_ce = &(CLG_(current_call_stack).entry[csp_up]);
+
+      /* We have a real return if
+       * - the stack pointer (SP) left the current stack frame, or
+       * - SP has the same value as when reaching the current function
+       *   and the address of this BB is the return address of last call
+       *   (we even allow to leave multiple frames if the SP stays the
+       *    same and we find a matching return address)
+       * The latter condition is needed because on PPC, SP can stay
+       * the same over CALL=b(c)l / RET=b(c)lr boundaries
+       */
+      if (sp < top_ce->sp) popcount_on_return = 0;
+      else if (top_ce->sp == sp) {
+	  while(1) {
+	      if (top_ce->ret_addr == bb_addr(bb)) break;
+	      if (csp_up>0) {
+		  csp_up--;
+		  top_ce = &(CLG_(current_call_stack).entry[csp_up]);
+		  if (top_ce->sp == sp) {
+		      popcount_on_return++;
+		      continue; 
+		  }
+	      }
+	      popcount_on_return = 0;
+	      break;
+	  }
+      }
+      if (popcount_on_return == 0) {
+	  jmpkind = jk_Jump;
+	  ret_without_call = True;
+      }
+  }
+
+  /* Should this jump be converted to call or pop/call ? */
+  if (( jmpkind != jk_Return) &&
+      ( jmpkind != jk_Call) && last_bb) {
+
+    /* We simulate a JMP/Cont to be a CALL if
+     * - jump is in another ELF object or section kind
+     * - jump is to first instruction of a function (tail recursion)
+     */
+    if (ret_without_call ||
+	/* This is for detection of optimized tail recursion.
+	 * On PPC, this is only detected as call when going to another
+	 * function. The problem is that on PPC it can go wrong
+	 * more easily (no stack frame setup needed)
+	 */
+#if defined(VGA_ppc32)
+	(bb->is_entry && (last_bb->fn != bb->fn)) ||
+#else
+	bb->is_entry ||
+#endif
+	(last_bb->sect_kind != bb->sect_kind) ||
+	(last_bb->obj->number != bb->obj->number)) {
+
+	CLG_DEBUG(1,"     JMP: %s[%s] to %s[%s]%s!\n",
+		  last_bb->fn->name, last_bb->obj->name,
+		  bb->fn->name, bb->obj->name,
+		  ret_without_call?" (RET w/o CALL)":"");
+
+	if (CLG_(get_fn_node)(last_bb)->pop_on_jump && (csp>0)) {
+
+	    call_entry* top_ce = &(CLG_(current_call_stack).entry[csp-1]);
+	    
+	    if (top_ce->jcc) {
+
+		CLG_DEBUG(1,"     Pop on Jump!\n");
+
+		/* change source for delayed push */
+		CLG_(current_state).bbcc = top_ce->jcc->from;
+		sp = top_ce->sp;
+		passed = top_ce->jcc->jmp;
+		CLG_(pop_call_stack)();
+	    }
+	    else {
+		CLG_ASSERT(CLG_(current_state).nonskipped != 0);
+	    }
+	}
+
+	jmpkind = jk_Call;
+	call_emulation = True;
+    }
+  }
+
+  if (jmpkind == jk_Call)
+    skip = CLG_(get_fn_node)(bb)->skip;
+
+  CLG_DEBUGIF(1) {
+    if (isConditionalJump)
+      VG_(printf)("Cond-");
+    switch(jmpkind) {
+    case jk_None:   VG_(printf)("Fall-through"); break;
+    case jk_Jump:   VG_(printf)("Jump"); break;
+    case jk_Call:   VG_(printf)("Call"); break;
+    case jk_Return: VG_(printf)("Return"); break;
+    default:        tl_assert(0);
+    }
+    VG_(printf)(" %08lx -> %08lx, SP %08lx\n",
+		last_bb ? bb_jmpaddr(last_bb) : 0,
+		bb_addr(bb), sp);
+  }
+
+  /* Handle CALL/RET and update context to get correct BBCC */
+  
+  if (jmpkind == jk_Return) {
+    
+    if ((csp == 0) || 
+	((CLG_(current_fn_stack).top > CLG_(current_fn_stack).bottom) &&
+	 ( *(CLG_(current_fn_stack).top-1)==0)) ) {
+
+      /* On an empty call stack or at a signal separation marker,
+       * a RETURN generates an call stack underflow.
+       */	
+      handleUnderflow(bb);
+      CLG_(pop_call_stack)();
+    }
+    else {
+	CLG_ASSERT(popcount_on_return >0);
+	CLG_(unwind_call_stack)(sp, popcount_on_return);
+    }
+  }
+  else {
+    Int unwind_count = CLG_(unwind_call_stack)(sp, 0);
+    if (unwind_count > 0) {
+      /* if unwinding was done, this actually is a return */
+      jmpkind = jk_Return;
+    }
+    
+    if (jmpkind == jk_Call) {
+      delayed_push = True;
+
+      csp = CLG_(current_call_stack).sp;
+      if (call_emulation && csp>0)
+	sp = CLG_(current_call_stack).entry[csp-1].sp;	
+
+    }
+  }
+  
+  /* Change new context if needed, taking delayed_push into account */
+  if ((delayed_push && !skip) || (CLG_(current_state).cxt == 0)) {
+    CLG_(push_cxt)(CLG_(get_fn_node)(bb));
+  }
+  CLG_ASSERT(CLG_(current_fn_stack).top > CLG_(current_fn_stack).bottom);
+  
+  /* If there is a fresh instrumented BBCC, assign current context */
+  bbcc = CLG_(get_bbcc)(bb);
+  if (bbcc->cxt == 0) {
+    CLG_ASSERT(bbcc->rec_array == 0);
+      
+    bbcc->cxt = CLG_(current_state).cxt;
+    bbcc->rec_array = 
+      new_recursion((*CLG_(current_fn_stack).top)->separate_recursions);
+    bbcc->rec_array[0] = bbcc;
+      
+    insert_bbcc_into_hash(bbcc);
+  }
+  else {
+    /* get BBCC with current context */
+    
+    /* first check LRU of last bbcc executed */
+    
+    if (last_bbcc) {
+      bbcc = last_bbcc->lru_next_bbcc;
+      if (bbcc &&
+	  ((bbcc->bb != bb) ||
+	   (bbcc->cxt != CLG_(current_state).cxt)))
+	bbcc = 0;
+    }
+    else
+      bbcc = 0;
+
+    if (!bbcc)
+      bbcc = lookup_bbcc(bb, CLG_(current_state).cxt);
+    if (!bbcc)
+      bbcc = clone_bbcc(bb->bbcc_list, CLG_(current_state).cxt, 0);
+    
+    bb->last_bbcc = bbcc;
+  }
+
+  /* save for fast lookup */
+  if (last_bbcc)
+    last_bbcc->lru_next_bbcc = bbcc;
+
+  if ((*CLG_(current_fn_stack).top)->separate_recursions >1) {
+    UInt level, idx;
+    fn_node* top = *(CLG_(current_fn_stack).top);
+
+    level = *CLG_(get_fn_entry)(top->number);
+
+    if (delayed_push && !skip) {
+      if (CLG_(clo).skip_direct_recursion) {
+        /* a call was detected, which means that the source BB != 0 */
+	CLG_ASSERT(CLG_(current_state).bbcc != 0);
+	/* only increment rec. level if called from different function */ 
+	if (CLG_(current_state).bbcc->cxt->fn[0] != bbcc->cxt->fn[0])
+	  level++;
+      }
+      else level++;
+    }
+    if (level> top->separate_recursions)
+      level = top->separate_recursions;
+
+    if (level == 0) {
+      /* can only happen if instrumentation just was switched on */
+      level = 1;
+      *CLG_(get_fn_entry)(top->number) = 1;
+    }
+
+    idx = level -1;
+    if (bbcc->rec_array[idx])
+      bbcc = bbcc->rec_array[idx];
+    else
+      bbcc = clone_bbcc(bbcc, CLG_(current_state).cxt, idx);
+
+    CLG_ASSERT(bbcc->rec_array[bbcc->rec_index] == bbcc);
+  }
+
+  if (delayed_push) {
+    if (!skip && CLG_(current_state).nonskipped) {
+      /* a call from skipped to nonskipped */
+      CLG_(current_state).bbcc = CLG_(current_state).nonskipped;
+      /* FIXME: take the real passed count from shadow stack */
+      passed = CLG_(current_state).bbcc->bb->cjmp_count;
+    }
+    CLG_(push_call_stack)(CLG_(current_state).bbcc, passed,
+			 bbcc, sp, skip);
+  }
+
+  if (CLG_(clo).collect_jumps && (jmpkind == jk_Jump)) {
+    
+    /* Handle conditional jumps followed, i.e. trace arcs
+     * This uses JCC structures, too */
+    
+    jCC* jcc = CLG_(get_jcc)(last_bbcc, passed, bbcc);
+    CLG_ASSERT(jcc != 0);
+    // Change from default, and check if already changed
+    if (jcc->jmpkind == jk_Call)
+      jcc->jmpkind = isConditionalJump ? jk_CondJump : jk_Jump;
+    else {
+	// FIXME: Why can this fail?
+	// CLG_ASSERT(jcc->jmpkind == jmpkind);
+    }
+    
+    jcc->call_counter++;
+    if (isConditionalJump)
+      CLG_(stat).jcnd_counter++;
+    else
+      CLG_(stat).jump_counter++;
+  }
+  
+  CLG_(current_state).bbcc = bbcc;
+  /* Even though this will be set in instrumented code directly before
+   * side exits, it needs to be set to 0 here in case an exception
+   * happens in first instructions of the BB */
+  CLG_(current_state).jmps_passed = 0;
+  
+  CLG_DEBUGIF(1) {
+    VG_(printf)("     ");
+    CLG_(print_bbcc_fn)(bbcc);
+    VG_(printf)("\n");
+  }
+  
+  CLG_DEBUG(3,"- setup_bbcc (BB %#lx): Cost %p (Len %u), Instrs %u (Len %u)\n",
+	   bb_addr(bb), bbcc->cost, bb->cost_count, 
+	   bb->instr_count, bb->instr_len);
+  CLG_DEBUGIF(3)
+    CLG_(print_cxt)(-8, CLG_(current_state).cxt, bbcc->rec_index);
+  CLG_DEBUG(3,"\n");
+  
+  CLG_(stat).bb_executions++;
+}
diff --git a/sigrind/callgrind.h b/sigrind/callgrind.h
new file mode 100644
index 000000000..6d980ad0c
--- /dev/null
+++ b/sigrind/callgrind.h
@@ -0,0 +1,363 @@
+
+/*
+   ----------------------------------------------------------------
+
+   Notice that the following BSD-style license applies to this one
+   file (callgrind.h) only.  The rest of Valgrind is licensed under the
+   terms of the GNU General Public License, version 2, unless
+   otherwise indicated.  See the COPYING file in the source
+   distribution for details.
+
+   ----------------------------------------------------------------
+
+   This file is part of callgrind, a valgrind tool for cache simulation
+   and call tree tracing.
+
+   Copyright (C) 2003-2015 Josef Weidendorfer.  All rights reserved.
+
+   Redistribution and use in source and binary forms, with or without
+   modification, are permitted provided that the following conditions
+   are met:
+
+   1. Redistributions of source code must retain the above copyright
+      notice, this list of conditions and the following disclaimer.
+
+   2. The origin of this software must not be misrepresented; you must
+      not claim that you wrote the original software.  If you use this
+      software in a product, an acknowledgment in the product
+      documentation would be appreciated but is not required.
+
+   3. Altered source versions must be plainly marked as such, and must
+      not be misrepresented as being the original software.
+
+   4. The name of the author may not be used to endorse or promote
+      products derived from this software without specific prior written
+      permission.
+
+   THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS
+   OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+   WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
+   ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY
+   DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+   DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE
+   GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
+   INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+   WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
+   NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+   SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+   ----------------------------------------------------------------
+
+   Notice that the above BSD-style license applies to this one file
+   (callgrind.h) only.  The entire rest of Valgrind is licensed under
+   the terms of the GNU General Public License, version 2.  See the
+   COPYING file in the source distribution for details.
+
+   ----------------------------------------------------------------
+*/
+
+#ifndef SIGRIND_H
+#define SIGRIND_H
+
+#include "valgrind.h"
+
+/* !! ABIWARNING !! ABIWARNING !! ABIWARNING !! ABIWARNING !!
+   This enum comprises an ABI exported by Valgrind to programs
+   which use client requests.  DO NOT CHANGE THE ORDER OF THESE
+   ENTRIES, NOR DELETE ANY -- add new ones at the end.
+
+   The identification ('C','T') for Callgrind has historical
+   reasons: it was called "Calltree" before. Besides, ('C','G') would
+   clash with cachegrind.
+ */
+
+typedef
+   enum {
+      VG_USERREQ__DUMP_STATS = VG_USERREQ_TOOL_BASE('C','T'),
+      VG_USERREQ__ZERO_STATS,
+      VG_USERREQ__TOGGLE_COLLECT,
+      VG_USERREQ__DUMP_STATS_AT,
+      VG_USERREQ__START_INSTRUMENTATION,
+      VG_USERREQ__STOP_INSTRUMENTATION,
+
+      VG_USERREQ__SIGIL_PTHREAD_CREATE_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_CREATE_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_JOIN_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_JOIN_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_LOCK_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_LOCK_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_UNLOCK_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_UNLOCK_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_BARRIER_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_BARRIER_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_CONDSIG_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_CONDSIG_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_LEAVE,
+      VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_ENTER,
+      VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_LEAVE,
+
+      VG_USERREQ__SIGIL_GOMP_LOCK_ENTER,
+      VG_USERREQ__SIGIL_GOMP_LOCK_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_UNLOCK_ENTER,
+      VG_USERREQ__SIGIL_GOMP_UNLOCK_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_BARRIER_ENTER,
+      VG_USERREQ__SIGIL_GOMP_BARRIER_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_ATOMICSTART_ENTER,
+      VG_USERREQ__SIGIL_GOMP_ATOMICSTART_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_ATOMICEND_ENTER,
+      VG_USERREQ__SIGIL_GOMP_ATOMICEND_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_CRITSTART_ENTER,
+      VG_USERREQ__SIGIL_GOMP_CRITSTART_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_CRITEND_ENTER,
+      VG_USERREQ__SIGIL_GOMP_CRITEND_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_ENTER,
+      VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_ENTER,
+      VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_SETLOCK_ENTER,
+      VG_USERREQ__SIGIL_GOMP_SETLOCK_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_UNSETLOCK_ENTER,
+      VG_USERREQ__SIGIL_GOMP_UNSETLOCK_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_ENTER,
+      VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_LEAVE,
+      VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_ENTER,
+      VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_LEAVE
+   } Vg_CallgrindClientRequest;
+
+/* Dump current state of cost centers, and zero them afterwards */
+#define CALLGRIND_DUMP_STATS                                    \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__DUMP_STATS,       \
+                                  0, 0, 0, 0, 0)
+
+/* Dump current state of cost centers, and zero them afterwards.
+   The argument is appended to a string stating the reason which triggered
+   the dump. This string is written as a description field into the
+   profile data dump. */
+#define CALLGRIND_DUMP_STATS_AT(pos_str)                        \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__DUMP_STATS_AT,    \
+                                  pos_str, 0, 0, 0, 0)
+
+/* Zero cost centers */
+#define CALLGRIND_ZERO_STATS                                    \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__ZERO_STATS,       \
+                                  0, 0, 0, 0, 0)
+
+/* Toggles collection state.
+   The collection state specifies whether the happening of events
+   should be noted or if they are to be ignored. Events are noted
+   by increment of counters in a cost center */
+#define CALLGRIND_TOGGLE_COLLECT                                \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__TOGGLE_COLLECT,   \
+                                  0, 0, 0, 0, 0)
+
+/* Start full callgrind instrumentation if not already switched on.
+   When cache simulation is done, it will flush the simulated cache;
+   this will lead to an artifical cache warmup phase afterwards with
+   cache misses which would not have happened in reality. */
+#define CALLGRIND_START_INSTRUMENTATION                              \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__START_INSTRUMENTATION, \
+                                  0, 0, 0, 0, 0)
+
+/* Stop full callgrind instrumentation if not already switched off.
+   This flushes Valgrinds translation cache, and does no additional
+   instrumentation afterwards, which effectivly will run at the same
+   speed as the "none" tool (ie. at minimal slowdown).
+   Use this to bypass Callgrind aggregation for uninteresting code parts.
+   To start Callgrind in this mode to ignore the setup phase, use
+   the option "--instr-atstart=no". */
+#define CALLGRIND_STOP_INSTRUMENTATION                               \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__STOP_INSTRUMENTATION,  \
+                                  0, 0, 0, 0, 0)
+
+/*------------------------------------------------------------*/
+/*---  Sigil handling for synchronization sys calls        ---*/
+/*------------------------------------------------------------*/
+#define SIGIL_PTHREAD_CREATE_ENTER(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CREATE_ENTER,     \
+                                  thr, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_CREATE_LEAVE(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CREATE_LEAVE,     \
+                                  thr, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_JOIN_ENTER(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_JOIN_ENTER,     \
+                                  thr, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_JOIN_LEAVE(thr) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_JOIN_LEAVE,     \
+                                  thr, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_LOCK_ENTER(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_LOCK_ENTER,     \
+                                  mut, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_LOCK_LEAVE(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_LOCK_LEAVE,     \
+                                  mut, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_UNLOCK_ENTER(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_UNLOCK_ENTER,     \
+                                  mut, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_UNLOCK_LEAVE(mut) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_UNLOCK_LEAVE,     \
+                                  mut, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_BARRIER_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_BARRIER_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_BARRIER_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_BARRIER_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_CONDWAIT_ENTER(cond, mtx) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_ENTER,     \
+                                  cond, mtx, 0, 0, 0)
+#define SIGIL_PTHREAD_CONDWAIT_LEAVE(cond, mtx) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_LEAVE,     \
+                                  cond, mtx, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_CONDSIG_ENTER(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDSIG_ENTER,     \
+                                  cond, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_CONDSIG_LEAVE(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDSIG_LEAVE,     \
+                                  cond, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_CONDBROAD_ENTER(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_ENTER,     \
+                                  cond, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_CONDBROAD_LEAVE(cond) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_LEAVE,     \
+                                  cond, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_SPINLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_SPINLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_PTHREAD_SPINUNLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_PTHREAD_SPINUNLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_LOCK_ENTER(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_LOCK_ENTER,     \
+                                  mutex, 0, 0, 0, 0)
+#define SIGIL_GOMP_LOCK_LEAVE(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_LOCK_LEAVE,     \
+                                  mutex, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_UNLOCK_ENTER(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_UNLOCK_ENTER,     \
+                                  mutex, 0, 0, 0, 0)
+#define SIGIL_GOMP_UNLOCK_LEAVE(mutex) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_UNLOCK_LEAVE,     \
+                                  mutex, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_BARRIER_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_BARRIER_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define SIGIL_GOMP_BARRIER_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_BARRIER_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_ATOMICSTART_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_ATOMICSTART_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_ATOMICSTART_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_ATOMICSTART_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_ATOMICEND_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_ATOMICEND_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_ATOMICEND_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_ATOMICEND_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_CRITSTART_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITSTART_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_CRITSTART_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITSTART_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_CRITEND_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITEND_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_CRITEND_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITEND_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_CRITNAMESTART_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_CRITNAMESTART_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_CRITNAMEEND_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_CRITNAMEEND_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_SETLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_SETLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_SETLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_SETLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_UNSETLOCK_ENTER(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_UNSETLOCK_ENTER,     \
+                                  lock, 0, 0, 0, 0)
+#define SIGIL_GOMP_UNSETLOCK_LEAVE(lock) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_UNSETLOCK_LEAVE,     \
+                                  lock, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_TEAMBARRIERWAIT_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define SIGIL_GOMP_TEAMBARRIERWAIT_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+
+#define SIGIL_GOMP_TEAMBARRIERWAITFINAL_ENTER(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_ENTER,     \
+                                  bar, 0, 0, 0, 0)
+#define SIGIL_GOMP_TEAMBARRIERWAITFINAL_LEAVE(bar) \
+  VALGRIND_DO_CLIENT_REQUEST_STMT(VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_LEAVE,     \
+                                  bar, 0, 0, 0, 0)
+
+#endif /* __CALLGRIND_H */
diff --git a/sigrind/callstack.c b/sigrind/callstack.c
new file mode 100644
index 000000000..6f5184b3c
--- /dev/null
+++ b/sigrind/callstack.c
@@ -0,0 +1,425 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                               ct_callstack.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+#include "log_events.h"
+#include "Core/PrimitiveEnums.h"
+
+/*------------------------------------------------------------*/
+/*--- Call stack, operations                               ---*/
+/*------------------------------------------------------------*/
+
+/* Stack of current thread. Gets initialized when switching to 1st thread.
+ *
+ * The artificial call stack is an array of call_entry's, representing
+ * stack frames of the executing program. 
+ * Array call_stack and call_stack_esp have same size and grow on demand.
+ * Array call_stack_esp holds SPs of corresponding stack frames.
+ *
+ */
+
+#define N_CALL_STACK_INITIAL_ENTRIES 500
+
+call_stack CLG_(current_call_stack);
+
+void CLG_(init_call_stack)(call_stack* s)
+{
+  CLG_ASSERT(s != 0);
+
+  s->size = N_CALL_STACK_INITIAL_ENTRIES;   
+  s->entry = (call_entry*) CLG_MALLOC("cl.callstack.ics.1",
+                                      s->size * sizeof(call_entry));
+  s->sp = 0;
+  s->entry[0].cxt = 0; /* for assertion in push_cxt() */
+}
+
+call_entry* CLG_(get_call_entry)(Int sp)
+{
+  CLG_ASSERT(sp <= CLG_(current_call_stack).sp);
+  return &(CLG_(current_call_stack).entry[sp]);
+}
+
+void CLG_(copy_current_call_stack)(call_stack* dst)
+{
+  CLG_ASSERT(dst != 0);
+
+  dst->size  = CLG_(current_call_stack).size;
+  dst->entry = CLG_(current_call_stack).entry;
+  dst->sp    = CLG_(current_call_stack).sp;
+}
+
+void CLG_(set_current_call_stack)(call_stack* s)
+{
+  CLG_ASSERT(s != 0);
+
+  CLG_(current_call_stack).size  = s->size;
+  CLG_(current_call_stack).entry = s->entry;
+  CLG_(current_call_stack).sp    = s->sp;
+}
+
+
+static __inline__
+void ensure_stack_size(Int i)
+{
+  call_stack *cs = &CLG_(current_call_stack);
+
+  if (i < cs->size) return;
+
+  cs->size *= 2;
+  while (i > cs->size) cs->size *= 2;
+
+  cs->entry = (call_entry*) VG_(realloc)("cl.callstack.ess.1",
+                                         cs->entry,
+					 cs->size * sizeof(call_entry));
+
+  CLG_(stat).call_stack_resizes++;
+ 
+  CLG_DEBUGIF(2)
+    VG_(printf)("        call stack enlarged to %u entries\n",
+		CLG_(current_call_stack).size);
+}
+
+
+/* Called when function entered nonrecursive */
+static void function_entered(fn_node* fn)
+{
+  CLG_ASSERT(fn != 0);
+
+  if ( (SGL_(clo).collect_func != NULL) && (VG_(strcmp)(fn->name, SGL_(clo).collect_func) == 0) )
+  {
+    VG_(umsg)("*********************************************\n");
+    VG_(umsg)("Entering %s: turning on event collection\n", fn->name);
+    VG_(umsg)("*********************************************\n");
+    SGL_(is_in_event_collect_func) = True;
+
+    // let Sigil2 know which thread this function starts in
+    SGL_(log_sync)(SGLPRIM_SYNC_SWAP, SGL_(active_tid), UNUSED_SYNC_DATA);
+  }
+  else if ( (SGL_(clo).start_collect_func != NULL) && (VG_(strcmp)(fn->name, SGL_(clo).start_collect_func) == 0) )
+  {
+    VG_(umsg)("*********************************************\n");
+    VG_(umsg)("Entering %s: turning on event collection\n", fn->name);
+    VG_(umsg)("*********************************************\n");
+    SGL_(is_in_event_collect_func) = True;
+
+    // let Sigil2 know which thread this function starts in
+    SGL_(log_sync)(SGLPRIM_SYNC_SWAP, SGL_(active_tid), UNUSED_SYNC_DATA);
+  }
+
+  /* send to sigil */
+  SGL_(log_fn_entry)(fn);
+
+#if CLG_ENABLE_DEBUG
+  if (fn->verbosity >=0) {
+    Int old = CLG_(clo).verbose;
+    CLG_(clo).verbose = fn->verbosity;
+    fn->verbosity = old;
+    VG_(message)(Vg_DebugMsg, 
+    "Entering %s: Verbosity set to %d\n",
+    fn->name, CLG_(clo).verbose);
+  }
+#endif
+}
+
+/* Called when function left (no recursive level active) */
+static void function_left(fn_node* fn)
+{
+  CLG_ASSERT(fn != 0);
+
+  /*send to sigil*/
+  SGL_(log_fn_leave)(fn);
+
+  if ( (SGL_(clo).collect_func != NULL) && (VG_(strcmp)(fn->name, SGL_(clo).collect_func) == 0) )
+  {
+    VG_(umsg)("*********************************************\n");
+    VG_(umsg)("Leaving %s: turning off event collection\n", fn->name);
+    VG_(umsg)("*********************************************\n");
+    SGL_(is_in_event_collect_func) = False;
+  }
+  else if ( (SGL_(clo).stop_collect_func != NULL) && (VG_(strcmp)(fn->name, SGL_(clo).stop_collect_func) == 0) )
+  {
+    VG_(umsg)("*********************************************\n");
+    VG_(umsg)("Leaving %s: turning off event collection\n", fn->name);
+    VG_(umsg)("*********************************************\n");
+    SGL_(is_in_event_collect_func) = False;
+  }
+
+#if CLG_ENABLE_DEBUG
+  if (fn->verbosity >=0) {
+    Int old = CLG_(clo).verbose;
+    CLG_(clo).verbose = fn->verbosity;
+    fn->verbosity = old;
+    VG_(message)(Vg_DebugMsg, 
+    "Leaving %s: Verbosity set back to %d\n",
+    fn->name, CLG_(clo).verbose);
+  }
+#endif
+}
+
+
+/* Push call on call stack.
+ *
+ * Increment the usage count for the function called.
+ * A jump from <from> to <to>, with <sp>.
+ * If <skip> is true, this is a call to a function to be skipped;
+ * for this, we set jcc = 0.
+ */
+void CLG_(push_call_stack)(BBCC* from, UInt jmp, BBCC* to, Addr sp, Bool skip)
+{
+    jCC* jcc;
+    UInt* pdepth;
+    call_entry* current_entry;
+    Addr ret_addr;
+
+    /* Ensure a call stack of size <current_sp>+1.
+     * The +1 is needed as push_cxt will store the
+     * context at [current_sp]
+     */
+    ensure_stack_size(CLG_(current_call_stack).sp +1);
+    current_entry = &(CLG_(current_call_stack).entry[CLG_(current_call_stack).sp]);
+
+    if (skip) {
+	jcc = 0;
+    }
+    else {
+	fn_node* to_fn = to->cxt->fn[0];
+
+	if (CLG_(current_state).nonskipped) {
+	    /* this is a jmp from skipped to nonskipped */
+	    CLG_ASSERT(CLG_(current_state).nonskipped == from);
+	}
+
+	/* As push_cxt() has to be called before push_call_stack if not
+	 * skipping, the old context should already be saved on the stack */
+	CLG_ASSERT(current_entry->cxt != 0);
+
+	jcc = CLG_(get_jcc)(from, jmp, to);
+	CLG_ASSERT(jcc != 0);
+
+	pdepth = CLG_(get_fn_entry)(to_fn->number);
+	if (CLG_(clo).skip_direct_recursion) {
+	    /* only increment depth if another function is called */
+	  if (jcc->from->cxt->fn[0] != to_fn) (*pdepth)++;
+	}
+	else (*pdepth)++;
+
+	if (*pdepth>1)
+	  CLG_(stat).rec_call_counter++;
+	
+	jcc->call_counter++;
+	CLG_(stat).call_counter++;
+
+	if (*pdepth == 1) function_entered(to_fn);
+    }
+
+    /* return address is only is useful with a real call;
+     * used to detect RET w/o CALL */
+    if (from->bb->jmp[jmp].jmpkind == jk_Call) {
+      UInt instr = from->bb->jmp[jmp].instr;
+      ret_addr = bb_addr(from->bb) +
+	from->bb->instr[instr].instr_offset +
+	from->bb->instr[instr].instr_size;
+    }
+    else
+      ret_addr = 0;
+
+    /* put jcc on call stack */
+    current_entry->jcc = jcc;
+    current_entry->sp = sp;
+    current_entry->ret_addr = ret_addr;
+    current_entry->nonskipped = CLG_(current_state).nonskipped;
+
+    CLG_(current_call_stack).sp++;
+
+    /* To allow for above assertion we set context of next frame to 0 */
+    CLG_ASSERT(CLG_(current_call_stack).sp < CLG_(current_call_stack).size);
+    current_entry++;
+    current_entry->cxt = 0;
+
+    if (!skip)
+	CLG_(current_state).nonskipped = 0;
+    else if (!CLG_(current_state).nonskipped) {
+	/* a call from nonskipped to skipped */
+	CLG_(current_state).nonskipped = from;
+	if (!CLG_(current_state).nonskipped->skipped) {
+	  CLG_(stat).distinct_skips++;
+	}
+    }
+
+#if CLG_ENABLE_DEBUG
+    CLG_DEBUGIF(0) {
+	if (CLG_(clo).verbose<2) {
+	  if (jcc && jcc->to && jcc->to->bb) {
+	    const HChar spaces[][41] = {
+                                  "   .   .   .   .   .   .   .   .   .   .",
+				  "  .   .   .   .   .   .   .   .   .   . ",
+				  " .   .   .   .   .   .   .   .   .   .  ",
+				  ".   .   .   .   .   .   .   .   .   .   " };
+
+	    int s = CLG_(current_call_stack).sp;
+	    UInt* pars = (UInt*) sp;
+
+	    BB* bb = jcc->to->bb;
+	    if (s>40) s=40;
+	    VG_(printf)("%s> %s(0x%x, 0x%x, ...) [%s / %#lx]\n", spaces[s%4]+40-s, bb->fn->name,
+                        pars ? pars[1]:0,
+			pars ? pars[2]:0,
+			bb->obj->name + bb->obj->last_slash_pos,
+			(UWord)bb->offset);
+	  }
+	}
+	else if (CLG_(clo).verbose<4) {
+	    VG_(printf)("+ %2d ", CLG_(current_call_stack).sp);
+	    VG_(printf)(", SP %#lx, RA %#lx\n", sp, ret_addr);
+	}
+	else {
+	    VG_(printf)("  Pushed ");
+	    CLG_(print_stackentry)(3, CLG_(current_call_stack).sp-1);
+	}
+    }
+#endif
+
+}
+
+
+/* Pop call stack and update inclusive sums.
+ * Returns modified fcc.
+ *
+ * If the JCC becomes inactive, call entries are freed if possible
+ */
+void CLG_(pop_call_stack)()
+{
+    jCC* jcc;
+    Int depth = 0;
+    call_entry* lower_entry;
+
+    if (CLG_(current_state).sig >0) {
+	/* Check if we leave a signal handler; this can happen when
+	 * calling longjmp() in the handler */
+	CLG_(run_post_signal_on_call_stack_bottom)();
+    }
+
+    lower_entry =
+	&(CLG_(current_call_stack).entry[CLG_(current_call_stack).sp-1]);
+
+    CLG_DEBUG(4,"+ pop_call_stack: frame %d, jcc %p\n", 
+		CLG_(current_call_stack).sp, lower_entry->jcc);
+
+    /* jCC item not any more on real stack: pop */
+    jcc = lower_entry->jcc;
+    CLG_(current_state).nonskipped = lower_entry->nonskipped;
+
+    if (jcc) {
+	fn_node* to_fn  = jcc->to->cxt->fn[0];
+	UInt* pdepth =  CLG_(get_fn_entry)(to_fn->number);
+	if (CLG_(clo).skip_direct_recursion) {
+	    /* only decrement depth if another function was called */
+	  if (jcc->from->cxt->fn[0] != to_fn) (*pdepth)--;
+	}
+	else (*pdepth)--;
+	depth = *pdepth;
+
+	/* restore context */
+	CLG_(current_state).cxt  = lower_entry->cxt;
+	CLG_(current_fn_stack).top =
+	  CLG_(current_fn_stack).bottom + lower_entry->fn_sp;
+	CLG_ASSERT(CLG_(current_state).cxt != 0);
+
+	if (depth == 0) function_left(to_fn);
+    }
+
+    /* To allow for an assertion in push_call_stack() */
+    lower_entry->cxt = 0;
+
+    CLG_(current_call_stack).sp--;
+
+#if CLG_ENABLE_DEBUG
+    CLG_DEBUGIF(1) {
+	if (CLG_(clo).verbose<4) {
+	    if (jcc) {
+		/* popped JCC target first */
+		VG_(printf)("- %2d %#lx => ",
+			    CLG_(current_call_stack).sp,
+			    bb_addr(jcc->to->bb));
+		CLG_(print_addr)(bb_jmpaddr(jcc->from->bb));
+		VG_(printf)(", SP %#lx\n",
+			    CLG_(current_call_stack).entry[CLG_(current_call_stack).sp].sp);
+	    }
+	    else
+		VG_(printf)("- %2d [Skipped JCC], SP %#lx\n",
+			    CLG_(current_call_stack).sp,
+			    CLG_(current_call_stack).entry[CLG_(current_call_stack).sp].sp);
+	}
+	else {
+	    VG_(printf)("  Popped ");
+	    CLG_(print_stackentry)(7, CLG_(current_call_stack).sp);
+	    if (jcc) {
+		VG_(printf)("       returned to ");
+		CLG_(print_addr_ln)(bb_jmpaddr(jcc->from->bb));
+	    }
+	}
+    }
+#endif
+
+}
+
+
+/* Unwind enough CallStack items to sync with current stack pointer.
+ * Returns the number of stack frames unwinded.
+ */
+Int CLG_(unwind_call_stack)(Addr sp, Int minpops)
+{
+    Int csp;
+    Int unwind_count = 0;
+    CLG_DEBUG(4,"+ unwind_call_stack(sp %#lx, minpops %d): frame %d\n",
+	      sp, minpops, CLG_(current_call_stack).sp);
+
+    /* We pop old stack frames.
+     * For a call, be p the stack address with return address.
+     *  - call_stack_esp[] has SP after the CALL: p-4
+     *  - current sp is after a RET: >= p
+     */
+    
+    while( (csp=CLG_(current_call_stack).sp) >0) {
+	call_entry* top_ce = &(CLG_(current_call_stack).entry[csp-1]);
+
+	if ((top_ce->sp < sp) ||
+	    ((top_ce->sp == sp) && minpops>0)) {
+
+	    minpops--;
+	    unwind_count++;
+	    CLG_(pop_call_stack)();
+	    csp=CLG_(current_call_stack).sp;
+	    continue;
+	}
+	break;
+    }
+
+    CLG_DEBUG(4,"- unwind_call_stack\n");
+    return unwind_count;
+}
diff --git a/sigrind/clo.c b/sigrind/clo.c
new file mode 100644
index 000000000..d1d3e526e
--- /dev/null
+++ b/sigrind/clo.c
@@ -0,0 +1,687 @@
+/*
+   This file is part of Callgrind, a Valgrind tool for call graph
+   profiling programs.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This tool is derived from and contains lot of code from Cachegrind
+   Copyright (C) 2002-2015 Nicholas Nethercote (njn@valgrind.org)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "config.h" // for VG_PREFIX
+
+#include "global.h"
+
+
+
+/*------------------------------------------------------------*/
+/*--- Function specific configuration options              ---*/
+/*------------------------------------------------------------*/
+
+/* Special value for separate_callers: automatic = adaptive */
+#define CONFIG_AUTO    -1
+
+#define CONFIG_DEFAULT -1
+#define CONFIG_FALSE    0
+#define CONFIG_TRUE     1
+
+/* Logging configuration for a function */
+struct _fn_config {
+    Int dump_before;
+    Int dump_after;
+    Int zero_before;
+    Int toggle_collect;
+
+    Int skip;    /* Handle CALL to this function as JMP (= Skip)? */
+    Int group;   /* don't change caller dependency inside group !=0 */
+    Int pop_on_jump; 
+
+    Int separate_callers;    /* separate logging dependent on caller  */
+    Int separate_recursions; /* separate logging of rec. levels       */
+
+#if CLG_ENABLE_DEBUG
+    Int verbosity; /* Change debug verbosity level while in function */
+#endif
+};
+
+/* Configurations for function name prefix patterns.
+ * Currently, only very limit patterns are possible:
+ * Exact prefix patterns and "*::" are allowed.
+ * E.g.
+ *  - "abc" matches all functions starting with "abc".
+ *  - "abc*::def" matches all functions starting with "abc" and
+ *    starting with "def" after the first "::" separator.
+ *  - "*::print(" matches C++ methods "print" in all classes
+ *    without namespace. I.e. "*" doesn't match a "::".
+ *
+ * We build a trie from patterns, and for a given function, we
+ * go down the tree and apply all non-default configurations.
+ */
+
+
+#define NODE_DEGREE 30
+
+/* node of compressed trie search structure */
+typedef struct _config_node config_node;
+struct _config_node {
+  Int length;
+    
+  fn_config* config;
+  config_node* sub_node[NODE_DEGREE];
+  config_node* next;
+  config_node* wild_star;
+  config_node* wild_char;
+
+  HChar name[1];
+};
+
+/* root of trie */
+static config_node* fn_configs = 0;
+
+static __inline__ 
+fn_config* new_fnc(void)
+{
+   fn_config* fnc = (fn_config*) CLG_MALLOC("cl.clo.nf.1",
+                                            sizeof(fn_config));
+
+   fnc->dump_before  = CONFIG_DEFAULT;
+   fnc->dump_after   = CONFIG_DEFAULT;
+   fnc->zero_before  = CONFIG_DEFAULT;
+   fnc->toggle_collect = CONFIG_DEFAULT;
+   fnc->skip         = CONFIG_DEFAULT;
+   fnc->pop_on_jump  = CONFIG_DEFAULT;
+   fnc->group        = CONFIG_DEFAULT;
+   fnc->separate_callers    = CONFIG_DEFAULT;
+   fnc->separate_recursions = CONFIG_DEFAULT;
+
+#if CLG_ENABLE_DEBUG
+   fnc->verbosity    = CONFIG_DEFAULT;
+#endif
+
+   return fnc;
+}
+
+
+static config_node* new_config(const HChar* name, int length)
+{
+    int i;
+    config_node* node = (config_node*) CLG_MALLOC("cl.clo.nc.1",
+                                                  sizeof(config_node) + length);
+
+    for(i=0;i<length;i++) {
+      if (name[i] == 0) break;
+      node->name[i] = name[i];
+    }
+    node->name[i] = 0;
+
+    node->length = length;
+    node->config = 0;
+    for(i=0;i<NODE_DEGREE;i++)
+	node->sub_node[i] = 0;
+    node->next = 0;
+    node->wild_char = 0;
+    node->wild_star = 0;
+
+    CLG_DEBUG(3, "   new_config('%s', len %d)\n", node->name, length);
+
+    return node;
+}
+
+static __inline__
+Bool is_wild(HChar n)
+{
+  return (n == '*') || (n == '?');
+}
+
+/* Recursively build up function matching tree (prefix tree).
+ * Returns function config object for pattern <name>
+ * and starting at tree node <*pnode>.
+ *
+ * Tree nodes (config_node) are created as needed,
+ * tree root is stored into <*pnode>, and the created
+ * leaf (fn_config) for the given pattern is returned.
+ */
+static fn_config* get_fnc2(config_node* node, const HChar* name)
+{
+  config_node *new_sub, *n, *nprev;
+  int offset, len;
+
+  CLG_DEBUG(3, "  get_fnc2(%p, '%s')\n", node, name);
+
+  if (name[0] == 0) {
+    if (!node->config) node->config = new_fnc();
+    return node->config;
+  }
+
+  if (is_wild(*name)) {
+    if (*name == '*') {
+      while(name[1] == '*') name++;
+      new_sub = node->wild_star;
+    }
+    else
+      new_sub = node->wild_char;
+
+    if (!new_sub) {
+      new_sub = new_config(name, 1);
+      if (*name == '*')
+	node->wild_star = new_sub;
+      else
+	node->wild_char = new_sub;
+    }
+
+    return get_fnc2( new_sub, name+1);
+  }
+
+  n = node->sub_node[ name[0]%NODE_DEGREE ];
+  nprev = 0;
+  len = 0;
+  while(n) {
+    for(len=0; name[len] == n->name[len]; len++);
+    if (len>0) break;
+    nprev = n;
+    n = n->next;
+  }
+
+  if (!n) {
+    len = 1;
+    while(name[len] && (!is_wild(name[len]))) len++;
+    new_sub = new_config(name, len);
+    new_sub->next = node->sub_node[ name[0]%NODE_DEGREE ];
+    node->sub_node[ name[0]%NODE_DEGREE ] = new_sub;	
+
+    if (name[len] == 0) {
+      new_sub->config = new_fnc();
+      return new_sub->config;
+    }
+    
+    /* recurse on wildcard */
+    return get_fnc2( new_sub, name+len);
+  }
+
+  if (len < n->length) {
+
+    /* split up the subnode <n> */
+    config_node *new_node;
+    int i;
+
+    new_node = new_config(n->name, len);
+    if (nprev)
+      nprev->next = new_node;
+    else
+      node->sub_node[ n->name[0]%NODE_DEGREE ] = new_node;
+    new_node->next = n->next;
+
+    new_node->sub_node[ n->name[len]%NODE_DEGREE ] = n;
+
+    for(i=0, offset=len; offset < n->length; i++, offset++)
+      n->name[i] = n->name[offset];
+    n->name[i] = 0;
+    n->length = i;
+
+    name += len;
+    offset = 0;
+    while(name[offset] && (!is_wild(name[offset]))) offset++;
+    new_sub  = new_config(name, offset);
+    /* this sub_node of new_node could already be set: chain! */
+    new_sub->next = new_node->sub_node[ name[0]%NODE_DEGREE ];
+    new_node->sub_node[ name[0]%NODE_DEGREE ] = new_sub;
+
+    if (name[offset]==0) {
+      new_sub->config = new_fnc();
+      return new_sub->config;
+    }
+
+    /* recurse on wildcard */
+    return get_fnc2( new_sub, name+offset);
+  }
+
+  name += n->length;
+
+  if (name[0] == 0) {
+    /* name and node name are the same */
+    if (!n->config) n->config = new_fnc();
+    return n->config;
+  }
+
+  offset = 1;
+  while(name[offset] && (!is_wild(name[offset]))) offset++;
+
+  new_sub = new_config(name, offset);
+  new_sub->next = n->sub_node[ name[0]%NODE_DEGREE ];
+  n->sub_node[ name[0]%NODE_DEGREE ] = new_sub;
+
+  return get_fnc2(new_sub, name+offset);
+}
+
+static void print_config_node(int depth, int hash, config_node* node)
+{
+  config_node* n;
+  int i;
+
+  if (node != fn_configs) {
+    const HChar sp[] = "                                        ";
+
+    if (depth>40) depth=40;
+    VG_(printf)("%s", sp+40-depth);
+    if (hash >=0) VG_(printf)(" [hash %2d]", hash);
+    else if (hash == -2) VG_(printf)(" [wildc ?]");
+    else if (hash == -3) VG_(printf)(" [wildc *]");
+    VG_(printf)(" '%s' (len %d)\n", node->name, node->length);
+  }
+  for(i=0;i<NODE_DEGREE;i++) {
+    n = node->sub_node[i];
+    while(n) {
+      print_config_node(depth+1, i, n);
+      n = n->next;
+    }
+  }
+  if (node->wild_char) print_config_node(depth+1, -2, node->wild_char);
+  if (node->wild_star) print_config_node(depth+1, -3, node->wild_star);
+}
+
+/* get a function config for a name pattern (from command line) */
+static fn_config* get_fnc(const HChar* name)
+{
+  fn_config* fnc;
+
+  CLG_DEBUG(3, " +get_fnc(%s)\n", name);
+  if (fn_configs == 0)
+    fn_configs = new_config(name, 0);
+  fnc =  get_fnc2(fn_configs, name);
+
+  CLG_DEBUGIF(3) {
+    CLG_DEBUG(3, " -get_fnc(%s):\n", name);
+    print_config_node(3, -1, fn_configs);
+  }
+  return fnc;
+}
+
+  
+
+static void update_fn_config1(fn_node* fn, fn_config* fnc)
+{
+    if (fnc->dump_before != CONFIG_DEFAULT)
+	fn->dump_before = (fnc->dump_before == CONFIG_TRUE);
+
+    if (fnc->dump_after != CONFIG_DEFAULT)
+	fn->dump_after = (fnc->dump_after == CONFIG_TRUE);
+
+    if (fnc->zero_before != CONFIG_DEFAULT)
+	fn->zero_before = (fnc->zero_before == CONFIG_TRUE);
+
+    if (fnc->toggle_collect != CONFIG_DEFAULT)
+	fn->toggle_collect = (fnc->toggle_collect == CONFIG_TRUE);
+
+    if (fnc->skip != CONFIG_DEFAULT)
+	fn->skip = (fnc->skip == CONFIG_TRUE);
+
+    if (fnc->pop_on_jump != CONFIG_DEFAULT)
+	fn->pop_on_jump = (fnc->pop_on_jump == CONFIG_TRUE);
+
+    if (fnc->group != CONFIG_DEFAULT)
+	fn->group = fnc->group;
+
+    if (fnc->separate_callers != CONFIG_DEFAULT)
+	fn->separate_callers = fnc->separate_callers;
+
+    if (fnc->separate_recursions != CONFIG_DEFAULT)
+	fn->separate_recursions = fnc->separate_recursions;
+
+#if CLG_ENABLE_DEBUG
+    if (fnc->verbosity != CONFIG_DEFAULT)
+	fn->verbosity = fnc->verbosity;
+#endif
+}
+
+/* Recursively go down the function matching tree,
+ * looking for a match to <name>. For every matching leaf,
+ * <fn> is updated with the pattern config.
+ */
+static void update_fn_config2(fn_node* fn, const HChar* name,
+                              config_node* node)
+{
+    config_node* n;
+
+    CLG_DEBUG(3, "  update_fn_config2('%s', node '%s'): \n",
+	     name, node->name);
+    if ((*name == 0) && node->config) {
+      CLG_DEBUG(3, "   found!\n");
+      update_fn_config1(fn, node->config);
+      return;
+    }
+
+    n = node->sub_node[ name[0]%NODE_DEGREE ];
+    while(n) {
+      if (VG_(strncmp)(name, n->name, n->length)==0) break;
+      n = n->next;
+    }
+    if (n) {
+	CLG_DEBUG(3, "   '%s' matching at hash %d\n",
+		  n->name, name[0]%NODE_DEGREE);
+	update_fn_config2(fn, name+n->length, n);
+    }
+    
+    if (node->wild_char) {
+	CLG_DEBUG(3, "   skip '%c' for wildcard '?'\n", *name);
+	update_fn_config2(fn, name+1, node->wild_char);
+    }
+
+    if (node->wild_star) {
+      CLG_DEBUG(3, "   wildcard '*'\n");
+      while(*name) {
+	update_fn_config2(fn, name, node->wild_star);
+	name++;
+      }
+      update_fn_config2(fn, name, node->wild_star);
+    }
+}
+
+/* Update function config according to configs of name prefixes */
+void CLG_(update_fn_config)(fn_node* fn)
+{
+    CLG_DEBUG(3, "  update_fn_config('%s')\n", fn->name);
+    if (fn_configs)
+      update_fn_config2(fn, fn->name, fn_configs);
+}
+
+
+/*--------------------------------------------------------------------*/
+/*--- Command line processing                                      ---*/
+/*--------------------------------------------------------------------*/
+
+Bool CLG_(process_cmd_line_option)(const HChar* arg)
+{
+   const HChar* tmp_str;
+
+   /* XXX tmpdir should not be set by the end-user, only for Sigil2 use */
+   if      VG_STR_CLO(arg,  "--ipc-dir",    SGL_(clo).ipc_dir) {}
+   else if VG_STR_CLO(arg,  "--at-func",    SGL_(clo).collect_func) {}
+   else if VG_STR_CLO(arg,  "--start-func", SGL_(clo).start_collect_func) {}
+   else if VG_STR_CLO(arg,  "--stop-func",  SGL_(clo).stop_collect_func) {}
+   else if VG_BOOL_CLO(arg, "--gen-mem",    SGL_(clo).gen_mem) {}
+   else if VG_BOOL_CLO(arg, "--gen-comp",   SGL_(clo).gen_comp) {}
+   else if VG_BOOL_CLO(arg, "--gen-sync",   SGL_(clo).gen_sync) {}
+   else if VG_BOOL_CLO(arg, "--gen-instr",  SGL_(clo).gen_instr) {}
+   else if VG_BOOL_CLO(arg, "--gen-fn",     SGL_(clo).gen_fn) {}
+   else if VG_BOOL_CLO(arg, "--gen-cf",     SGL_(clo).gen_cf) {}
+   else if VG_BOOL_CLO(arg, "--gen-bb",     SGL_(clo).gen_bb) {}
+
+   /* XXX
+    * ML: leftover from Callgrind. Most of these should be left at defaults
+    * for Sigrind, except perhaps --separate callers depending on the application
+    */
+   else if VG_BOOL_CLO(arg, "--skip-plt", CLG_(clo).skip_plt) {}
+
+   else if VG_BOOL_CLO(arg, "--collect-jumps", CLG_(clo).collect_jumps) {}
+   /* compatibility alias, deprecated option */
+   else if VG_BOOL_CLO(arg, "--trace-jump",    CLG_(clo).collect_jumps) {}
+
+   else if VG_BOOL_CLO(arg, "--combine-dumps", CLG_(clo).combine_dumps) {}
+
+   else if VG_BOOL_CLO(arg, "--collect-atstart", CLG_(clo).collect_atstart) {}
+
+   else if VG_BOOL_CLO(arg, "--instr-atstart", CLG_(clo).instrument_atstart) {}
+
+   else if VG_BOOL_CLO(arg, "--separate-threads", CLG_(clo).separate_threads) {}
+
+   else if VG_BOOL_CLO(arg, "--compress-strings", CLG_(clo).compress_strings) {}
+   else if VG_BOOL_CLO(arg, "--compress-mangled", CLG_(clo).compress_mangled) {}
+   else if VG_BOOL_CLO(arg, "--compress-pos",     CLG_(clo).compress_pos) {}
+
+   else if VG_STR_CLO(arg, "--dump-before", tmp_str) {
+       fn_config* fnc = get_fnc(tmp_str);
+       fnc->dump_before = CONFIG_TRUE;
+   }
+
+   else if VG_STR_CLO(arg, "--zero-before", tmp_str) {
+       fn_config* fnc = get_fnc(tmp_str);
+       fnc->zero_before = CONFIG_TRUE;
+   }
+
+   else if VG_STR_CLO(arg, "--dump-after", tmp_str) {
+       fn_config* fnc = get_fnc(tmp_str);
+       fnc->dump_after = CONFIG_TRUE;
+   }
+
+   else if VG_STR_CLO(arg, "--toggle-collect", tmp_str) {
+       fn_config* fnc = get_fnc(tmp_str);
+       fnc->toggle_collect = CONFIG_TRUE;
+       /* defaults to initial collection off */
+       CLG_(clo).collect_atstart = False;
+   }
+
+   else if VG_INT_CLO(arg, "--separate-recs", CLG_(clo).separate_recursions) {}
+
+   /* change handling of a jump between functions to ret+call */
+   else if VG_XACT_CLO(arg, "--pop-on-jump", CLG_(clo).pop_on_jump, True) {}
+   else if VG_STR_CLO( arg, "--pop-on-jump", tmp_str) {
+       fn_config* fnc = get_fnc(tmp_str);
+       fnc->pop_on_jump = CONFIG_TRUE;
+   }
+
+#if CLG_ENABLE_DEBUG
+   else if VG_INT_CLO(arg, "--ct-verbose", CLG_(clo).verbose) {}
+   else if VG_INT_CLO(arg, "--ct-vstart",  CLG_(clo).verbose_start) {}
+
+   else if VG_STREQN(12, arg, "--ct-verbose") {
+       fn_config* fnc;
+       HChar* s;
+       UInt n = VG_(strtoll10)(arg+12, &s);
+       if ((n <= 0) || *s != '=') return False;
+       fnc = get_fnc(s+1);
+       fnc->verbosity = n;
+   }
+#endif
+
+   else if VG_XACT_CLO(arg, "--separate-callers=auto", 
+                            CLG_(clo).separate_callers, CONFIG_AUTO) {}
+   else if VG_INT_CLO( arg, "--separate-callers", 
+                            CLG_(clo).separate_callers) {}
+
+   else if VG_STREQN(10, arg, "--fn-group") {
+       fn_config* fnc;
+       HChar* s;
+       UInt n = VG_(strtoll10)(arg+10, &s);
+       if ((n <= 0) || *s != '=') return False;
+       fnc = get_fnc(s+1);
+       fnc->group = n;
+   }
+
+   else if VG_STREQN(18, arg, "--separate-callers") {
+       fn_config* fnc;
+       HChar* s;
+       UInt n = VG_(strtoll10)(arg+18, &s);
+       if ((n <= 0) || *s != '=') return False;
+       fnc = get_fnc(s+1);
+       fnc->separate_callers = n;
+   }
+
+   else if VG_STREQN(15, arg, "--separate-recs") {
+       fn_config* fnc;
+       HChar* s;
+       UInt n = VG_(strtoll10)(arg+15, &s);
+       if ((n <= 0) || *s != '=') return False;
+       fnc = get_fnc(s+1);
+       fnc->separate_recursions = n;
+   }
+
+   else if VG_STR_CLO(arg, "--callgrind-out-file", CLG_(clo).out_format) {}
+
+   else if VG_BOOL_CLO(arg, "--mangle-names", CLG_(clo).mangle_names) {}
+
+   else if VG_BOOL_CLO(arg, "--skip-direct-rec",
+                            CLG_(clo).skip_direct_recursion) {}
+
+   else if VG_BOOL_CLO(arg, "--dump-bbs",   CLG_(clo).dump_bbs) {}
+   else if VG_BOOL_CLO(arg, "--dump-line",  CLG_(clo).dump_line) {}
+   else if VG_BOOL_CLO(arg, "--dump-instr", CLG_(clo).dump_instr) {}
+   else if VG_BOOL_CLO(arg, "--dump-bb",    CLG_(clo).dump_bb) {}
+
+   else if VG_INT_CLO( arg, "--dump-every-bb", CLG_(clo).dump_every_bb) {}
+
+   else if VG_BOOL_CLO(arg, "--collect-alloc",   CLG_(clo).collect_alloc) {}
+   else if VG_BOOL_CLO(arg, "--collect-systime", CLG_(clo).collect_systime) {}
+   else if VG_BOOL_CLO(arg, "--collect-bus",     CLG_(clo).collect_bus) {}
+   /* for option compatibility with cachegrind */
+   else if VG_BOOL_CLO(arg, "--branch-sim",      CLG_(clo).simulate_branch) {}
+   else {
+       return False;
+   }
+
+   return True;
+}
+
+void CLG_(print_usage)(void)
+{
+	/*
+   VG_(printf)(
+"\n   dump creation options:\n"
+"    --callgrind-out-file=<f>  Output file name [callgrind.out.%%p]\n"
+"    --dump-line=no|yes        Dump source lines of costs? [yes]\n"
+"    --dump-instr=no|yes       Dump instruction address of costs? [no]\n"
+"    --compress-strings=no|yes Compress strings in profile dump? [yes]\n"
+"    --compress-pos=no|yes     Compress positions in profile dump? [yes]\n"
+"    --combine-dumps=no|yes    Concat all dumps into same file [no]\n"
+#if CLG_EXPERIMENTAL
+"    --compress-events=no|yes  Compress events in profile dump? [no]\n"
+"    --dump-bb=no|yes          Dump basic block address of costs? [no]\n"
+"    --dump-bbs=no|yes         Dump basic block info? [no]\n"
+"    --dump-skipped=no|yes     Dump info on skipped functions in calls? [no]\n"
+"    --mangle-names=no|yes     Mangle separation into names? [yes]\n"
+#endif
+
+"\n   activity options (for interactivity use callgrind_control):\n"
+"    --dump-every-bb=<count>   Dump every <count> basic blocks [0=never]\n"
+"    --dump-before=<func>      Dump when entering function\n"
+"    --zero-before=<func>      Zero all costs when entering function\n"
+"    --dump-after=<func>       Dump when leaving function\n"
+#if CLG_EXPERIMENTAL
+"    --dump-objs=no|yes        Dump static object information [no]\n"
+#endif
+
+"\n   data collection options:\n"
+"    --instr-atstart=no|yes    Do instrumentation at callgrind start [yes]\n"
+"    --collect-atstart=no|yes  Collect at process/thread start [yes]\n"
+"    --toggle-collect=<func>   Toggle collection on enter/leave function\n"
+"    --collect-jumps=no|yes    Collect jumps? [no]\n"
+"    --collect-bus=no|yes      Collect global bus events? [no]\n"
+#if CLG_EXPERIMENTAL
+"    --collect-alloc=no|yes    Collect memory allocation info? [no]\n"
+#endif
+"    --collect-systime=no|yes  Collect system call time info? [no]\n"
+
+"\n   cost entity separation options:\n"
+"    --separate-threads=no|yes Separate data per thread [no]\n"
+"    --separate-callers=<n>    Separate functions by call chain length [0]\n"
+"    --separate-callers<n>=<f> Separate <n> callers for function <f>\n"
+"    --separate-recs=<n>       Separate function recursions up to level [2]\n"
+"    --separate-recs<n>=<f>    Separate <n> recursions for function <f>\n"
+"    --skip-plt=no|yes         Ignore calls to/from PLT sections? [yes]\n"
+"    --skip-direct-rec=no|yes  Ignore direct recursions? [yes]\n"
+"    --fn-skip=<function>      Ignore calls to/from function?\n"
+#if CLG_EXPERIMENTAL
+"    --fn-group<no>=<func>     Put function into separation group <no>\n"
+#endif
+"\n   simulation options:\n"
+"    --branch-sim=no|yes       Do branch prediction simulation [no]\n"
+"    --cache-sim=no|yes        Do cache simulation [no]\n"
+    );
+
+//   VG_(printf)("\n"
+//	       "  For full callgrind documentation, see\n"
+//	       "  "VG_PREFIX"/share/doc/callgrind/html/callgrind.html\n\n");
+	*/
+}
+
+void CLG_(print_debug_usage)(void)
+{
+    VG_(printf)(
+
+#if CLG_ENABLE_DEBUG
+"    --ct-verbose=<level>       Verbosity of standard debug output [0]\n"
+"    --ct-vstart=<BB number>    Only be verbose after basic block [0]\n"
+"    --ct-verbose<level>=<func> Verbosity while in <func>\n"
+#else
+"    (none)\n"
+#endif
+
+    );
+}
+
+void SGL_(set_clo_defaults)(void)
+{
+  SGL_(clo).ipc_dir            = NULL;
+  SGL_(clo).collect_func       = NULL;
+  SGL_(clo).start_collect_func = NULL;
+  SGL_(clo).stop_collect_func  = NULL;
+  SGL_(clo).gen_mem            = False;
+  SGL_(clo).gen_comp           = False;
+  SGL_(clo).gen_cf             = False;
+  SGL_(clo).gen_sync           = False;
+  SGL_(clo).gen_instr          = False;
+  SGL_(clo).gen_bb             = False;
+  SGL_(clo).gen_fn             = False;
+  SGL_(clo).gen_thr            = False;
+}
+
+void CLG_(set_clo_defaults)(void)
+{
+  /* Default values for command line arguments */
+
+  /* dump options */
+  CLG_(clo).out_format       = 0;
+  CLG_(clo).combine_dumps    = False;
+  CLG_(clo).compress_strings = True;
+  CLG_(clo).compress_mangled = False;
+  CLG_(clo).compress_events  = False;
+  CLG_(clo).compress_pos     = True;
+  CLG_(clo).mangle_names     = True;
+  CLG_(clo).dump_line        = True;
+  CLG_(clo).dump_instr       = False;
+  CLG_(clo).dump_bb          = False;
+  CLG_(clo).dump_bbs         = False;
+
+  CLG_(clo).dump_every_bb    = 0;
+
+  /* Collection */
+  CLG_(clo).separate_threads = False;
+  CLG_(clo).collect_atstart  = True;
+  CLG_(clo).collect_jumps    = False;
+  CLG_(clo).collect_alloc    = False;
+  CLG_(clo).collect_systime  = False;
+  CLG_(clo).collect_bus      = False;
+
+  CLG_(clo).skip_plt         = True;
+  CLG_(clo).separate_callers = 0;
+  CLG_(clo).separate_recursions = 2;
+  CLG_(clo).skip_direct_recursion = False;
+
+  /* Instrumentation */
+  CLG_(clo).instrument_atstart = True;
+  CLG_(clo).simulate_branch = False;
+
+  /* Call graph */
+  CLG_(clo).pop_on_jump = False;
+
+#if CLG_ENABLE_DEBUG
+  CLG_(clo).verbose = 0;
+  CLG_(clo).verbose_start = 0;
+#endif
+}
diff --git a/sigrind/context.c b/sigrind/context.c
new file mode 100644
index 000000000..33f738627
--- /dev/null
+++ b/sigrind/context.c
@@ -0,0 +1,332 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                 ct_context.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+
+/*------------------------------------------------------------*/
+/*--- Context operations                                   ---*/
+/*------------------------------------------------------------*/
+
+#define N_FNSTACK_INITIAL_ENTRIES 500
+#define N_CXT_INITIAL_ENTRIES 2537
+
+fn_stack CLG_(current_fn_stack);
+
+void CLG_(init_fn_stack)(fn_stack* s)
+{
+  CLG_ASSERT(s != 0);
+
+  s->size   = N_FNSTACK_INITIAL_ENTRIES;   
+  s->bottom = (fn_node**) CLG_MALLOC("cl.context.ifs.1",
+                                     s->size * sizeof(fn_node*));
+  s->top    = s->bottom;
+  s->bottom[0] = 0;
+}
+
+void CLG_(copy_current_fn_stack)(fn_stack* dst)
+{
+  CLG_ASSERT(dst != 0);
+
+  dst->size   = CLG_(current_fn_stack).size;
+  dst->bottom = CLG_(current_fn_stack).bottom;
+  dst->top    = CLG_(current_fn_stack).top;
+}
+
+void CLG_(set_current_fn_stack)(fn_stack* s)
+{
+  CLG_ASSERT(s != 0);
+
+  CLG_(current_fn_stack).size   = s->size;
+  CLG_(current_fn_stack).bottom = s->bottom;
+  CLG_(current_fn_stack).top    = s->top;
+}
+
+static cxt_hash cxts;
+
+void CLG_(init_cxt_table)()
+{
+   Int i;
+   
+   cxts.size    = N_CXT_INITIAL_ENTRIES;
+   cxts.entries = 0;
+   cxts.table   = (Context**) CLG_MALLOC("cl.context.ict.1",
+                                         cxts.size * sizeof(Context*));
+
+   for (i = 0; i < cxts.size; i++)
+     cxts.table[i] = 0;
+}
+
+/* double size of cxt table  */
+static void resize_cxt_table(void)
+{
+    UInt i, new_size, conflicts1 = 0, conflicts2 = 0;
+    Context **new_table, *curr, *next;
+    UInt new_idx;
+
+    new_size  = 2* cxts.size +3;
+    new_table = (Context**) CLG_MALLOC("cl.context.rct.1",
+                                       new_size * sizeof(Context*));
+
+    for (i = 0; i < new_size; i++)
+      new_table[i] = NULL;
+
+    for (i = 0; i < cxts.size; i++) {
+        if (cxts.table[i] == NULL) continue;
+
+        curr = cxts.table[i];
+        while (NULL != curr) {
+            next = curr->next;
+
+            new_idx = (UInt) (curr->hash % new_size);
+
+            curr->next = new_table[new_idx];
+            new_table[new_idx] = curr;
+            if (curr->next) {
+                conflicts1++;
+                if (curr->next->next)
+                    conflicts2++;
+            }
+
+            curr = next;
+        }
+    }
+
+    VG_(free)(cxts.table);
+
+
+    CLG_DEBUG(0, "Resize Context Hash: %u => %u (entries %u, conflicts %u/%u)\n",
+             cxts.size, new_size,
+             cxts.entries, conflicts1, conflicts2);
+
+    cxts.size  = new_size;
+    cxts.table = new_table;
+    CLG_(stat).cxt_hash_resizes++;
+}
+
+__inline__
+static UWord cxt_hash_val(fn_node** fn, UInt size)
+{
+    UWord hash = 0;
+    UInt count = size;
+    while(*fn != 0) {
+        hash = (hash<<7) + (hash>>25) + (UWord)(*fn);
+        fn--;
+        count--;
+        if (count==0) break;
+    }
+    return hash;
+}
+
+__inline__
+static Bool is_cxt(UWord hash, fn_node** fn, Context* cxt)
+{
+    int count;
+    fn_node** cxt_fn;
+
+    if (hash != cxt->hash) return False;
+
+    count = cxt->size;
+    cxt_fn = &(cxt->fn[0]);
+    while((*fn != 0) && (count>0)) {
+        if (*cxt_fn != *fn) return False;
+        fn--;
+        cxt_fn++;
+        count--;
+    }
+    return True;
+}
+
+/**
+ * Allocate new Context structure
+ */
+static Context* new_cxt(fn_node** fn)
+{
+    Context* cxt;
+    UInt idx, offset;
+    UWord hash;
+    int size, recs;
+    fn_node* top_fn;
+
+    CLG_ASSERT(fn);
+    top_fn = *fn;
+    if (top_fn == 0) return 0;
+
+    size = top_fn->separate_callers +1;
+    recs = top_fn->separate_recursions;
+    if (recs<1) recs=1;
+
+    /* check fill degree of context hash table and resize if needed (>80%) */
+    cxts.entries++;
+    if (10 * cxts.entries / cxts.size > 8)
+        resize_cxt_table();
+
+    cxt = (Context*) CLG_MALLOC("cl.context.nc.1",
+                                sizeof(Context)+sizeof(fn_node*)*size);
+
+    // hash value calculation similar to cxt_hash_val(), but additionally
+    // copying function pointers in one run
+    hash = 0;
+    offset = 0;
+    while(*fn != 0) {
+        hash = (hash<<7) + (hash>>25) + (UWord)(*fn);
+	cxt->fn[offset] = *fn;
+        offset++;
+        fn--;
+        if (offset >= size) break;
+    }
+    if (offset < size) size = offset;
+
+    cxt->size        = size;
+    cxt->base_number = CLG_(stat).context_counter;
+    cxt->hash        = hash;
+
+    CLG_(stat).context_counter += recs;
+    CLG_(stat).distinct_contexts++;
+
+    /* insert into Context hash table */
+    idx = (UInt) (hash % cxts.size);
+    cxt->next = cxts.table[idx];
+    cxts.table[idx] = cxt;
+
+#if CLG_ENABLE_DEBUG
+    CLG_DEBUGIF(3) {
+      VG_(printf)("  new_cxt ox%p: ", cxt);
+      CLG_(print_cxt)(12, cxt, 0);
+    }
+#endif
+
+    return cxt;
+}
+
+/* get the Context structure for current context */
+Context* CLG_(get_cxt)(fn_node** fn)
+{
+    Context* cxt;
+    UInt size, idx;
+    UWord hash;
+
+    CLG_ASSERT(fn != 0);
+    if (*fn == 0) return 0;
+    size = (*fn)->separate_callers+1;
+    if (size<=0) { size = -size+1; }
+
+    CLG_DEBUG(5, "+ get_cxt(fn '%s'): size %u\n",
+                (*fn)->name, size);
+
+    hash = cxt_hash_val(fn, size);
+
+    if ( ((cxt = (*fn)->last_cxt) != 0) && is_cxt(hash, fn, cxt)) {
+        CLG_DEBUG(5, "- get_cxt: %p\n", cxt);
+        return cxt;
+    }
+
+    CLG_(stat).cxt_lru_misses++;
+
+    idx = (UInt) (hash % cxts.size);
+    cxt = cxts.table[idx];
+
+    while(cxt) {
+        if (is_cxt(hash,fn,cxt)) break;
+        cxt = cxt->next;
+    }
+
+    if (!cxt)
+        cxt = new_cxt(fn);
+
+    (*fn)->last_cxt = cxt;
+
+    CLG_DEBUG(5, "- get_cxt: %p\n", cxt);
+
+    return cxt;
+}
+
+
+/**
+ * Change execution context by calling a new function from current context
+ * Pushing 0x0 specifies a marker for a signal handler entry
+ */
+void CLG_(push_cxt)(fn_node* fn)
+{
+  call_stack* cs = &CLG_(current_call_stack);
+  Int fn_entries;
+
+  CLG_DEBUG(5, "+ push_cxt(fn '%s'): old ctx %d\n", 
+	    fn ? fn->name : "0x0",
+	    CLG_(current_state).cxt ?
+	    (Int)CLG_(current_state).cxt->base_number : -1);
+
+  /* save old context on stack (even if not changed at all!) */
+  CLG_ASSERT(cs->sp < cs->size);
+  CLG_ASSERT(cs->entry[cs->sp].cxt == 0);
+  cs->entry[cs->sp].cxt = CLG_(current_state).cxt;
+  cs->entry[cs->sp].fn_sp = CLG_(current_fn_stack).top - CLG_(current_fn_stack).bottom;
+
+  if (fn && (*(CLG_(current_fn_stack).top) == fn)) return;
+  if (fn && (fn->group>0) &&
+      ((*(CLG_(current_fn_stack).top))->group == fn->group)) return;
+
+  /* resizing needed ? */
+  fn_entries = CLG_(current_fn_stack).top - CLG_(current_fn_stack).bottom;
+  if (fn_entries == CLG_(current_fn_stack).size-1) {
+    UInt new_size = CLG_(current_fn_stack).size *2;
+    fn_node** new_array = (fn_node**) CLG_MALLOC("cl.context.pc.1",
+						 new_size * sizeof(fn_node*));
+    int i;
+    for(i=0;i<CLG_(current_fn_stack).size;i++)
+      new_array[i] = CLG_(current_fn_stack).bottom[i];
+    VG_(free)(CLG_(current_fn_stack).bottom);
+    CLG_(current_fn_stack).top = new_array + fn_entries;
+    CLG_(current_fn_stack).bottom = new_array;
+
+    CLG_DEBUG(0, "Resize Context Stack: %u => %u (pushing '%s')\n", 
+	     CLG_(current_fn_stack).size, new_size,
+	     fn ? fn->name : "0x0");
+
+    CLG_(current_fn_stack).size = new_size;
+  }
+
+  if (fn && (*(CLG_(current_fn_stack).top) == 0)) {
+    UInt *pactive;
+
+    /* this is first function: increment its active count */
+    pactive = CLG_(get_fn_entry)(fn->number);
+    (*pactive)++;
+  }
+
+  CLG_(current_fn_stack).top++;
+  *(CLG_(current_fn_stack).top) = fn;
+  CLG_(current_state).cxt = CLG_(get_cxt)(CLG_(current_fn_stack).top);
+
+  CLG_DEBUG(5, "- push_cxt(fn '%s'): new cxt %d, fn_sp %ld\n",
+	    fn ? fn->name : "0x0",
+	    CLG_(current_state).cxt ?
+	    (Int)CLG_(current_state).cxt->base_number : -1,
+	    CLG_(current_fn_stack).top - CLG_(current_fn_stack).bottom + 0L);
+}
+			       
diff --git a/sigrind/debug.c b/sigrind/debug.c
new file mode 100644
index 000000000..011470692
--- /dev/null
+++ b/sigrind/debug.c
@@ -0,0 +1,447 @@
+/*
+   This file is part of Callgrind, a Valgrind tool for call graph
+   profiling programs.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This tool is derived from and contains lot of code from Cachegrind
+   Copyright (C) 2002-2015 Nicholas Nethercote (njn@valgrind.org)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+#include "events.h"
+
+/* If debugging mode of, dummy functions are provided (see below)
+ */
+#if CLG_ENABLE_DEBUG
+
+/*------------------------------------------------------------*/
+/*--- Debug output helpers                                 ---*/
+/*------------------------------------------------------------*/
+
+static void print_indent(int s)
+{
+    /* max of 40 spaces */
+    const HChar sp[] = "                                        ";
+    if (s>40) s=40;
+    VG_(printf)("%s", sp+40-s);
+}
+
+void CLG_(print_bb)(int s, BB* bb)
+{
+    if (s<0) {
+	s = -s;
+	print_indent(s);
+    }
+
+    VG_(printf)("BB %#lx (Obj '%s')", bb_addr(bb), bb->obj->name);
+}
+
+static
+void print_mangled_cxt(Context* cxt, int rec_index)
+{
+    int i;
+
+    if (!cxt)
+      VG_(printf)("(none)");
+    else {
+      VG_(printf)("%s", cxt->fn[0]->name);
+      if (rec_index >0)
+	VG_(printf)("'%d", rec_index +1);
+      for(i=1;i<cxt->size;i++)
+	VG_(printf)("'%s", cxt->fn[i]->name);
+    }
+}
+
+
+
+void CLG_(print_cxt)(Int s, Context* cxt, int rec_index)
+{
+  if (s<0) {
+    s = -s;
+    print_indent(s);
+  }
+  
+  if (cxt) {
+    UInt *pactive = CLG_(get_fn_entry)(cxt->fn[0]->number);
+    CLG_ASSERT(rec_index < cxt->fn[0]->separate_recursions);
+    
+    VG_(printf)("Cxt %u" ,cxt->base_number + rec_index);
+    if (*pactive>0)
+      VG_(printf)(" [active=%u]", *pactive);
+    VG_(printf)(": ");	
+    print_mangled_cxt(cxt, rec_index);
+    VG_(printf)("\n");
+  }
+  else
+    VG_(printf)("(no context)\n");
+}
+
+void CLG_(print_execstate)(int s, exec_state* es)
+{
+  if (s<0) {
+    s = -s;
+    print_indent(s);
+  }
+  
+  if (!es) {
+    VG_(printf)("ExecState 0x0\n");
+    return;
+  }
+
+  VG_(printf)("ExecState [Sig %d, collect %s, nonskipped %p]: jmps_passed %d\n",
+	      es->sig, es->collect?"yes":"no",
+	      es->nonskipped, es->jmps_passed);
+}
+
+
+void CLG_(print_bbcc)(int s, BBCC* bbcc)
+{
+  BB* bb;
+
+  if (s<0) {
+    s = -s;
+    print_indent(s);
+  }
+  
+  if (!bbcc) {
+    VG_(printf)("BBCC 0x0\n");
+    return;
+  }
+ 
+  bb = bbcc->bb;
+  CLG_ASSERT(bb!=0);
+
+  VG_(printf)("%s +%#lx=%#lx, ",
+	      bb->obj->name + bb->obj->last_slash_pos,
+	      (UWord)bb->offset, bb_addr(bb));
+  CLG_(print_cxt)(s+8, bbcc->cxt, bbcc->rec_index);
+}
+
+void CLG_(print_eventset)(int s, EventSet* es)
+{
+    int i, j;
+    UInt mask;
+    EventGroup* eg;
+
+    if (s<0) {
+	s = -s;
+	print_indent(s);
+    }
+
+    if (!es) {
+	VG_(printf)("(EventSet not set)\n");
+	return;
+    }
+
+    VG_(printf)("EventSet %u (%d groups, size %d):",
+		es->mask, es->count, es->size);
+
+    if (es->count == 0) {
+	VG_(printf)("-\n");
+	return;
+    }
+
+    for(i=0, mask=1; i<MAX_EVENTGROUP_COUNT; i++, mask=mask<<1) {
+	if ((es->mask & mask)==0) continue;
+	eg = CLG_(get_event_group)(i);
+	if (!eg) continue;
+	VG_(printf)(" (%d: %s", i, eg->name[0]);
+	for(j=1; j<eg->size; j++)
+	    VG_(printf)(" %s", eg->name[j]);
+	VG_(printf)(")");
+    }
+    VG_(printf)("\n");
+}
+
+
+void CLG_(print_cost)(int s, EventSet* es, ULong* c)
+{
+    Int i, j, pos, off;
+    UInt mask;
+    EventGroup* eg;
+
+    if (s<0) {
+	s = -s;
+	print_indent(s);
+    }
+
+    if (!es) {
+      VG_(printf)("Cost (Nothing, EventSet not set)\n");
+      return;
+    }
+    if (!c) {
+      VG_(printf)("Cost (Null, EventSet %u)\n", es->mask);
+      return;
+    }
+
+    if (es->size == 0) {
+      VG_(printf)("Cost (Nothing, EventSet with len 0)\n");
+      return;
+    } 
+
+    pos = s;
+    pos += VG_(printf)("Cost [%p]: ", c);
+    off = 0;
+    for(i=0, mask=1; i<MAX_EVENTGROUP_COUNT; i++, mask=mask<<1) {
+	if ((es->mask & mask)==0) continue;
+	eg = CLG_(get_event_group)(i);
+	if (!eg) continue;
+	for(j=0; j<eg->size; j++) {
+
+	    if (off>0) {
+		if (pos > 70) {
+		    VG_(printf)(",\n");
+		    print_indent(s+5);
+		    pos = s+5;
+		}
+		else
+		    pos += VG_(printf)(", ");
+	    }
+
+	    pos += VG_(printf)("%s %llu", eg->name[j], c[off++]);
+	}
+    }
+    VG_(printf)("\n");
+}
+
+
+void CLG_(print_jcc)(int s, jCC* jcc)
+{
+    if (s<0) {
+	s = -s;
+	print_indent(s);
+    }
+
+    if (!jcc) {
+	VG_(printf)("JCC to skipped function\n");
+	return;
+    }
+    VG_(printf)("JCC %p from ", jcc);
+    CLG_(print_bbcc)(s+9, jcc->from);
+    print_indent(s+4);    
+    VG_(printf)("to   ");
+    CLG_(print_bbcc)(s+9, jcc->to);
+    print_indent(s+4);
+    VG_(printf)("Calls %llu\n", jcc->call_counter);
+    print_indent(s+4);
+}
+
+/* dump out the current call stack */
+void CLG_(print_stackentry)(int s, int sp)
+{
+    call_entry* ce;
+
+    if (s<0) {
+	s = -s;
+	print_indent(s);
+    }
+
+    ce = CLG_(get_call_entry)(sp);
+    VG_(printf)("[%-2d] SP %#lx, RA %#lx", sp, ce->sp, ce->ret_addr);
+    if (ce->nonskipped)
+	VG_(printf)(" NonSkipped BB %#lx / %s",
+		    bb_addr(ce->nonskipped->bb),
+		    ce->nonskipped->cxt->fn[0]->name);
+    VG_(printf)("\n");
+    print_indent(s+5);
+    CLG_(print_jcc)(5,ce->jcc);
+}
+
+/* debug output */
+#if 0
+static void print_call_stack()
+{
+    int c;
+
+    VG_(printf)("Call Stack:\n");
+    for(c=0;c<CLG_(current_call_stack).sp;c++)
+      CLG_(print_stackentry)(-2, c);
+}
+#endif
+
+void CLG_(print_bbcc_fn)(BBCC* bbcc)
+{
+    obj_node* obj;
+
+    if (!bbcc) {
+	VG_(printf)("%08x", 0u);
+	return;
+    }
+
+    VG_(printf)("%08lx/%c  %u:", bb_addr(bbcc->bb), 
+		(bbcc->bb->sect_kind == Vg_SectText) ? 'T' :
+		(bbcc->bb->sect_kind == Vg_SectData) ? 'D' :
+		(bbcc->bb->sect_kind == Vg_SectBSS) ? 'B' :
+		(bbcc->bb->sect_kind == Vg_SectGOT) ? 'G' :
+		(bbcc->bb->sect_kind == Vg_SectPLT) ? 'P' : 'U',
+		bbcc->cxt->base_number+bbcc->rec_index);
+    print_mangled_cxt(bbcc->cxt, bbcc->rec_index);
+
+    obj = bbcc->cxt->fn[0]->file->obj;
+    if (obj->name[0])
+	VG_(printf)(" %s", obj->name+obj->last_slash_pos);
+
+    if (VG_(strcmp)(bbcc->cxt->fn[0]->file->name, "???") !=0) {
+	VG_(printf)(" %s", bbcc->cxt->fn[0]->file->name);
+	if ((bbcc->cxt->fn[0] == bbcc->bb->fn) && (bbcc->bb->line>0))
+	    VG_(printf)(":%u", bbcc->bb->line);
+    }
+}	
+
+void CLG_(print_bbcc_cost)(int s, BBCC* bbcc)
+{
+  BB* bb;
+  Int i, cjmpNo;
+  ULong ecounter;
+
+  if (s<0) {
+    s = -s;
+    print_indent(s);
+  }
+  
+  if (!bbcc) {
+    VG_(printf)("BBCC 0x0\n");
+    return;
+  }
+ 
+  bb = bbcc->bb;
+  CLG_ASSERT(bb!=0);
+    
+  CLG_(print_bbcc)(s, bbcc);
+
+  ecounter = bbcc->ecounter_sum;
+
+  print_indent(s+2);
+  VG_(printf)("ECounter: sum %llu ", ecounter);
+  for(i=0; i<bb->cjmp_count; i++) {
+      VG_(printf)("[%u]=%llu ",
+		  bb->jmp[i].instr, bbcc->jmp[i].ecounter);
+  }
+  VG_(printf)("\n");
+
+  cjmpNo = 0; 
+  for(i=0; i<bb->instr_count; i++) {
+      InstrInfo* ii = &(bb->instr[i]);
+      print_indent(s+2);
+      VG_(printf)("[%2d] IOff %2u ecnt %3llu ",
+		  i, ii->instr_offset, ecounter);
+      CLG_(print_cost)(s+5, ii->eventset, bbcc->cost + ii->cost_offset);
+
+      /* update execution counter */
+      if (cjmpNo < bb->cjmp_count)
+	  if (bb->jmp[cjmpNo].instr == i) {
+	      ecounter -= bbcc->jmp[cjmpNo].ecounter;
+	      cjmpNo++;
+	  }
+  }
+}
+
+
+/* dump out an address with source info if available */
+void CLG_(print_addr)(Addr addr)
+{
+    const HChar *fn_buf, *fl_buf, *dir_buf;
+    const HChar* obj_name;
+    DebugInfo* di;
+    UInt ln, i=0, opos=0;
+	
+    if (addr == 0) {
+	VG_(printf)("%08lx", addr);
+	return;
+    }
+
+    CLG_(get_debug_info)(addr, &dir_buf, &fl_buf, &fn_buf, &ln, &di);
+
+    if (VG_(strcmp)(fn_buf,"???")==0)
+	VG_(printf)("%#lx", addr);
+    else
+	VG_(printf)("%#lx %s", addr, fn_buf);
+
+    if (di) {
+      obj_name = VG_(DebugInfo_get_filename)(di);
+      if (obj_name) {
+	while(obj_name[i]) {
+	  if (obj_name[i]=='/') opos = i+1;
+	  i++;
+	}
+	if (obj_name[0])
+	  VG_(printf)(" %s", obj_name+opos);
+      }
+    }
+
+    if (ln>0) {
+       if (dir_buf[0])
+          VG_(printf)(" (%s/%s:%u)", dir_buf, fl_buf, ln);
+       else
+          VG_(printf)(" (%s:%u)", fl_buf, ln);
+    }
+}
+
+void CLG_(print_addr_ln)(Addr addr)
+{
+  CLG_(print_addr)(addr);
+  VG_(printf)("\n");
+}
+
+static ULong bb_written = 0;
+
+void CLG_(print_bbno)(void)
+{
+  if (bb_written != CLG_(stat).bb_executions) {
+    bb_written = CLG_(stat).bb_executions;
+    VG_(printf)("BB# %llu\n",CLG_(stat).bb_executions);
+  }
+}
+
+void CLG_(print_context)(void)
+{
+  BBCC* bbcc;
+
+  CLG_DEBUG(0,"In tid %u [%d] ",
+	   CLG_(current_tid),  CLG_(current_call_stack).sp);
+  bbcc =  CLG_(current_state).bbcc;
+  print_mangled_cxt(CLG_(current_state).cxt,
+		    bbcc ? bbcc->rec_index : 0);
+  VG_(printf)("\n");
+}
+
+void* CLG_(malloc)(const HChar* cc, UWord s, const HChar* f)
+{
+    CLG_DEBUG(3, "Malloc(%lu) in %s.\n", s, f);
+    return VG_(malloc)(cc,s);
+}
+
+#else /* CLG_ENABLE_DEBUG */
+
+void CLG_(print_bbno)(void) {}
+void CLG_(print_context)(void) {}
+void CLG_(print_jcc)(int s, jCC* jcc) {}
+void CLG_(print_bbcc)(int s, BBCC* bbcc) {}
+void CLG_(print_bbcc_fn)(BBCC* bbcc) {}
+void CLG_(print_cost)(int s, EventSet* es, ULong* cost) {}
+void CLG_(print_bb)(int s, BB* bb) {}
+void CLG_(print_cxt)(int s, Context* cxt, int rec_index) {}
+void CLG_(print_short_jcc)(jCC* jcc) {}
+void CLG_(print_stackentry)(int s, int sp) {}
+void CLG_(print_addr)(Addr addr) {}
+void CLG_(print_addr_ln)(Addr addr) {}
+
+#endif
diff --git a/sigrind/events.c b/sigrind/events.c
new file mode 100644
index 000000000..47cf6ee18
--- /dev/null
+++ b/sigrind/events.c
@@ -0,0 +1,261 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                     events.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+/* This should be 2**MAX_EVENTGROUP_COUNT */
+#define MAX_EVENTSET_COUNT 1024
+
+static EventGroup* eventGroup[MAX_EVENTGROUP_COUNT];
+static EventSet* eventSetTable[MAX_EVENTSET_COUNT];
+static Bool eventSets_initialized = 0;
+
+static
+void initialize_event_sets(void)
+{
+    Int i;
+
+    if (eventSets_initialized) return;
+
+    for(i=0; i< MAX_EVENTGROUP_COUNT; i++)
+        eventGroup[i] = 0;
+
+    for(i=0; i< MAX_EVENTSET_COUNT; i++)
+        eventSetTable[i] = 0; 
+
+    eventSets_initialized = 1;
+ }
+
+static
+EventGroup* new_event_group(int id, int n)
+{
+    EventGroup* eg;
+
+    initialize_event_sets();
+
+    CLG_ASSERT(id>=0 && id<MAX_EVENTGROUP_COUNT);
+    CLG_ASSERT(eventGroup[id]==0);
+
+    eg = (EventGroup*) CLG_MALLOC("cl.events.group.1",
+                                  sizeof(EventGroup) + n * sizeof(HChar*));
+    eg->size = n;
+    eventGroup[id] = eg;
+    return eg;
+}
+
+EventGroup* CLG_(register_event_group) (int id, const HChar* n1)
+{
+    EventGroup* eg = new_event_group(id, 1);
+    eg->name[0] = n1;
+
+    return eg;
+}
+
+EventGroup* CLG_(register_event_group2)(int id, const HChar* n1,
+                                        const HChar* n2)
+{
+    EventGroup* eg = new_event_group(id, 2);
+    eg->name[0] = n1;
+    eg->name[1] = n2;
+
+    return eg;
+}
+
+EventGroup* CLG_(register_event_group3)(int id, const HChar* n1,
+                                        const HChar* n2, const HChar* n3)
+{
+    EventGroup* eg = new_event_group(id, 3);
+    eg->name[0] = n1;
+    eg->name[1] = n2;
+    eg->name[2] = n3;
+
+    return eg;
+}
+
+EventGroup* CLG_(register_event_group4)(int id, const HChar* n1,
+                                        const HChar* n2, const HChar* n3,
+                                        const HChar* n4)
+{
+    EventGroup* eg = new_event_group(id, 4);
+    eg->name[0] = n1;
+    eg->name[1] = n2;
+    eg->name[2] = n3;
+    eg->name[3] = n4;
+
+    return eg;
+}
+
+EventGroup* CLG_(get_event_group)(int id)
+{
+    CLG_ASSERT(id>=0 && id<MAX_EVENTGROUP_COUNT);
+
+    return eventGroup[id];
+}
+
+
+static
+EventSet* eventset_from_mask(UInt mask)
+{
+    EventSet* es;
+    Int i, count, offset;
+
+    if (mask >= MAX_EVENTSET_COUNT) return 0;
+
+    initialize_event_sets();
+    if (eventSetTable[mask]) return eventSetTable[mask];
+
+    es = (EventSet*) CLG_MALLOC("cl.events.eventset.1", sizeof(EventSet));
+    es->mask = mask;
+
+    offset = 0;
+    count = 0;
+    for(i=0;i<MAX_EVENTGROUP_COUNT;i++) {
+        es->offset[i] = offset;
+        if ( ((mask & (1u<<i))==0) || (eventGroup[i]==0))
+            continue;
+
+        offset += eventGroup[i]->size;
+        count++;
+    }
+    es->size = offset;
+    es->count = count;
+
+    eventSetTable[mask] = es;
+    return es;
+}
+
+EventSet* CLG_(get_event_set)(Int id)
+{
+    CLG_ASSERT(id>=0 && id<MAX_EVENTGROUP_COUNT);
+    return eventset_from_mask(1u << id);
+}
+
+EventSet* CLG_(get_event_set2)(Int id1, Int id2)
+{
+    CLG_ASSERT(id1>=0 && id1<MAX_EVENTGROUP_COUNT);
+    CLG_ASSERT(id2>=0 && id2<MAX_EVENTGROUP_COUNT);
+    return eventset_from_mask((1u << id1) | (1u << id2));
+}
+
+EventSet* CLG_(add_event_group)(EventSet* es, Int id)
+{
+    CLG_ASSERT(id>=0 && id<MAX_EVENTGROUP_COUNT);
+    if (!es) es = eventset_from_mask(0);
+    return eventset_from_mask(es->mask | (1u << id));
+}
+
+EventSet* CLG_(add_event_group2)(EventSet* es, Int id1, Int id2)
+{
+    CLG_ASSERT(id1>=0 && id1<MAX_EVENTGROUP_COUNT);
+    CLG_ASSERT(id2>=0 && id2<MAX_EVENTGROUP_COUNT);
+    if (!es) es = eventset_from_mask(0);
+    return eventset_from_mask(es->mask | (1u << id1) | (1u << id2));
+}
+
+EventSet* CLG_(add_event_set)(EventSet* es1, EventSet* es2)
+{
+    if (!es1) es1 = eventset_from_mask(0);
+    if (!es2) es2 = eventset_from_mask(0);
+    return eventset_from_mask(es1->mask | es2->mask);
+}
+
+
+
+
+/* Allocate space for an event mapping */
+EventMapping* CLG_(get_eventmapping)(EventSet* es)
+{
+    EventMapping* em;
+
+    CLG_ASSERT(es != 0);
+
+    em = (EventMapping*) CLG_MALLOC("cl.events.geMapping.1",
+                                    sizeof(EventMapping) +
+                                    sizeof(struct EventMappingEntry) *
+                                    es->size);
+    em->capacity = es->size;
+    em->size = 0;
+    em->es = es;
+
+    return em;
+}
+
+void CLG_(append_event)(EventMapping* em, const HChar* n)
+{
+    Int i, j, offset = 0;
+    UInt mask;
+    EventGroup* eg;
+
+    CLG_ASSERT(em != 0);
+    for(i=0, mask=1; i<MAX_EVENTGROUP_COUNT; i++, mask=mask<<1) {
+        if ((em->es->mask & mask)==0) continue;
+        if (eventGroup[i] ==0) continue;
+
+        eg = eventGroup[i];
+        for(j=0; j<eg->size; j++, offset++) {
+            if (VG_(strcmp)(n, eg->name[j])!=0)
+                    continue;
+
+            CLG_ASSERT(em->capacity > em->size);
+            em->entry[em->size].group = i;
+            em->entry[em->size].index = j;
+            em->entry[em->size].offset = offset;
+            em->size++;
+            return;
+        }
+    }
+}
+
+
+/* Returns pointer to dynamically string. The string will be overwritten
+   with each invocation. */
+HChar *CLG_(eventmapping_as_string)(const EventMapping* em)
+{
+    Int i;
+    EventGroup* eg;
+
+    CLG_ASSERT(em != 0);
+
+    XArray *xa = VG_(newXA)(VG_(malloc), "cl.events.emas", VG_(free),
+                            sizeof(HChar));
+
+    for(i=0; i< em->size; i++) {
+        if (i > 0) {
+           VG_(xaprintf)(xa, "%c", ' ');
+        }
+        eg = eventGroup[em->entry[i].group];
+        CLG_ASSERT(eg != 0);
+        VG_(xaprintf)(xa, "%s", eg->name[em->entry[i].index]);
+    }
+    VG_(xaprintf)(xa, "%c", '\0');   // zero terminate the string
+
+    HChar *buf = VG_(strdup)("cl.events.emas", VG_(indexXA)(xa, 0));
+    VG_(deleteXA)(xa);
+
+    return buf;
+}
diff --git a/sigrind/events.h b/sigrind/events.h
new file mode 100644
index 000000000..b38b3c313
--- /dev/null
+++ b/sigrind/events.h
@@ -0,0 +1,133 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                     events.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+/* Abstractions for 64-bit cost lists (events.h) */
+
+#ifndef CLG_EVENTS
+#define CLG_EVENTS
+
+#include "pub_tool_basics.h"
+
+#define CLG_(str) VGAPPEND(vgCallgrind_,str)
+
+/* Event groups consist of one or more named event types.
+ * Event sets are constructed from such event groups.
+ *
+ * Event groups have to be registered globally with a unique ID
+ * before they can be used in an event set.
+ * A group can appear at most once in a event set.
+ */
+
+#define MAX_EVENTGROUP_COUNT 10
+
+typedef struct _EventGroup EventGroup;
+struct _EventGroup {
+    Int size;
+    const HChar* name[0];
+};
+
+/* return 0 if event group can not be registered */
+EventGroup* CLG_(register_event_group) (int id, const HChar*);
+EventGroup* CLG_(register_event_group2)(int id, const HChar*, const HChar*);
+EventGroup* CLG_(register_event_group3)(int id, const HChar*, const HChar*,
+                                        const HChar*);
+EventGroup* CLG_(register_event_group4)(int id, const HChar*, const HChar*,
+                                        const HChar*, const HChar*);
+EventGroup* CLG_(get_event_group)(int id);
+
+/* Event sets are defined by event groups they consist of. */
+
+typedef struct _EventSet EventSet;
+struct _EventSet {
+    /* if subset with ID x is in the set, then bit x is set */
+    UInt mask;
+    Int count;
+    Int size;
+    Int offset[MAX_EVENTGROUP_COUNT];
+ };
+
+/* Same event set is returned when requesting same event groups */
+EventSet* CLG_(get_event_set)(Int id);
+EventSet* CLG_(get_event_set2)(Int id1, Int id2);
+EventSet* CLG_(add_event_group)(EventSet*, Int id);
+EventSet* CLG_(add_event_group2)(EventSet*, Int id1, Int id2);
+EventSet* CLG_(add_event_set)(EventSet*, EventSet*);
+
+
+/* Operations on costs. A cost pointer of 0 means zero cost.
+ * Functions ending in _lz allocate cost arrays only when needed
+ */
+ULong* CLG_(get_eventset_cost)(EventSet*);
+/* Set costs of event set to 0 */
+void CLG_(init_cost)(EventSet*,ULong*);
+/* This always allocates counter and sets them to 0 */
+void CLG_(init_cost_lz)(EventSet*,ULong**);
+/* Set costs of an event set to zero */
+void CLG_(zero_cost)(EventSet*,ULong*);
+Bool CLG_(is_zero_cost)(EventSet*,ULong*);
+void CLG_(copy_cost)(EventSet*,ULong* dst, ULong* src);
+void CLG_(copy_cost_lz)(EventSet*,ULong** pdst, ULong* src);
+void CLG_(add_cost)(EventSet*,ULong* dst, ULong* src);
+void CLG_(add_cost_lz)(EventSet*,ULong** pdst, ULong* src);
+/* Adds src to dst and zeros src. Returns false if nothing changed */
+Bool CLG_(add_and_zero_cost)(EventSet*,ULong* dst, ULong* src);
+Bool CLG_(add_and_zero_cost2)(EventSet*,ULong* dst,EventSet*,ULong* src);
+/* Adds difference of new and old to to dst, and set old to new.
+ * Returns false if nothing changed */
+Bool CLG_(add_diff_cost)(EventSet*,ULong* dst, ULong* old, ULong* new_cost);
+Bool CLG_(add_diff_cost_lz)(EventSet*,ULong** pdst, ULong* old, ULong* new_cost);
+
+/* EventMapping: An ordered subset of events from an event set.
+ * This is used to print out part of an EventSet, or in another order.
+ */
+struct EventMappingEntry {
+    Int group;
+    Int index;
+    Int offset;
+};
+typedef struct _EventMapping EventMapping;
+struct _EventMapping {
+  EventSet* es;
+  Int size;
+  Int capacity;
+  struct EventMappingEntry entry[0];
+};
+
+/* Allocate space for an event mapping */
+EventMapping* CLG_(get_eventmapping)(EventSet*);
+void CLG_(append_event)(EventMapping*, const HChar*);
+/* Returns event mapping as a character string. That string is dynamically
+   allocated and it is the caller's responsibility to free it.
+   The function never returns NULL. */
+HChar *CLG_(eventmapping_as_string)(const EventMapping*);
+/* Returns mapping cost as a character string. That string is dynamically
+   allocated and it is the caller's responsibility to free it.
+   The function never returns NULL. */
+HChar *CLG_(mappingcost_as_string)(const EventMapping*, const ULong*);
+
+#endif /* CLG_EVENTS */
diff --git a/sigrind/fn.c b/sigrind/fn.c
new file mode 100644
index 000000000..243494146
--- /dev/null
+++ b/sigrind/fn.c
@@ -0,0 +1,686 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                      ct_fn.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+#define N_INITIAL_FN_ARRAY_SIZE 10071
+
+static fn_array current_fn_active;
+
+static Addr runtime_resolve_addr = 0;
+static int  runtime_resolve_length = 0;
+
+// a code pattern is a list of tuples (start offset, length)
+struct chunk_t { int start, len; };
+struct pattern
+{
+    const HChar* name;
+    int len;
+    struct chunk_t chunk[];
+};
+
+/* Scan for a pattern in the code of an ELF object.
+ * If found, return true and set runtime_resolve_{addr,length}
+ */
+__attribute__((unused))    // Possibly;  depends on the platform.
+static Bool check_code(obj_node* obj,
+                       UChar code[], struct pattern* pat)
+{
+    Bool found;
+    Addr addr, end;
+    int chunk, start, len;
+
+    /* first chunk of pattern should always start at offset 0 and
+     * have at least 3 bytes */
+    CLG_ASSERT((pat->chunk[0].start == 0) && (pat->chunk[0].len >2));
+    
+    CLG_DEBUG(1, "check_code: %s, pattern %s, check %d bytes of [%x %x %x...]\n",
+              obj->name, pat->name, pat->chunk[0].len, code[0], code[1], code[2]);
+
+    end = obj->start + obj->size - pat->len;
+    addr = obj->start;
+    while(addr < end) {
+	found = (VG_(memcmp)( (void*)addr, code, pat->chunk[0].len) == 0);
+
+        if (found) {
+	    chunk = 1;
+	    while(1) {		
+		start = pat->chunk[chunk].start;
+		len   = pat->chunk[chunk].len;
+		if (len == 0) break;
+
+		CLG_ASSERT(len >2);
+                CLG_DEBUG(1, " found chunk %d at %#lx, checking %d bytes "
+                             "of [%x %x %x...]\n",
+                          chunk-1, addr - obj->start, len,
+			  code[start], code[start+1], code[start+2]);
+
+                if (VG_(memcmp)( (void*)(addr+start), code+start, len) != 0) {
+                    found = False;
+                    break;
+                }
+		chunk++;
+	    }
+
+            if (found) {
+		CLG_DEBUG(1, "found at offset %#lx.\n", addr - obj->start);
+		if (VG_(clo_verbosity) > 1)
+		    VG_(message)(Vg_DebugMsg, "Found runtime_resolve (%s): "
+                                              "%s +%#lx=%#lx, length %d\n",
+				 pat->name, obj->name + obj->last_slash_pos,
+				 addr - obj->start, addr, pat->len);
+		    
+		runtime_resolve_addr   = addr;
+		runtime_resolve_length = pat->len;
+		return True;
+	    }
+        }
+        addr++;
+    }
+    CLG_DEBUG(1, " found nothing.\n");
+    return False;
+}
+
+
+/* _ld_runtime_resolve, located in ld.so, needs special handling:
+ * The jump at end into the resolved function should not be
+ * represented as a call (as usually done in callgrind with jumps),
+ * but as a return + call. Otherwise, the repeated existance of
+ * _ld_runtime_resolve in call chains will lead to huge cycles,
+ * making the profile almost worthless.
+ *
+ * If ld.so is stripped, the symbol will not appear. But as this
+ * function is handcrafted assembler, we search for it.
+ *
+ * We stop if the ELF object name does not seem to be the runtime linker
+ */
+static Bool search_runtime_resolve(obj_node* obj)
+{
+#if defined(VGP_x86_linux)
+    static UChar code[] = {
+	/* 0*/ 0x50, 0x51, 0x52, 0x8b, 0x54, 0x24, 0x10, 0x8b,
+	/* 8*/ 0x44, 0x24, 0x0c, 0xe8, 0x70, 0x01, 0x00, 0x00,
+	/*16*/ 0x5a, 0x59, 0x87, 0x04, 0x24, 0xc2, 0x08, 0x00 };
+    /* Check ranges [0-11] and [16-23] ([12-15] is an absolute address) */
+    static struct pattern pat = {
+	"x86-def", 24, {{ 0,12 }, { 16,8 }, { 24,0}} };
+
+    /* Pattern for glibc-2.8 on OpenSuse11.0 */
+    static UChar code_28[] = {
+	/* 0*/ 0x50, 0x51, 0x52, 0x8b, 0x54, 0x24, 0x10, 0x8b,
+	/* 8*/ 0x44, 0x24, 0x0c, 0xe8, 0x70, 0x01, 0x00, 0x00,
+	/*16*/ 0x5a, 0x8b, 0x0c, 0x24, 0x89, 0x04, 0x24, 0x8b,
+	/*24*/ 0x44, 0x24, 0x04, 0xc2, 0x0c, 0x00 };
+    static struct pattern pat_28 = {
+	"x86-glibc2.8", 30, {{ 0,12 }, { 16,14 }, { 30,0}} };
+
+    if (VG_(strncmp)(obj->name, "/lib/ld", 7) != 0) return False;
+    if (check_code(obj, code, &pat)) return True;
+    if (check_code(obj, code_28, &pat_28)) return True;
+    return False;
+#endif
+
+#if defined(VGP_ppc32_linux)
+    static UChar code[] = {
+	/* 0*/ 0x94, 0x21, 0xff, 0xc0, 0x90, 0x01, 0x00, 0x0c,
+	/* 8*/ 0x90, 0x61, 0x00, 0x10, 0x90, 0x81, 0x00, 0x14,
+	/*16*/ 0x7d, 0x83, 0x63, 0x78, 0x90, 0xa1, 0x00, 0x18,
+	/*24*/ 0x7d, 0x64, 0x5b, 0x78, 0x90, 0xc1, 0x00, 0x1c,
+	/*32*/ 0x7c, 0x08, 0x02, 0xa6, 0x90, 0xe1, 0x00, 0x20,
+	/*40*/ 0x90, 0x01, 0x00, 0x30, 0x91, 0x01, 0x00, 0x24,
+	/*48*/ 0x7c, 0x00, 0x00, 0x26, 0x91, 0x21, 0x00, 0x28,
+	/*56*/ 0x91, 0x41, 0x00, 0x2c, 0x90, 0x01, 0x00, 0x08,
+	/*64*/ 0x48, 0x00, 0x02, 0x91, 0x7c, 0x69, 0x03, 0xa6, /* at 64: bl aff0 <fixup> */
+	/*72*/ 0x80, 0x01, 0x00, 0x30, 0x81, 0x41, 0x00, 0x2c,
+	/*80*/ 0x81, 0x21, 0x00, 0x28, 0x7c, 0x08, 0x03, 0xa6,
+	/*88*/ 0x81, 0x01, 0x00, 0x24, 0x80, 0x01, 0x00, 0x08,
+	/*96*/ 0x80, 0xe1, 0x00, 0x20, 0x80, 0xc1, 0x00, 0x1c,
+	/*104*/0x7c, 0x0f, 0xf1, 0x20, 0x80, 0xa1, 0x00, 0x18,
+	/*112*/0x80, 0x81, 0x00, 0x14, 0x80, 0x61, 0x00, 0x10,
+	/*120*/0x80, 0x01, 0x00, 0x0c, 0x38, 0x21, 0x00, 0x40,
+	/*128*/0x4e, 0x80, 0x04, 0x20 };
+    static struct pattern pat = {
+	"ppc32-def", 132, {{ 0,65 }, { 68,64 }, { 132,0 }} };
+
+    if (VG_(strncmp)(obj->name, "/lib/ld", 7) != 0) return False;
+    return check_code(obj, code, &pat);
+#endif
+
+#if defined(VGP_amd64_linux)
+    static UChar code[] = {
+	/* 0*/ 0x48, 0x83, 0xec, 0x38, 0x48, 0x89, 0x04, 0x24,
+	/* 8*/ 0x48, 0x89, 0x4c, 0x24, 0x08, 0x48, 0x89, 0x54, 0x24, 0x10,
+	/*18*/ 0x48, 0x89, 0x74, 0x24, 0x18, 0x48, 0x89, 0x7c, 0x24, 0x20,
+	/*28*/ 0x4c, 0x89, 0x44, 0x24, 0x28, 0x4c, 0x89, 0x4c, 0x24, 0x30,
+	/*38*/ 0x48, 0x8b, 0x74, 0x24, 0x40, 0x49, 0x89, 0xf3,
+	/*46*/ 0x4c, 0x01, 0xde, 0x4c, 0x01, 0xde, 0x48, 0xc1, 0xe6, 0x03,
+	/*56*/ 0x48, 0x8b, 0x7c, 0x24, 0x38, 0xe8, 0xee, 0x01, 0x00, 0x00,
+	/*66*/ 0x49, 0x89, 0xc3, 0x4c, 0x8b, 0x4c, 0x24, 0x30,
+	/*74*/ 0x4c, 0x8b, 0x44, 0x24, 0x28, 0x48, 0x8b, 0x7c, 0x24, 0x20,
+	/*84*/ 0x48, 0x8b, 0x74, 0x24, 0x18, 0x48, 0x8b, 0x54, 0x24, 0x10,
+	/*94*/ 0x48, 0x8b, 0x4c, 0x24, 0x08, 0x48, 0x8b, 0x04, 0x24,
+	/*103*/0x48, 0x83, 0xc4, 0x48, 0x41, 0xff, 0xe3 };
+    static struct pattern pat = {
+	"amd64-def", 110, {{ 0,62 }, { 66,44 }, { 110,0 }} };
+
+    if ((VG_(strncmp)(obj->name, "/lib/ld", 7) != 0) &&
+	(VG_(strncmp)(obj->name, "/lib64/ld", 9) != 0)) return False;
+    return check_code(obj, code, &pat);
+#endif
+
+    /* For other platforms, no patterns known */
+    return False;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Object/File/Function hash entry operations           ---*/
+/*------------------------------------------------------------*/
+
+/* Object hash table, fixed */
+static obj_node* obj_table[N_OBJ_ENTRIES];
+
+void CLG_(init_obj_table)()
+{
+    Int i;
+    for (i = 0; i < N_OBJ_ENTRIES; i++)
+	obj_table[i] = 0;
+}
+
+#define HASH_CONSTANT   256
+
+static UInt str_hash(const HChar *s, UInt table_size)
+{
+    int hash_value = 0;
+    for ( ; *s; s++)
+        hash_value = (HASH_CONSTANT * hash_value + *s) % table_size;
+    return hash_value;
+}
+
+
+static const HChar* anonymous_obj = "???";
+
+static __inline__ 
+obj_node* new_obj_node(DebugInfo* di, obj_node* next)
+{
+   Int i;
+   obj_node* obj;
+
+   obj = (obj_node*) CLG_MALLOC("cl.fn.non.1", sizeof(obj_node));
+   obj->name  = di ? VG_(strdup)( "cl.fn.non.2",
+                                  VG_(DebugInfo_get_filename)(di) )
+                   : anonymous_obj;
+   for (i = 0; i < N_FILE_ENTRIES; i++) {
+      obj->files[i] = NULL;
+   }
+   CLG_(stat).distinct_objs ++;
+   obj->number  = CLG_(stat).distinct_objs;
+   /* JRS 2008 Feb 19: maybe rename .start/.size/.offset to
+      .text_avma/.text_size/.test_bias to make it clearer what these
+      fields really mean */
+   obj->start   = di ? VG_(DebugInfo_get_text_avma)(di) : 0;
+   obj->size    = di ? VG_(DebugInfo_get_text_size)(di) : 0;
+   obj->offset  = di ? VG_(DebugInfo_get_text_bias)(di) : 0;
+   obj->next    = next;
+
+   // not only used for debug output (see static.c)
+   obj->last_slash_pos = 0;
+   i = 0;
+   while(obj->name[i]) {
+	if (obj->name[i]=='/') obj->last_slash_pos = i+1;
+	i++;
+   }
+
+   if (runtime_resolve_addr == 0) search_runtime_resolve(obj);
+
+   return obj;
+}
+
+obj_node* CLG_(get_obj_node)(DebugInfo* di)
+{
+    obj_node*    curr_obj_node;
+    UInt         objname_hash;
+    const HChar* obj_name;
+    
+    obj_name = di ? VG_(DebugInfo_get_filename)(di) : anonymous_obj;
+
+    /* lookup in obj hash */
+    objname_hash = str_hash(obj_name, N_OBJ_ENTRIES);
+    curr_obj_node = obj_table[objname_hash];
+    while (NULL != curr_obj_node && 
+	   VG_(strcmp)(obj_name, curr_obj_node->name) != 0) {
+	curr_obj_node = curr_obj_node->next;
+    }
+    if (NULL == curr_obj_node) {
+	obj_table[objname_hash] = curr_obj_node = 
+	    new_obj_node(di, obj_table[objname_hash]);
+    }
+
+    return curr_obj_node;
+}
+
+
+static __inline__ 
+file_node* new_file_node(const HChar *filename,
+			 obj_node* obj, file_node* next)
+{
+  Int i;
+  file_node* file = (file_node*) CLG_MALLOC("cl.fn.nfn.1",
+                                           sizeof(file_node));
+  file->name  = VG_(strdup)("cl.fn.nfn.2", filename);
+  for (i = 0; i < N_FN_ENTRIES; i++) {
+    file->fns[i] = NULL;
+  }
+  CLG_(stat).distinct_files++;
+  file->number  = CLG_(stat).distinct_files;
+  file->obj     = obj;
+  file->next      = next;
+  return file;
+}
+
+ 
+file_node* CLG_(get_file_node)(obj_node* curr_obj_node,
+                               const HChar *dir, const HChar *file)
+{
+    file_node* curr_file_node;
+    UInt       filename_hash;
+
+    /* Build up an absolute pathname, if there is a directory available */
+    HChar filename[VG_(strlen)(dir) + 1 + VG_(strlen)(file) + 1];
+    VG_(strcpy)(filename, dir);
+    if (filename[0] != '\0') {
+       VG_(strcat)(filename, "/");
+    }
+    VG_(strcat)(filename, file);
+
+    /* lookup in file hash */
+    filename_hash = str_hash(filename, N_FILE_ENTRIES);
+    curr_file_node = curr_obj_node->files[filename_hash];
+    while (NULL != curr_file_node && 
+	   VG_(strcmp)(filename, curr_file_node->name) != 0) {
+	curr_file_node = curr_file_node->next;
+    }
+    if (NULL == curr_file_node) {
+	curr_obj_node->files[filename_hash] = curr_file_node = 
+	    new_file_node(filename, curr_obj_node, 
+			  curr_obj_node->files[filename_hash]);
+    }
+
+    return curr_file_node;
+}
+
+/* forward decl. */
+static void resize_fn_array(void);
+
+static __inline__ 
+fn_node* new_fn_node(const HChar *fnname,
+		     file_node* file, fn_node* next)
+{
+    fn_node* fn = (fn_node*) CLG_MALLOC("cl.fn.nfnnd.1",
+                                         sizeof(fn_node));
+    fn->name = VG_(strdup)("cl.fn.nfnnd.2", fnname);
+
+    CLG_(stat).distinct_fns++;
+    fn->number   = CLG_(stat).distinct_fns;
+    fn->last_cxt = 0;
+    fn->pure_cxt = 0;
+    fn->file     = file;
+    fn->next     = next;
+
+    fn->dump_before  = False;
+    fn->dump_after   = False;
+    fn->zero_before  = False;
+    fn->toggle_collect = False;
+    fn->skip         = False;
+    fn->pop_on_jump  = CLG_(clo).pop_on_jump;
+    fn->is_malloc    = False;
+    fn->is_realloc   = False;
+    fn->is_free      = False;
+
+    fn->group        = 0;
+    fn->separate_callers    = CLG_(clo).separate_callers;
+    fn->separate_recursions = CLG_(clo).separate_recursions;
+
+#if CLG_ENABLE_DEBUG
+    fn->verbosity    = -1;
+#endif
+
+    if (CLG_(stat).distinct_fns >= current_fn_active.size)
+	resize_fn_array();
+
+    return fn;
+}
+
+
+/* Get a function node in hash2 with known file node.
+ * hash nodes are created if needed
+ */
+static
+fn_node* get_fn_node_infile(file_node* curr_file_node,
+			    const HChar *fnname)
+{
+    fn_node* curr_fn_node;
+    UInt     fnname_hash;
+
+    CLG_ASSERT(curr_file_node != 0);
+
+    /* lookup in function hash */
+    fnname_hash = str_hash(fnname, N_FN_ENTRIES);
+    curr_fn_node = curr_file_node->fns[fnname_hash];
+    while (NULL != curr_fn_node && 
+	   VG_(strcmp)(fnname, curr_fn_node->name) != 0) {
+	curr_fn_node = curr_fn_node->next;
+    }
+    if (NULL == curr_fn_node) {
+	curr_file_node->fns[fnname_hash] = curr_fn_node = 
+            new_fn_node(fnname, curr_file_node,
+			curr_file_node->fns[fnname_hash]);
+    }
+
+    return curr_fn_node;
+}
+
+
+/* Get a function node in a Segment.
+ * Hash nodes are created if needed.
+ */
+static __inline__
+fn_node* get_fn_node_inseg(DebugInfo* di,
+			   const HChar *dirname,
+			   const HChar *filename,
+			   const HChar *fnname)
+{
+  obj_node  *obj  = CLG_(get_obj_node)(di);
+  file_node *file = CLG_(get_file_node)(obj, dirname, filename);
+  fn_node   *fn   = get_fn_node_infile(file, fnname);
+
+  return fn;
+}
+
+
+Bool CLG_(get_debug_info)(Addr instr_addr,
+                          const HChar **dir,
+                          const HChar **file,
+                          const HChar **fn_name, UInt* line_num,
+                          DebugInfo** pDebugInfo)
+{
+  Bool found_file_line, found_fn, result = True;
+  UInt line;
+  
+  CLG_DEBUG(6, "  + get_debug_info(%#lx)\n", instr_addr);
+
+  if (pDebugInfo) {
+      *pDebugInfo = VG_(find_DebugInfo)(instr_addr);
+
+      // for generated code in anonymous space, pSegInfo is 0
+   }
+
+   found_file_line = VG_(get_filename_linenum)(instr_addr,
+					       file,
+					       dir,
+					       &line);
+   found_fn = VG_(get_fnname)(instr_addr, fn_name);
+
+   if (!found_file_line && !found_fn) {
+     CLG_(stat).no_debug_BBs++;
+     *file = "???";
+     *fn_name = "???";
+     if (line_num) *line_num=0;
+     result = False;
+
+   } else if ( found_file_line &&  found_fn) {
+     CLG_(stat).full_debug_BBs++;
+     if (line_num) *line_num=line;
+
+   } else if ( found_file_line && !found_fn) {
+     CLG_(stat).file_line_debug_BBs++;
+     *fn_name = "???";
+     if (line_num) *line_num=line;
+
+   } else  /*(!found_file_line &&  found_fn)*/ {
+     CLG_(stat).fn_name_debug_BBs++;
+     *file = "???";
+     if (line_num) *line_num=0;
+   }
+
+   CLG_DEBUG(6, "  - get_debug_info(%#lx): seg '%s', fn %s\n",
+	    instr_addr,
+	    !pDebugInfo   ? "-" :
+	    (*pDebugInfo) ? VG_(DebugInfo_get_filename)(*pDebugInfo) :
+	    "(None)",
+	    *fn_name);
+
+  return result;
+}
+
+/* for _libc_freeres_wrapper => _exit renaming */
+static BB* exit_bb = 0;
+
+
+/*
+ * Attach function struct to a BB from debug info.
+ */
+fn_node* CLG_(get_fn_node)(BB* bb)
+{
+    const HChar *fnname, *filename, *dirname;
+    DebugInfo* di;
+    UInt       line_num;
+    fn_node*   fn;
+
+    /* fn from debug info is idempotent for a BB */
+    if (bb->fn) return bb->fn;
+
+    CLG_DEBUG(3,"+ get_fn_node(BB %#lx)\n", bb_addr(bb));
+
+    /* get function/file name, line number and object of
+     * the BB according to debug information
+     */
+    CLG_(get_debug_info)(bb_addr(bb),
+                         &dirname, &filename, &fnname, &line_num, &di);
+
+    if (0 == VG_(strcmp)(fnname, "???")) {
+	int p;
+        static HChar buf[32];  // for sure large enough
+	/* Use address as found in library */
+	if (sizeof(Addr) == 4)
+          p = VG_(sprintf)(buf, "%#08lx", (UWord)bb->offset);
+	else 	    
+	    // 64bit address
+          p = VG_(sprintf)(buf, "%#016lx", (UWord)bb->offset);
+
+	VG_(sprintf)(buf + p, "%s", 
+		     (bb->sect_kind == Vg_SectData) ? " [Data]" :
+		     (bb->sect_kind == Vg_SectBSS)  ? " [BSS]"  :
+		     (bb->sect_kind == Vg_SectGOT)  ? " [GOT]"  :
+		     (bb->sect_kind == Vg_SectPLT)  ? " [PLT]"  : "");
+        fnname = buf;
+    }
+    else {
+      if (VG_(get_fnname_if_entry)(bb_addr(bb), &fnname))
+	bb->is_entry = 1;
+    }
+
+    /* HACK for correct _exit: 
+     * _exit is redirected to VG_(__libc_freeres_wrapper) by valgrind,
+     * so we rename it back again :-)
+     */
+    if (0 == VG_(strcmp)(fnname, "vgPlain___libc_freeres_wrapper")
+	&& exit_bb) {
+      CLG_(get_debug_info)(bb_addr(exit_bb),
+                           &dirname, &filename, &fnname, &line_num, &di);
+	
+	CLG_DEBUG(1, "__libc_freeres_wrapper renamed to _exit\n");
+    }
+    if (0 == VG_(strcmp)(fnname, "_exit") && !exit_bb)
+	exit_bb = bb;
+    
+    if (runtime_resolve_addr && 
+	(bb_addr(bb) >= runtime_resolve_addr) &&
+	(bb_addr(bb) < runtime_resolve_addr + runtime_resolve_length)) {
+	/* BB in runtime_resolve found by code check; use this name */
+      fnname = "_dl_runtime_resolve";
+    }
+
+    /* get fn_node struct for this function */
+    fn = get_fn_node_inseg( di, dirname, filename, fnname);
+
+    /* if this is the 1st time the function is seen,
+     * some attributes are set */
+    if (fn->pure_cxt == 0) {
+
+      /* Every function gets a "pure" context, i.e. a context with stack
+       * depth 1 only with this function. This is for compression of mangled
+       * names
+       */
+      fn_node* pure[2];
+      pure[0] = 0;
+      pure[1] = fn;
+      fn->pure_cxt = CLG_(get_cxt)(pure+1);
+
+      if (bb->sect_kind == Vg_SectPLT)	
+	fn->skip = CLG_(clo).skip_plt;
+
+      if (VG_(strcmp)(fn->name, "_dl_runtime_resolve")==0) {
+	  fn->pop_on_jump = True;
+
+	  if (VG_(clo_verbosity) > 1)
+	      VG_(message)(Vg_DebugMsg, "Symbol match: found runtime_resolve:"
+                                        " %s +%#lx=%#lx\n",
+		      bb->obj->name + bb->obj->last_slash_pos,
+                      (UWord)bb->offset, bb_addr(bb));
+      }
+
+      fn->is_malloc  = (VG_(strcmp)(fn->name, "malloc")==0);
+      fn->is_realloc = (VG_(strcmp)(fn->name, "realloc")==0);
+      fn->is_free    = (VG_(strcmp)(fn->name, "free")==0);
+
+      /* apply config options from function name patterns
+       * given on command line */
+      CLG_(update_fn_config)(fn);
+    }
+
+
+    bb->fn   = fn;
+    bb->line = line_num;
+
+    if (dirname[0]) {
+       CLG_DEBUG(3,"- get_fn_node(BB %#lx): %s (in %s:%u)\n",
+                 bb_addr(bb), fnname, filename, line_num);
+    } else
+       CLG_DEBUG(3,"- get_fn_node(BB %#lx): %s (in %s/%s:%u)\n",
+                 bb_addr(bb), fnname, dirname, filename, line_num);
+
+    return fn;
+}
+
+
+/*------------------------------------------------------------*/
+/*--- Active function array operations                     ---*/
+/*------------------------------------------------------------*/
+
+/* The active function array is a thread-specific array
+ * of UInts, mapping function numbers to the active count of
+ * functions.
+ * The active count is the number of times a function appears
+ * in the current call stack, and is used when costs for recursion
+ * levels should be separated.
+ */
+
+UInt* CLG_(get_fn_entry)(Int n)
+{
+  CLG_ASSERT(n < current_fn_active.size);
+  return current_fn_active.array + n;
+}
+
+void CLG_(init_fn_array)(fn_array* a)
+{
+  Int i;
+
+  CLG_ASSERT(a != 0);
+
+  a->size = N_INITIAL_FN_ARRAY_SIZE;
+  if (a->size <= CLG_(stat).distinct_fns)
+    a->size = CLG_(stat).distinct_fns+1;
+  
+  a->array = (UInt*) CLG_MALLOC("cl.fn.gfe.1",
+                                a->size * sizeof(UInt));
+  for(i=0;i<a->size;i++)
+    a->array[i] = 0;
+}
+
+void CLG_(copy_current_fn_array)(fn_array* dst)
+{
+  CLG_ASSERT(dst != 0);
+
+  dst->size  = current_fn_active.size;
+  dst->array = current_fn_active.array;
+}
+
+fn_array* CLG_(get_current_fn_array)()
+{
+  return &current_fn_active;
+}
+
+void CLG_(set_current_fn_array)(fn_array* a)
+{
+  CLG_ASSERT(a != 0);
+
+  current_fn_active.size  = a->size;
+  current_fn_active.array = a->array;
+  if (current_fn_active.size <= CLG_(stat).distinct_fns)
+    resize_fn_array();
+}
+
+/* ensure that active_array is big enough:
+ *  <distinct_fns> is the highest index, so <fn_active_array_size>
+ *  has to be bigger than that.
+ */
+static void resize_fn_array(void)
+{
+    UInt* new_array;
+    Int i;
+
+    UInt newsize = current_fn_active.size;
+    while (newsize <= CLG_(stat).distinct_fns) newsize *=2;
+
+    CLG_DEBUG(0, "Resize fn_active_array: %u => %u\n",
+	     current_fn_active.size, newsize);
+
+    new_array = (UInt*) CLG_MALLOC("cl.fn.rfa.1", newsize * sizeof(UInt));
+    for(i=0;i<current_fn_active.size;i++)
+      new_array[i] = current_fn_active.array[i];
+    while(i<newsize)
+	new_array[i++] = 0;
+
+    VG_(free)(current_fn_active.array);
+    current_fn_active.size = newsize;
+    current_fn_active.array = new_array;
+    CLG_(stat).fn_array_resizes++;
+}
+
+
diff --git a/sigrind/global.h b/sigrind/global.h
new file mode 100644
index 000000000..5ead5a6b6
--- /dev/null
+++ b/sigrind/global.h
@@ -0,0 +1,886 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind data structures, functions.               global.h ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Valgrind, a dynamic binary instrumentation
+   framework.
+
+   Copyright (C) 2004-2015 Josef Weidendorfer
+      josef.weidendorfer@gmx.de
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#ifndef CLG_GLOBAL
+#define CLG_GLOBAL
+
+#include "pub_tool_basics.h"
+#include "pub_tool_vki.h"
+#include "pub_tool_debuginfo.h"
+#include "pub_tool_libcbase.h"
+#include "pub_tool_libcassert.h"
+#include "pub_tool_libcfile.h"
+#include "pub_tool_libcprint.h"
+#include "pub_tool_libcproc.h"
+#include "pub_tool_machine.h"
+#include "pub_tool_mallocfree.h"
+#include "pub_tool_options.h"
+#include "pub_tool_tooliface.h"
+#include "pub_tool_xarray.h"
+#include "pub_tool_clientstate.h"
+#include "pub_tool_machine.h"      // VG_(fnptr_to_fnentry)
+
+#include "events.h" // defines CLG_ macro
+
+#define SGL_(str) VGAPPEND(vgSigrind_,str)
+
+
+/*------------------------------------------------------------*/
+/*--- Callgrind compile options                           --- */
+/*------------------------------------------------------------*/
+
+/* Enable debug output */
+#define CLG_ENABLE_DEBUG 1
+
+/* Enable experimental features? */
+#define CLG_EXPERIMENTAL 0
+
+/* Syscall Timing in microseconds? 
+ * (define to 0 if you get compile errors) */
+#define CLG_MICROSYSTIME 0
+
+
+
+/*------------------------------------------------------------*/
+/*--- Command line options                                 ---*/
+/*------------------------------------------------------------*/
+
+#define DEFAULT_OUTFORMAT   "callgrind.out.%p"
+
+typedef struct _SglCommandLineOptions SglCommandLineOptions;
+struct _SglCommandLineOptions {
+  const HChar* ipc_dir;
+  const HChar* collect_func;
+  const HChar* start_collect_func;
+  const HChar* stop_collect_func;
+  Bool gen_mem;
+  Bool gen_comp;
+  Bool gen_cf;
+  Bool gen_sync;
+  Bool gen_instr;
+  Bool gen_bb;
+  Bool gen_fn;
+  Bool gen_thr;
+};
+
+typedef struct _CommandLineOptions CommandLineOptions;
+struct _CommandLineOptions {
+
+  /* Dump format options */
+  const HChar* out_format;  /* Format string for callgrind output file name */
+  Bool combine_dumps;       /* Dump trace parts into same file? */
+  Bool compress_strings;
+  Bool compress_events;
+  Bool compress_pos;
+  Bool mangle_names;
+  Bool compress_mangled;
+  Bool dump_line;
+  Bool dump_instr;
+  Bool dump_bb;
+  Bool dump_bbs;         /* Dump basic block information? */
+  
+  /* Dump generation options */
+  ULong dump_every_bb;     /* Dump every xxx BBs. */
+  
+  /* Collection options */
+  Bool separate_threads; /* Separate threads in dump? */
+  Int  separate_callers; /* Separate dependent on how many callers? */
+  Int  separate_recursions; /* Max level of recursions to separate */
+  Bool skip_plt;         /* Skip functions in PLT section? */
+  Bool skip_direct_recursion; /* Increment direct recursions the level? */
+
+  Bool collect_atstart;  /* Start in collecting state ? */
+  Bool collect_jumps;    /* Collect (cond.) jumps in functions ? */
+
+  Bool collect_alloc;    /* Collect size of allocated memory */
+  Bool collect_systime;  /* Collect time for system calls */
+
+  Bool collect_bus;      /* Collect global bus events */
+
+  /* Instrument options */
+  Bool instrument_atstart;  /* Instrument at start? */
+  Bool simulate_cache;      /* Call into cache simulator ? */
+  Bool simulate_branch;     /* Call into branch prediction simulator ? */
+
+  /* Call graph generation */
+  Bool pop_on_jump;       /* Handle a jump between functions as ret+call */
+
+#if CLG_ENABLE_DEBUG
+  Int   verbose;
+  ULong verbose_start;
+#endif
+};
+
+/*------------------------------------------------------------*/
+/*--- Constants                                            ---*/
+/*------------------------------------------------------------*/
+
+/* Minimum cache line size allowed */
+#define MIN_LINE_SIZE   16
+
+
+/*------------------------------------------------------------*/
+/*--- Statistics                                           ---*/
+/*------------------------------------------------------------*/
+
+typedef struct _Statistics Statistics;
+struct _Statistics {
+  ULong call_counter;
+  ULong jcnd_counter;
+  ULong jump_counter;
+  ULong rec_call_counter;
+  ULong ret_counter;
+  ULong bb_executions;
+
+  Int  context_counter;
+  Int  bb_retranslations;  
+
+  Int  distinct_objs;
+  Int  distinct_files;
+  Int  distinct_fns;
+  Int  distinct_contexts;
+  Int  distinct_bbs;
+  Int  distinct_jccs;
+  Int  distinct_bbccs;
+  Int  distinct_instrs;
+  Int  distinct_skips;
+
+  Int  bb_hash_resizes;
+  Int  bbcc_hash_resizes;
+  Int  jcc_hash_resizes;
+  Int  cxt_hash_resizes;
+  Int  fn_array_resizes;
+  Int  call_stack_resizes;
+  Int  fn_stack_resizes;
+
+  Int  full_debug_BBs;
+  Int  file_line_debug_BBs;
+  Int  fn_name_debug_BBs;
+  Int  no_debug_BBs;
+  Int  bbcc_lru_misses;
+  Int  jcc_lru_misses;
+  Int  cxt_lru_misses;
+  Int  bbcc_clones;
+};
+
+
+/*------------------------------------------------------------*/
+/*--- Structure declarations                               ---*/
+/*------------------------------------------------------------*/
+
+typedef struct _Context     Context;
+typedef struct _CC          CC;
+typedef struct _BB          BB;
+typedef struct _BBCC        BBCC;
+typedef struct _jCC         jCC;
+typedef struct _fCC         fCC;
+typedef struct _fn_node     fn_node;
+typedef struct _file_node   file_node;
+typedef struct _obj_node    obj_node;
+typedef struct _fn_config   fn_config;
+typedef struct _call_entry  call_entry;
+typedef struct _thread_info thread_info;
+
+/* Costs of event sets. Aliases to arrays of 64-bit values */
+typedef ULong* SimCost;  /* All events the simulator can produce */
+typedef ULong* UserCost;
+typedef ULong* FullCost; /* Simulator + User */
+
+
+/* The types of control flow changes that can happen between
+ * execution of two BBs in a thread.
+ */
+typedef enum {
+  jk_None = 0,   /* no explicit change by a guest instruction */
+  jk_Jump,       /* regular jump */
+  jk_Call,
+  jk_Return,
+  jk_CondJump    /* conditional jump taken (only used as jCC type) */
+} ClgJumpKind;
+
+
+/* JmpCall cost center
+ * for subroutine call (from->bb->jmp_addr => to->bb->addr)
+ *
+ * Each BB has at most one CALL instruction. The list of JCC from
+ * this call is a pointer to the list head (stored in BBCC), and
+ * <next_from> in the JCC struct.
+ *
+ * For fast lookup, JCCs are reachable with a hash table, keyed by
+ * the (from_bbcc,to) pair. <next_hash> is used for the JCC chain
+ * of one hash table entry.
+ *
+ * Cost <sum> holds event counts for already returned executions.
+ * <last> are the event counters at last enter of the subroutine.
+ * <sum> is updated on returning from the subroutine by
+ * adding the diff of <last> and current event counters to <sum>.
+ *
+ * After updating, <last> is set to current event counters. Thus,
+ * events are not counted twice for recursive calls (TODO: True?)
+ */
+
+struct _jCC {
+  ClgJumpKind jmpkind; /* jk_Call, jk_Jump, jk_CondJump */
+  jCC* next_hash;   /* for hash entry chain */
+  jCC* next_from;   /* next JCC from a BBCC */
+  BBCC *from, *to;  /* call arc from/to this BBCC */
+  UInt jmp;         /* jump no. in source */
+
+  ULong call_counter; /* no wraparound with 64 bit */
+
+  FullCost cost; /* simulator + user counters */
+};
+
+
+/* 
+ * Info for one instruction of a basic block.
+ */
+typedef struct _InstrInfo InstrInfo;
+struct _InstrInfo {
+  UInt instr_offset;
+  Addr instr_addr;
+  UInt instr_size;
+  UInt cost_offset;
+  EventSet* eventset;
+};
+
+
+
+/*
+ * Info for a side exit in a BB
+ */
+typedef struct _CJmpInfo CJmpInfo;
+struct _CJmpInfo {
+  UInt instr;          /* instruction index for BB.instr array */
+  ClgJumpKind jmpkind; /* jump kind when leaving BB at this side exit */
+};
+
+
+/**
+ * An instrumented basic block (BB).
+ *
+ * BBs are put into a resizable hash to allow for fast detection if a
+ * BB is to be retranslated but cost info is already available.
+ * The key for a BB is a (object, offset) tupel making it independent
+ * from possibly multiple mappings of the same ELF object.
+ *
+ * At the beginning of each instrumented BB,
+ * a call to setup_bbcc(), specifying a pointer to the
+ * according BB structure, is added.
+ *
+ * As cost of a BB has to be distinguished depending on the context,
+ * multiple cost centers for one BB (struct BBCC) exist and the according
+ * BBCC is set by setup_bbcc.
+ */
+struct _BB {
+  obj_node*  obj;         /* ELF object of BB */
+  PtrdiffT   offset;      /* offset of BB in ELF object file */
+  BB*        next;       /* chaining for a hash entry */
+
+  VgSectKind sect_kind;  /* section of this BB, e.g. PLT */
+  UInt       instr_count;
+  
+  /* filled by CLG_(get_fn_node) if debug info is available */
+  fn_node*   fn;          /* debug info for this BB */
+  UInt       line;
+  Bool       is_entry;    /* True if this BB is a function entry */
+        
+  BBCC*      bbcc_list;  /* BBCCs for same BB (see next_bbcc in BBCC) */
+  BBCC*      last_bbcc;  /* Temporary: Cached for faster access (LRU) */
+
+  /* filled by CLG_(instrument) if not seen before */
+  UInt       cjmp_count;  /* number of side exits */
+  CJmpInfo*  jmp;         /* array of info for condition jumps,
+			   * allocated directly after this struct */
+  Bool       cjmp_inverted; /* is last side exit actually fall through? */
+
+  UInt       instr_len;
+  UInt       cost_count;
+  InstrInfo  instr[0];   /* info on instruction sizes and costs */
+};
+
+
+
+/**
+ * Function context
+ *
+ * Basic blocks are always executed in the scope of a context.
+ * A function context is a list of function nodes representing
+ * the call chain to the current context: I.e. fn[0] is the
+ * function we are currently in, fn[1] has called fn[0], and so on.
+ * Recursion levels are used for fn[0].
+ *
+ * To get a unique number for a full execution context, use
+ *  rec_index = min(<fn->rec_separation>,<active>) - 1;
+ *  unique_no = <number> + rec_index
+ *
+ * For each Context, recursion index and BB, there can be a BBCC.
+ */
+struct _Context {
+    UInt size;        // number of function dependencies
+    UInt base_number; // for context compression & dump array
+    Context* next;    // entry chaining for hash
+    UWord hash;       // for faster lookup...
+    fn_node* fn[0];
+};
+
+
+/*
+ * Cost info for a side exits from a BB
+ */
+typedef struct _JmpData JmpData;
+struct _JmpData {
+    ULong ecounter; /* number of times the BB was left at this exit */
+    jCC*  jcc_list; /* JCCs used for this exit */
+};
+
+
+/*
+ * Basic Block Cost Center
+ *
+ * On demand, multiple BBCCs will be created for the same BB
+ * dependend on command line options and:
+ * - current function (it's possible that a BB is executed in the
+ *   context of different functions, e.g. in manual assembler/PLT)
+ * - current thread ID
+ * - position where current function is called from
+ * - recursion level of current function
+ *
+ * The cost centres for the instructions of a basic block are
+ * stored in a contiguous array.
+ * They are distinguishable by their tag field.
+ */
+struct _BBCC {
+    BB*      bb;           /* BB for this cost center */
+
+    Context* cxt;          /* execution context of this BBCC */
+    ThreadId tid;          /* only for assertion check purpose */
+    UInt     rec_index;    /* Recursion index in rec->bbcc for this bbcc */
+    BBCC**   rec_array;    /* Variable sized array of pointers to 
+			    * recursion BBCCs. Shared. */
+    ULong    ret_counter;  /* how often returned from jccs of this bbcc;
+			    * used to check if a dump for this BBCC is needed */
+    
+    BBCC*    next_bbcc;    /* Chain of BBCCs for same BB */
+    BBCC*    lru_next_bbcc; /* BBCC executed next the last time */
+    
+    jCC*     lru_from_jcc; /* Temporary: Cached for faster access (LRU) */
+    jCC*     lru_to_jcc;   /* Temporary: Cached for faster access (LRU) */
+    FullCost skipped;      /* cost for skipped functions called from 
+			    * jmp_addr. Allocated lazy */
+    
+    BBCC*    next;         /* entry chain in hash */
+    ULong*   cost;         /* start of 64bit costs for this BBCC */
+    ULong    ecounter_sum; /* execution counter for first instruction of BB */
+    JmpData  jmp[0];
+};
+
+
+/* the <number> of fn_node, file_node and obj_node are for compressed dumping
+ * and a index into the dump boolean table and fn_info_table
+ */
+
+struct _fn_node {
+  HChar*     name;
+  UInt       number;
+  Context*   last_cxt; /* LRU info */
+  Context*   pure_cxt; /* the context with only the function itself */
+  file_node* file;     /* reverse mapping for 2nd hash */
+  fn_node* next;
+
+  Bool dump_before :1;
+  Bool dump_after :1;
+  Bool zero_before :1;
+  Bool toggle_collect :1;
+  Bool skip :1;
+  Bool pop_on_jump : 1;
+
+  Bool is_malloc :1;
+  Bool is_realloc :1;
+  Bool is_free :1;
+
+  Int  group;
+  Int  separate_callers;
+  Int  separate_recursions;
+#if CLG_ENABLE_DEBUG
+  Int  verbosity; /* Stores old verbosity level while in function */
+#endif
+};
+
+/* Quite arbitrary fixed hash sizes */
+
+#define   N_OBJ_ENTRIES         47
+#define  N_FILE_ENTRIES         53
+#define    N_FN_ENTRIES         87
+
+struct _file_node {
+   HChar*     name;
+   fn_node*   fns[N_FN_ENTRIES];
+   UInt       number;
+   obj_node*  obj;
+   file_node* next;
+};
+
+/* If an object is dlopened multiple times, we hope that <name> is unique;
+ * <start> and <offset> can change with each dlopen, and <start> is
+ * zero when object is unmapped (possible at dump time).
+ */
+struct _obj_node {
+   const HChar* name;
+   UInt       last_slash_pos;
+
+   Addr       start;  /* Start address of text segment mapping */
+   SizeT      size;   /* Length of mapping */
+   PtrdiffT   offset; /* Offset between symbol address and file offset */
+
+   file_node* files[N_FILE_ENTRIES];
+   UInt       number;
+   obj_node*  next;
+};
+
+/* an entry in the callstack
+ *
+ * <nonskipped> is 0 if the function called is not skipped (usual case).
+ * Otherwise, it is the last non-skipped BBCC. This one gets all
+ * the calls to non-skipped functions and all costs in skipped 
+ * instructions.
+ */
+struct _call_entry {
+    jCC* jcc;           /* jCC for this call */
+    FullCost enter_cost; /* cost event counters at entering frame */
+    Addr sp;            /* stack pointer directly after call */
+    Addr ret_addr;      /* address to which to return to
+			 * is 0 on a simulated call */
+    BBCC* nonskipped;   /* see above */
+    Context* cxt;       /* context before call */
+    Int fn_sp;          /* function stack index before call */
+};
+
+
+/*
+ * Execution state of main thread or a running signal handler in
+ * a thread while interrupted by another signal handler.
+ * As there's no scheduling among running signal handlers of one thread,
+ * we only need a subset of a full thread state:
+ * - event counter
+ * - collect state
+ * - last BB, last jump kind, last nonskipped BB
+ * - callstack pointer for sanity checking and correct unwinding
+ *   after exit
+ */
+typedef struct _exec_state exec_state;
+struct _exec_state {
+
+  /* the signum of the handler, 0 for main thread context
+   */
+  Int sig;
+  
+  /* the old call stack pointer at entering the signal handler */
+  Int orig_sp;
+  
+  FullCost cost;
+  Bool     collect;
+  Context* cxt;
+  
+  /* number of conditional jumps passed in last BB */
+  Int   jmps_passed;
+  BBCC* bbcc;      /* last BB executed */
+  BBCC* nonskipped;
+
+  Int call_stack_bottom; /* Index into fn_stack */
+};
+
+/* Global state structures */
+typedef struct _bb_hash bb_hash;
+struct _bb_hash {
+  UInt size, entries;
+  BB** table;
+};
+
+typedef struct _cxt_hash cxt_hash;
+struct _cxt_hash {
+  UInt size, entries;
+  Context** table;
+};  
+
+/* Thread specific state structures, i.e. parts of a thread state.
+ * There are variables for the current state of each part,
+ * on which a thread state is copied at thread switch.
+ */
+typedef struct _bbcc_hash bbcc_hash;
+struct _bbcc_hash {
+  UInt size, entries;
+  BBCC** table;
+};
+
+typedef struct _jcc_hash jcc_hash;
+struct _jcc_hash {
+  UInt size, entries;
+  jCC** table;
+  jCC* spontaneous;
+};
+
+typedef struct _fn_array fn_array;
+struct _fn_array {
+  UInt size;
+  UInt* array;
+};
+
+typedef struct _call_stack call_stack;
+struct _call_stack {
+  UInt size;
+  Int sp;
+  call_entry* entry;
+};
+
+typedef struct _fn_stack fn_stack;
+struct _fn_stack {
+  UInt size;
+  fn_node **bottom, **top;
+};
+
+/* The maximum number of simultaneous running signal handlers per thread.
+ * This is the number of execution states storable in a thread.
+ */
+#define MAX_SIGHANDLERS 10
+
+typedef struct _exec_stack exec_stack;
+struct _exec_stack {
+  Int sp; /* > 0 if a handler is running */
+  exec_state* entry[MAX_SIGHANDLERS];
+};
+
+/* Thread State 
+ *
+ * This structure stores thread specific info while a thread is *not*
+ * running. See function switch_thread() for save/restore on thread switch.
+ *
+ * If --separate-threads=no, BBCCs and JCCs can be shared by all threads, i.e.
+ * only structures of thread 1 are used.
+ * This involves variables fn_info_table, bbcc_table and jcc_table.
+ */
+struct _thread_info {
+
+  /* state */
+  fn_stack fns;       /* function stack */
+  call_stack calls;   /* context call arc stack */
+  exec_stack states;  /* execution states interrupted by signals */
+
+  /* dump statistics */
+  FullCost lastdump_cost;    /* Cost at last dump */
+  FullCost sighandler_cost;
+
+  /* thread specific data structure containers */
+  fn_array fn_active;
+  jcc_hash jccs;
+  bbcc_hash bbccs;
+};
+
+/* Structs used for dumping */
+
+/* Address position inside of a BBCC:
+ * This includes
+ * - the address offset from the BB start address
+ * - file/line from debug info for that address (can change inside a BB)
+ */
+typedef struct _AddrPos AddrPos;
+struct _AddrPos {
+    Addr addr;
+    Addr bb_addr;
+    file_node* file;
+    UInt line;
+};
+
+/* a simulator cost entity that can be written out in one line */
+typedef struct _AddrCost AddrCost;
+struct _AddrCost {
+    AddrPos p;
+    SimCost cost;
+};
+
+/* A function in an execution context */
+typedef struct _FnPos FnPos;
+struct _FnPos {
+    file_node* file;
+    fn_node* fn;
+    obj_node* obj;
+    Context* cxt;
+    int rec_index;
+    UInt line;
+};
+
+/*------------------------------------------------------------*/
+/*--- Cache simulator interface                            ---*/
+/*------------------------------------------------------------*/
+
+struct cachesim_if
+{
+    void (*print_opts)(void);
+    Bool (*parse_opt)(const HChar* arg);
+    void (*post_clo_init)(void);
+    void (*clear)(void);
+    void (*dump_desc)(VgFile *fp);
+    void (*printstat)(Int,Int,Int);
+    void (*add_icost)(SimCost, BBCC*, InstrInfo*, ULong);
+    void (*finish)(void);
+    
+    void (*log_1I0D)(InstrInfo*) VG_REGPARM(1);
+    void (*log_2I0D)(InstrInfo*, InstrInfo*) VG_REGPARM(2);
+    void (*log_3I0D)(InstrInfo*, InstrInfo*, InstrInfo*) VG_REGPARM(3);
+
+    void (*log_1I1Dr)(InstrInfo*, Addr, Word) VG_REGPARM(3);
+    void (*log_1I1Dw)(InstrInfo*, Addr, Word) VG_REGPARM(3);
+
+    void (*log_0I1Dr)(InstrInfo*, Addr, Word) VG_REGPARM(3);
+    void (*log_0I1Dw)(InstrInfo*, Addr, Word) VG_REGPARM(3);
+
+    // function names of helpers (for debugging generated code)
+    const HChar *log_1I0D_name, *log_2I0D_name, *log_3I0D_name;
+    const HChar *log_1I1Dr_name, *log_1I1Dw_name;
+    const HChar *log_0I1Dr_name, *log_0I1Dw_name;
+};
+
+// Event groups
+#define EG_USE   0
+#define EG_IR    1
+#define EG_DR    2
+#define EG_DW    3
+#define EG_BC    4
+#define EG_BI    5
+#define EG_BUS   6
+#define EG_ALLOC 7
+#define EG_SYS   8
+
+struct event_sets {
+    EventSet *base, *full;
+};
+
+#define fullOffset(group) (CLG_(sets).full->offset[group])
+
+
+/*------------------------------------------------------------*/
+/*--- Functions                                            ---*/
+/*------------------------------------------------------------*/
+
+/* from clo.c */
+
+void SGL_(set_clo_defaults)(void);
+void CLG_(set_clo_defaults)(void);
+void CLG_(update_fn_config)(fn_node*);
+Bool CLG_(process_cmd_line_option)(const HChar*);
+void CLG_(print_usage)(void);
+void CLG_(print_debug_usage)(void);
+
+/* from sim.c */
+void CLG_(init_eventsets)(void);
+
+/* from main.c */
+Bool CLG_(get_debug_info)(Addr, const HChar **dirname,
+                          const HChar **filename,
+                          const HChar **fn_name, UInt*, DebugInfo**);
+void CLG_(collectBlockInfo)(IRSB* bbIn, UInt*, UInt*, Bool*);
+void CLG_(set_instrument_state)(const HChar*,Bool);
+void CLG_(dump_profile)(const HChar* trigger,Bool only_current_thread);
+void CLG_(zero_all_cost)(Bool only_current_thread);
+Int CLG_(get_dump_counter)(void);
+void CLG_(fini)(Int exitcode);
+
+/* from bb.c */
+void CLG_(init_bb_hash)(void);
+bb_hash* CLG_(get_bb_hash)(void);
+BB*  CLG_(get_bb)(Addr addr, IRSB* bb_in, Bool *seen_before);
+void CLG_(delete_bb)(Addr addr);
+
+static __inline__ Addr bb_addr(BB* bb)
+ { return bb->offset + bb->obj->offset; }
+static __inline__ Addr bb_jmpaddr(BB* bb)
+ { UInt off = (bb->instr_count > 0) ? bb->instr[bb->instr_count-1].instr_offset : 0;
+   return off + bb->offset + bb->obj->offset; }
+
+/* from fn.c */
+void CLG_(init_fn_array)(fn_array*);
+void CLG_(copy_current_fn_array)(fn_array* dst);
+fn_array* CLG_(get_current_fn_array)(void);
+void CLG_(set_current_fn_array)(fn_array*);
+UInt* CLG_(get_fn_entry)(Int n);
+
+void      CLG_(init_obj_table)(void);
+obj_node* CLG_(get_obj_node)(DebugInfo* si);
+file_node* CLG_(get_file_node)(obj_node*, const HChar *dirname,
+                               const HChar* filename);
+fn_node*  CLG_(get_fn_node)(BB* bb);
+
+/* from bbcc.c */
+void CLG_(init_bbcc_hash)(bbcc_hash* bbccs);
+void CLG_(copy_current_bbcc_hash)(bbcc_hash* dst);
+bbcc_hash* CLG_(get_current_bbcc_hash)(void);
+void CLG_(set_current_bbcc_hash)(bbcc_hash*);
+void CLG_(forall_bbccs)(void (*func)(BBCC*));
+void CLG_(zero_bbcc)(BBCC* bbcc);
+BBCC* CLG_(get_bbcc)(BB* bb);
+BBCC* CLG_(clone_bbcc)(BBCC* orig, Context* cxt, Int rec_index);
+void CLG_(setup_bbcc)(BB* bb) VG_REGPARM(1);
+
+
+/* from jumps.c */
+void CLG_(init_jcc_hash)(jcc_hash*);
+void CLG_(copy_current_jcc_hash)(jcc_hash* dst);
+void CLG_(set_current_jcc_hash)(jcc_hash*);
+jCC* CLG_(get_jcc)(BBCC* from, UInt, BBCC* to);
+
+/* from callstack.c */
+void CLG_(init_call_stack)(call_stack*);
+void CLG_(copy_current_call_stack)(call_stack* dst);
+void CLG_(set_current_call_stack)(call_stack*);
+call_entry* CLG_(get_call_entry)(Int n);
+
+void CLG_(push_call_stack)(BBCC* from, UInt jmp, BBCC* to, Addr sp, Bool skip);
+void CLG_(pop_call_stack)(void);
+Int CLG_(unwind_call_stack)(Addr sp, Int);
+
+/* from context.c */
+void CLG_(init_fn_stack)(fn_stack*);
+void CLG_(copy_current_fn_stack)(fn_stack*);
+void CLG_(set_current_fn_stack)(fn_stack*);
+
+void CLG_(init_cxt_table)(void);
+Context* CLG_(get_cxt)(fn_node** fn);
+void CLG_(push_cxt)(fn_node* fn);
+
+/* from threads.c */
+void CLG_(init_threads)(void);
+thread_info** CLG_(get_threads)(void);
+thread_info* CLG_(get_current_thread)(void);
+void CLG_(switch_thread)(ThreadId tid);
+void CLG_(forall_threads)(void (*func)(thread_info*));
+void CLG_(run_thread)(ThreadId tid);
+void SGL_(switch_thread)(ThreadId tid);
+
+void CLG_(init_exec_state)(exec_state* es);
+void CLG_(init_exec_stack)(exec_stack*);
+void CLG_(copy_current_exec_stack)(exec_stack*);
+void CLG_(set_current_exec_stack)(exec_stack*);
+void CLG_(pre_signal)(ThreadId tid, Int sigNum, Bool alt_stack);
+void CLG_(post_signal)(ThreadId tid, Int sigNum);
+void CLG_(run_post_signal_on_call_stack_bottom)(void);
+
+/* from dump.c */
+void CLG_(init_dumps)(void);
+
+/*------------------------------------------------------------*/
+/*--- Exported global variables                            ---*/
+/*------------------------------------------------------------*/
+
+extern Bool SGL_(is_in_event_collect_func); //ML: long name? feel free to change
+extern SglCommandLineOptions SGL_(clo);
+extern Bool* SGL_(thread_in_synccall);
+extern ThreadId SGL_(active_tid);
+
+#define EVENT_GENERATION_ENABLED  \
+   (!SGL_(thread_in_synccall)[SGL_(active_tid)] && (SGL_(is_in_event_collect_func)))
+
+extern CommandLineOptions CLG_(clo);
+extern Statistics CLG_(stat);
+extern EventMapping* CLG_(dumpmap);
+
+/* Function active counter array, indexed by function number */
+extern UInt* CLG_(fn_active_array);
+extern Bool CLG_(instrument_state);
+ /* min of L1 and LL cache line sizes */
+extern Int CLG_(min_line_size);
+extern call_stack CLG_(current_call_stack);
+extern fn_stack   CLG_(current_fn_stack);
+extern exec_state CLG_(current_state);
+extern ThreadId   CLG_(current_tid);
+extern FullCost   CLG_(total_cost);
+extern struct cachesim_if CLG_(cachesim);
+extern struct event_sets  CLG_(sets);
+
+// set by setup_bbcc at start of every BB, and needed by log_* helpers
+extern Addr   CLG_(bb_base);
+extern ULong* CLG_(cost_base);
+
+
+/*------------------------------------------------------------*/
+/*--- Debug output                                         ---*/
+/*------------------------------------------------------------*/
+
+#if CLG_ENABLE_DEBUG
+
+#define CLG_DEBUGIF(x) \
+  if (UNLIKELY( (CLG_(clo).verbose >x) && \
+                (CLG_(stat).bb_executions >= CLG_(clo).verbose_start)))
+
+#define CLG_DEBUG(x,format,args...)   \
+    CLG_DEBUGIF(x) {                  \
+      CLG_(print_bbno)();	      \
+      VG_(printf)(format,##args);     \
+    }
+
+#define CLG_ASSERT(cond)              \
+    if (UNLIKELY(!(cond))) {          \
+      CLG_(print_context)();          \
+      CLG_(print_bbno)();	      \
+      tl_assert(cond);                \
+     }
+
+#else
+#define CLG_DEBUGIF(x) if (0)
+#define CLG_DEBUG(x...) {}
+#define CLG_ASSERT(cond) tl_assert(cond);
+#endif
+
+/* from debug.c */
+void CLG_(print_bbno)(void);
+void CLG_(print_context)(void);
+void CLG_(print_jcc)(int s, jCC* jcc);
+void CLG_(print_bbcc)(int s, BBCC* bbcc);
+void CLG_(print_bbcc_fn)(BBCC* bbcc);
+void CLG_(print_execstate)(int s, exec_state* es);
+void CLG_(print_eventset)(int s, EventSet* es);
+void CLG_(print_cost)(int s, EventSet*, ULong* cost);
+void CLG_(print_bb)(int s, BB* bb);
+void CLG_(print_bbcc_cost)(int s, BBCC*);
+void CLG_(print_cxt)(int s, Context* cxt, int rec_index);
+void CLG_(print_short_jcc)(jCC* jcc);
+void CLG_(print_stackentry)(int s, int sp);
+void CLG_(print_addr)(Addr addr);
+void CLG_(print_addr_ln)(Addr addr);
+
+void* CLG_(malloc)(const HChar* cc, UWord s, const HChar* f);
+void* CLG_(free)(void* p, const HChar* f);
+#if 0
+#define CLG_MALLOC(_cc,x) CLG_(malloc)((_cc),x,__FUNCTION__)
+#define CLG_FREE(p)       CLG_(free)(p,__FUNCTION__)
+#else
+#define CLG_MALLOC(_cc,x) VG_(malloc)((_cc),x)
+#define CLG_FREE(p)       VG_(free)(p)
+#endif
+
+#endif /* CLG_GLOBAL */
diff --git a/sigrind/jumps.c b/sigrind/jumps.c
new file mode 100644
index 000000000..dbd453353
--- /dev/null
+++ b/sigrind/jumps.c
@@ -0,0 +1,233 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                   ct_jumps.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "global.h"
+
+/*------------------------------------------------------------*/
+/*--- Jump Cost Center (JCC) operations, including Calls   ---*/
+/*------------------------------------------------------------*/
+
+#define N_JCC_INITIAL_ENTRIES  4437
+
+static jcc_hash current_jccs;
+
+void CLG_(init_jcc_hash)(jcc_hash* jccs)
+{
+   Int i;
+
+   CLG_ASSERT(jccs != 0);
+
+   jccs->size    = N_JCC_INITIAL_ENTRIES;
+   jccs->entries = 0;
+   jccs->table = (jCC**) CLG_MALLOC("cl.jumps.ijh.1",
+                                    jccs->size * sizeof(jCC*));
+   jccs->spontaneous = 0;
+
+   for (i = 0; i < jccs->size; i++)
+     jccs->table[i] = 0;
+}
+
+
+void CLG_(copy_current_jcc_hash)(jcc_hash* dst)
+{
+  CLG_ASSERT(dst != 0);
+
+  dst->size        = current_jccs.size;
+  dst->entries     = current_jccs.entries;
+  dst->table       = current_jccs.table;
+  dst->spontaneous = current_jccs.spontaneous;
+}
+
+void CLG_(set_current_jcc_hash)(jcc_hash* h)
+{
+  CLG_ASSERT(h != 0);
+
+  current_jccs.size        = h->size;
+  current_jccs.entries     = h->entries;
+  current_jccs.table       = h->table;
+  current_jccs.spontaneous = h->spontaneous;
+}
+
+__inline__
+static UInt jcc_hash_idx(BBCC* from, UInt jmp, BBCC* to, UInt size)
+{
+  return (UInt) ( (UWord)from + 7* (UWord)to + 13*jmp) % size;
+} 
+
+/* double size of jcc table  */
+static void resize_jcc_table(void)
+{
+    Int i, new_size, conflicts1 = 0, conflicts2 = 0;
+    jCC** new_table;
+    UInt new_idx;
+    jCC *curr_jcc, *next_jcc;
+
+    new_size  = 2* current_jccs.size +3;
+    new_table = (jCC**) CLG_MALLOC("cl.jumps.rjt.1",
+                                   new_size * sizeof(jCC*));
+ 
+    for (i = 0; i < new_size; i++)
+      new_table[i] = NULL;
+ 
+    for (i = 0; i < current_jccs.size; i++) {
+	if (current_jccs.table[i] == NULL) continue;
+ 
+	curr_jcc = current_jccs.table[i];
+	while (NULL != curr_jcc) {
+	    next_jcc = curr_jcc->next_hash;
+
+	    new_idx = jcc_hash_idx(curr_jcc->from, curr_jcc->jmp,
+				    curr_jcc->to, new_size);
+
+	    curr_jcc->next_hash = new_table[new_idx];
+	    new_table[new_idx] = curr_jcc;
+	    if (curr_jcc->next_hash) {
+		conflicts1++;
+		if (curr_jcc->next_hash->next_hash)
+		    conflicts2++;
+	    }
+
+	    curr_jcc = next_jcc;
+	}
+    }
+
+    VG_(free)(current_jccs.table);
+
+
+    CLG_DEBUG(0, "Resize JCC Hash: %u => %d (entries %u, conflicts %d/%d)\n",
+	     current_jccs.size, new_size,
+	     current_jccs.entries, conflicts1, conflicts2);
+
+    current_jccs.size  = new_size;
+    current_jccs.table = new_table;
+    CLG_(stat).jcc_hash_resizes++;
+}
+
+
+
+/* new jCC structure: a call was done to a BB of a BBCC 
+ * for a spontaneous call, from is 0 (i.e. caller unknown)
+ */
+static jCC* new_jcc(BBCC* from, UInt jmp, BBCC* to)
+{
+   jCC* jcc;
+   UInt new_idx;
+
+   /* check fill degree of jcc hash table and resize if needed (>80%) */
+   current_jccs.entries++;
+   if (10 * current_jccs.entries / current_jccs.size > 8)
+       resize_jcc_table();
+
+   jcc = (jCC*) CLG_MALLOC("cl.jumps.nj.1", sizeof(jCC));
+
+   jcc->from      = from;
+   jcc->jmp       = jmp;
+   jcc->to        = to;
+   jcc->jmpkind   = jk_Call;
+   jcc->call_counter = 0;
+   jcc->cost = 0;
+
+   /* insert into JCC chain of calling BBCC.
+    * This list is only used at dumping time */
+
+   if (from) {
+       /* Prohibit corruption by array overrun */
+       CLG_ASSERT((0 <= jmp) && (jmp <= from->bb->cjmp_count));
+       jcc->next_from = from->jmp[jmp].jcc_list;
+       from->jmp[jmp].jcc_list = jcc;
+   }
+   else {
+       jcc->next_from = current_jccs.spontaneous;
+       current_jccs.spontaneous = jcc;
+   }
+
+   /* insert into JCC hash table */
+   new_idx = jcc_hash_idx(from, jmp, to, current_jccs.size);
+   jcc->next_hash = current_jccs.table[new_idx];
+   current_jccs.table[new_idx] = jcc;
+
+   CLG_(stat).distinct_jccs++;
+
+   CLG_DEBUGIF(3) {
+     VG_(printf)("  new_jcc (now %d): %p\n",
+		 CLG_(stat).distinct_jccs, jcc);
+   }
+
+   return jcc;
+}
+
+
+/* get the jCC for a call arc (BBCC->BBCC) */
+jCC* CLG_(get_jcc)(BBCC* from, UInt jmp, BBCC* to)
+{
+    jCC* jcc;
+    UInt idx;
+
+    CLG_DEBUG(5, "+ get_jcc(bbcc %p/%u => bbcc %p)\n",
+		from, jmp, to);
+
+    /* first check last recently used JCC */
+    jcc = to->lru_to_jcc;
+    if (jcc && (jcc->from == from) && (jcc->jmp == jmp)) {
+	CLG_ASSERT(to == jcc->to);
+	CLG_DEBUG(5,"- get_jcc: [LRU to] jcc %p\n", jcc);
+	return jcc;
+    }
+
+    jcc = from->lru_from_jcc;
+    if (jcc && (jcc->to == to) && (jcc->jmp == jmp)) {
+	CLG_ASSERT(from == jcc->from);
+	CLG_DEBUG(5, "- get_jcc: [LRU from] jcc %p\n", jcc);
+	return jcc;
+    }
+
+    CLG_(stat).jcc_lru_misses++;
+
+    idx = jcc_hash_idx(from, jmp, to, current_jccs.size);
+    jcc = current_jccs.table[idx];
+
+    while(jcc) {
+	if ((jcc->from == from) &&
+	    (jcc->jmp == jmp) &&
+	    (jcc->to == to)) break;
+	jcc = jcc->next_hash;
+    }
+
+    if (!jcc)
+	jcc = new_jcc(from, jmp, to);
+
+    /* set LRU */
+    from->lru_from_jcc = jcc;
+    to->lru_to_jcc = jcc;
+
+    CLG_DEBUG(5, "- get_jcc(bbcc %p => bbcc %p)\n",
+		from, to);
+
+    return jcc;
+}
+
diff --git a/sigrind/log_events.c b/sigrind/log_events.c
new file mode 100644
index 000000000..74519a578
--- /dev/null
+++ b/sigrind/log_events.c
@@ -0,0 +1,239 @@
+/* This file is part of Callgrind, a Valgrind tool for call graph profiling programs.
+Copyright (C) 2003-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This tool is derived from and contains code from Cachegrind
+   Copyright (C) 2002-2015 Nicholas Nethercote (njn@valgrind.org)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "log_events.h"
+#include "sigil2_ipc.h"
+#include "coregrind/pub_core_libcprint.h"
+
+//TODO(someday) these aren't needed for Sigil, but deleting them causes
+/* Following global vars are setup before by setup_bbcc():
+ *
+ * - Addr   CLG_(bb_base)     (instruction start address of original BB)
+ * - ULong* CLG_(cost_base)   (start of cost array for BB)
+ */
+Addr   CLG_(bb_base);
+ULong* CLG_(cost_base);
+
+//#define COUNT_EVENT_CHECK
+#ifdef COUNT_EVENT_CHECK
+static unsigned long long mem_events = 0;
+static unsigned long long comp_events = 0;
+static unsigned long long sync_events = 0;
+static unsigned long long cxt_events = 0;
+#endif
+
+void SGL_(end_logging)()
+{
+#ifdef COUNT_EVENT_CHECK
+    VG_(printf)("Total Mem Events: %llu\n",  mem_events);
+    VG_(printf)("Total Comp Events: %llu\n", comp_events);
+    VG_(printf)("Total Sync Events: %llu\n", sync_events);
+    VG_(printf)("Total Cxt Events: %llu\n",  cxt_events);
+#endif
+}
+
+
+void SGL_(log_1I0D)(InstrInfo* ii)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+#ifdef COUNT_EVENT_CHECK
+        cxt_events++;
+#endif
+
+        SglEvVariant* slot = SGL_(acq_event_slot)();
+        slot->tag          = SGL_CXT_TAG;
+        slot->cxt.type     = SGLPRIM_CXT_INSTR;
+        slot->cxt.id       = ii->instr_addr;
+    }
+}
+
+
+/* Note that addEvent_D_guarded assumes that log_0I1Dr and log_0I1Dw
+   have exactly the same prototype.  If you change them, you must
+   change addEvent_D_guarded too. */
+static inline void log_mem(Int type, Addr data_addr, Word data_size)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+#ifdef COUNT_EVENT_CHECK
+        ++mem_events;
+#endif
+
+        SglEvVariant* slot   = SGL_(acq_event_slot)();
+        slot->tag            = SGL_MEM_TAG;
+        slot->mem.type       = type;
+        slot->mem.begin_addr = data_addr;
+        slot->mem.size       = data_size;
+    }
+}
+void SGL_(log_0I1Dr)(InstrInfo* ii, Addr data_addr, Word data_size)
+{
+    log_mem(SGLPRIM_MEM_LOAD, data_addr, data_size);
+}
+void SGL_(log_0I1Dw)(InstrInfo* ii, Addr data_addr, Word data_size)
+{
+    log_mem(SGLPRIM_MEM_STORE, data_addr, data_size);
+}
+
+
+void SGL_(log_comp_event)(InstrInfo* ii, IRType op_type, IRExprTag arity)
+{
+    /* SIMD and decimal floating point are unsupported
+     * See VEX/pub/libvex_ir.h : IROp
+     * for future updates on specific ops */
+    tl_assert(op_type < Ity_D32 || op_type == Ity_F128);
+
+    if (EVENT_GENERATION_ENABLED)
+    {
+#ifdef COUNT_EVENT_CHECK
+        ++comp_events;
+#endif
+
+        SglEvVariant* slot = SGL_(acq_event_slot)();
+        slot->tag = SGL_COMP_TAG;
+
+        if (op_type < Ity_F16)
+            slot->comp.type = SGLPRIM_COMP_IOP;
+        else
+            slot->comp.type = SGLPRIM_COMP_FLOP;
+
+        switch (arity)
+        {
+        case Iex_Unop:
+            slot->comp.arity = SGLPRIM_COMP_UNARY;
+            break;
+        case Iex_Binop:
+            slot->comp.arity = SGLPRIM_COMP_BINARY;
+            break;
+        case Iex_Triop:
+            slot->comp.arity = SGLPRIM_COMP_TERNARY;
+            break;
+        case Iex_Qop:
+            slot->comp.arity = SGLPRIM_COMP_QUARTERNARY;
+            break;
+        default:
+            tl_assert(False);
+            break;
+        }
+    }
+}
+
+
+void SGL_(log_sync)(UChar type, UWord data1, UWord data2)
+{
+    if (SGL_(clo).gen_sync == True)
+    {
+#ifdef COUNT_EVENT_CHECK
+        ++sync_events;
+#endif
+
+        SglEvVariant* slot  = SGL_(acq_event_slot)();
+        slot->tag           = SGL_SYNC_TAG;
+        slot->sync.type     = type;
+        slot->sync.data[0]  = data1;
+        slot->sync.data[1]  = data2;
+    }
+}
+
+
+static inline void log_fn(Int type, fn_node* fn)
+{
+    if (EVENT_GENERATION_ENABLED && SGL_(clo).gen_fn == True)
+    {
+#ifdef COUNT_EVENT_CHECK
+        cxt_events++;
+#endif
+
+        /* request both slots simultaneously to allow proper flushing */
+        /* TODO set max size for name length? */
+        Int len = VG_(strlen)(fn->name) + 1;
+        EventNameSlotTuple tuple = SGL_(acq_event_name_slot)(len);
+
+        VG_(strncpy)(tuple.name_slot, fn->name, len);
+        tuple.event_slot->tag      = SGL_CXT_TAG;
+        tuple.event_slot->cxt.type = type;
+        tuple.event_slot->cxt.len  = len;
+        tuple.event_slot->cxt.idx  = tuple.name_idx;
+    }
+}
+void SGL_(log_fn_entry)(fn_node* fn)
+{
+    log_fn(SGLPRIM_CXT_FUNC_ENTER, fn);
+}
+void SGL_(log_fn_leave)(fn_node* fn)
+{
+    log_fn(SGLPRIM_CXT_FUNC_EXIT, fn);
+}
+
+
+/***************************
+ * Aggregate event logging
+ ***************************/
+void SGL_(log_2I0D)(InstrInfo* ii1, InstrInfo* ii2)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+        SGL_(log_1I0D)(ii1);
+        SGL_(log_1I0D)(ii2);
+    }
+}
+void SGL_(log_3I0D)(InstrInfo* ii1, InstrInfo* ii2, InstrInfo* ii3)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+        SGL_(log_1I0D)(ii1);
+        SGL_(log_1I0D)(ii2);
+        SGL_(log_1I0D)(ii3);
+    }
+}
+void SGL_(log_1I1Dr)(InstrInfo* ii, Addr data_addr, Word data_size)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+        SGL_(log_1I0D)(ii);
+        SGL_(log_0I1Dr)(ii, data_addr, data_size);
+    }
+}
+void SGL_(log_1I1Dw)(InstrInfo* ii, Addr data_addr, Word data_size)
+{
+    if (EVENT_GENERATION_ENABLED)
+    {
+        SGL_(log_1I0D)(ii);
+        SGL_(log_0I1Dw)(ii, data_addr, data_size);
+    }
+}
+
+/***************************
+ * Unimplemented Logging
+ ***************************/
+void SGL_(log_global_event)(InstrInfo* ii)
+{
+}
+void SGL_(log_cond_branch)(InstrInfo* ii, Word taken)
+{
+}
+void SGL_(log_ind_branch)(InstrInfo* ii, UWord actual_dst)
+{
+}
diff --git a/sigrind/log_events.h b/sigrind/log_events.h
new file mode 100644
index 000000000..413158a0d
--- /dev/null
+++ b/sigrind/log_events.h
@@ -0,0 +1,63 @@
+#ifndef SGL_LOG_EVENTS_H
+#define SGL_LOG_EVENTS_H
+
+#include "global.h"
+
+/********************************************************************
+ * Event Logging in Sigrind
+ *
+ * Sigrind piggy-backs off of Callgrind's event instrumentation
+ * infrastructure. All Callgrind event processing (cost tracking)
+ * has been gutted, and replaced with interprocess communication
+ * handling. This is how dynamic application info is sent to Sigil2.
+ ********************************************************************/
+
+void SGL_(end_logging)(void);
+
+/* 1 Instruction */
+void SGL_(log_1I0D)(InstrInfo* ii);
+
+/* 2 Instructions */
+void SGL_(log_2I0D)(InstrInfo* ii1, InstrInfo* ii2);
+
+/* 3 Instructions */
+void SGL_(log_3I0D)(InstrInfo* ii1, InstrInfo* ii2, InstrInfo* ii3);
+
+/* 1 Instruction, 1 Data Read */
+void SGL_(log_1I1Dr)(InstrInfo* ii, Addr data_addr, Word data_size);
+
+/* 1 Data Read */
+void SGL_(log_0I1Dr)(InstrInfo* ii, Addr data_addr, Word data_size);
+
+/* 1 Instruction, 1 Data Write */
+void SGL_(log_1I1Dw)(InstrInfo* ii, Addr data_addr, Word data_size);
+
+/* 1 Data Write */
+void SGL_(log_0I1Dw)(InstrInfo* ii, Addr data_addr, Word data_size);
+
+/* 1 Compute event */
+void SGL_(log_comp_event)(InstrInfo* ii, IRType op_type, IRExprTag arity);
+
+/* Function fn entered */
+void SGL_(log_fn_entry)(fn_node* fn);
+
+/* Function fn exited */
+void SGL_(log_fn_leave)(fn_node* fn);
+
+/* Synchronization event or thread context swap
+ * Some sync events have two pieces of data,
+ * e.g. mutex and condition variable in a conditional wait.
+ * Otherwise only the first data argument is used */
+#define UNUSED_SYNC_DATA 0
+void SGL_(log_sync)(UChar type, UWord data1, UWord data2);
+
+/* unimplemented */
+void SGL_(log_global_event)(InstrInfo* ii);
+
+/* unimplemented */
+void SGL_(log_cond_branch)(InstrInfo* ii, Word taken);
+
+/* unimplemented */
+void SGL_(log_ind_branch)(InstrInfo* ii, UWord actual_dst);
+
+#endif
diff --git a/sigrind/sg_main.c b/sigrind/sg_main.c
new file mode 100644
index 000000000..bdf6208e4
--- /dev/null
+++ b/sigrind/sg_main.c
@@ -0,0 +1,1872 @@
+
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                       main.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call graph
+   profiling programs.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This tool is derived from and contains code from Cachegrind
+   Copyright (C) 2002-2015 Nicholas Nethercote (njn@valgrind.org)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+// TODO(cleanup) leftover Callgrind functionality
+
+
+#include "config.h"
+
+#include "global.h"
+#include "callgrind.h"
+
+#include "sigil2_ipc.h"
+#include "log_events.h"
+#include "Core/PrimitiveEnums.h"
+
+#include "coregrind/pub_core_libcfile.h"
+#include "coregrind/pub_core_clientstate.h"
+#include "pub_tool_threadstate.h"
+#include "pub_tool_gdbserver.h"
+#include "pub_tool_transtab.h"       // VG_(discard_translations_safely)
+
+
+/*------------------------------------------------------------*/
+/*--- Instrumentation structures and event queue handling  ---*/
+/*------------------------------------------------------------*/
+
+/* Maintain an ordered list of memory events which are outstanding, in
+   the sense that no IR has yet been generated to do the relevant
+   helper calls.  The BB is scanned top to bottom and memory events
+   are added to the end of the list, merging with the most recent
+   notified event where possible (Dw immediately following Dr and
+   having the same size and EA can be merged).
+
+   This merging is done so that for architectures which have
+   load-op-store instructions (x86, amd64), the insn is treated as if
+   it makes just one memory reference (a modify), rather than two (a
+   read followed by a write at the same address).
+
+   At various points the list will need to be flushed, that is, IR
+   generated from it.  That must happen before any possible exit from
+   the block (the end, or an IRStmt_Exit).  Flushing also takes place
+   when there is no space to add a new event.
+
+   If we require the simulation statistics to be up to date with
+   respect to possible memory exceptions, then the list would have to
+   be flushed before each memory reference.  That would however lose
+   performance by inhibiting event-merging during flushing.
+
+   Flushing the list consists of walking it start to end and emitting
+   instrumentation IR for each event, in the order in which they
+   appear.  It may be possible to emit a single call for two adjacent
+   events in order to reduce the number of helper function calls made.
+   For example, it could well be profitable to handle two adjacent Ir
+   events with a single helper call.  */
+
+typedef IRExpr IRAtom;
+
+typedef enum _EventTag EventTag;
+enum _EventTag {
+      Ev_Ir,  // Instruction read
+      Ev_Dr,  // Data read
+      Ev_Dw,  // Data write
+      Ev_Dm,  // Data modify (read then write)
+      Ev_Bc,  // branch conditional
+      Ev_Bi,  // branch indirect (to unknown destination)
+      Ev_G,   // Global bus event
+      Ev_Comp // Compute event
+};
+
+typedef struct _Event Event;
+struct _Event {
+   EventTag   tag;
+   InstrInfo* inode;
+   union 
+   {
+      struct {
+      } Ir;
+      struct {
+         IRAtom* ea;
+         Int     szB;
+      } Dr;
+      struct {
+         IRAtom* ea;
+         Int     szB;
+      } Dw;
+      struct 
+      {
+         IRAtom* ea;
+         Int     szB;
+      } Dm;
+      struct {
+         IRAtom* taken; /* :: Ity_I1 */
+      } Bc;
+      struct {
+         IRAtom* dst;
+      } Bi;
+      struct {
+      } G;
+      struct {
+         IRExprTag arity;
+         IRType op_type;
+      } Comp;
+   } Ev;
+};
+
+/* Up to this many unnotified events are allowed.  Number is
+   arbitrary.  Larger numbers allow more event merging to occur, but
+   potentially induce more spilling due to extending live ranges of
+   address temporaries. */
+#define N_EVENTS 16
+
+/* A struct which holds all the running state during instrumentation.
+   Mostly to avoid passing loads of parameters everywhere. */
+typedef struct {
+    /* The current outstanding-memory-event list. */
+    Event events[N_EVENTS];
+    Int   events_used;
+
+    /* The array of InstrInfo's is part of BB struct. */
+    BB* bb;
+
+    /* BB seen before (ie. re-instrumentation) */
+    Bool seen_before;
+
+    /* Number InstrInfo bins 'used' so far. */
+    UInt ii_index;
+
+    // current offset of guest instructions from BB start
+    UInt instr_offset;
+
+    /* The output SB being constructed. */
+    IRSB* sbOut;
+} ClgState;
+
+/*------------------------------------------------------------*/
+/*--- Global variables                                     ---*/
+/*------------------------------------------------------------*/
+
+Bool SGL_(is_in_event_collect_func);
+SglCommandLineOptions SGL_(clo);
+
+/* for all threads */
+CommandLineOptions CLG_(clo);
+
+Statistics CLG_(stat);
+Bool CLG_(instrument_state) = True; /* Instrumentation on ? */
+
+/* thread and signal handler specific */
+exec_state CLG_(current_state);
+
+/*------------------------------------------------------------*/
+/*--- Local declarations                                   ---*/
+/*------------------------------------------------------------*/
+
+static void flushEvents ( ClgState* clgs );
+static void addEvent_Comp( ClgState* clgs, InstrInfo* inode, IRExprTag arity, IRType op_type );
+static void addEvent_G ( ClgState* clgs, InstrInfo* inode );
+static void addEvent_Bi ( ClgState* clgs, InstrInfo* inode, IRAtom* whereTo );
+static void addEvent_Bc ( ClgState* clgs, InstrInfo* inode, IRAtom* guard );
+static void addEvent_D_guarded ( ClgState* clgs, InstrInfo* inode,
+                          Int datasize, IRAtom* ea, IRAtom* guard,
+                          Bool isWrite );
+static void addEvent_Dw ( ClgState* clgs, InstrInfo* inode, Int datasize, IRAtom* ea );
+static void addEvent_Dr ( ClgState* clgs, InstrInfo* inode, Int datasize, IRAtom* ea );
+static void addEvent_Ir ( ClgState* clgs, InstrInfo* inode );
+static InstrInfo* next_InstrInfo ( ClgState* clgs, Addr instr_addr, UInt instr_size );
+
+/* Inititalization functions */
+static void init_Event ( Event* ev );
+/* Helper functions */
+static IRAtom* get_Event_dea ( Event* ev );
+static Int get_Event_dszB ( Event* ev );
+static void showEvent ( Event* ev );
+static Addr IRConst2Addr(IRConst* con);
+static void addConstMemStoreStmt( IRSB* bbOut, UWord addr, UInt val, IRType hWordTy);
+
+
+
+/*------------------------------------------------------------*/
+/*--- Instrumentation                                      ---*/
+/*------------------------------------------------------------*/
+
+/* add helper call to setup_bbcc, with pointer to BB struct as argument
+ *
+ * precondition for setup_bbcc:
+ * - jmps_passed has number of cond.jumps passed in last executed BB
+ * - current_bbcc has a pointer to the BBCC of the last executed BB
+ *   Thus, if bbcc_jmpkind is != -1 (JmpNone),
+ *     current_bbcc->bb->jmp_addr
+ *   gives the address of the jump source.
+ *
+ * the setup does 1 thing:
+ * - trace call:
+ *   * Unwind own call stack, i.e sync our ESP with real ESP
+ *     This is for ESP manipulation (longjmps, C++ exec handling) and RET
+ *   * For CALLs or JMPs crossing objects, record call arg +
+ *     push are on own call stack
+ */
+static void addBBSetupCall(ClgState* clgs)
+{
+   IRDirty* di;
+   IRExpr  *arg1, **argv;
+
+   arg1 = mkIRExpr_HWord( (HWord)clgs->bb );
+   argv = mkIRExprVec_1(arg1);
+   di = unsafeIRDirty_0_N( 1, "setup_bbcc",
+			      VG_(fnptr_to_fnentry)( & CLG_(setup_bbcc) ),
+			      argv);
+   addStmtToIRSB( clgs->sbOut, IRStmt_Dirty(di) );
+}
+
+static IRSB* CLG_(instrument)( VgCallbackClosure* closure,
+                        IRSB* sbIn,
+			const VexGuestLayout* layout,
+			const VexGuestExtents* vge,
+                        const VexArchInfo* archinfo_host,
+			IRType gWordTy, IRType hWordTy )
+{
+   Int        i;
+   IRStmt*    st;
+   Addr       origAddr;
+   InstrInfo* curr_inode = NULL;
+   ClgState   clgs;
+   UInt       cJumps = 0;
+   IRTypeEnv* tyenv = sbIn->tyenv;
+
+   if (gWordTy != hWordTy) {
+      /* We don't currently support this case. */
+      VG_(tool_panic)("host/guest word size mismatch");
+   }
+
+   // No instrumentation if it is switched off
+   if (! CLG_(instrument_state)) {
+       CLG_DEBUG(5, "instrument(BB %#lx) [Instrumentation OFF]\n",
+		 (Addr)closure->readdr);
+       return sbIn;
+   }
+
+   CLG_DEBUG(3, "+ instrument(BB %#lx)\n", (Addr)closure->readdr);
+
+   /* Set up SB for instrumented IR */
+   clgs.sbOut = deepCopyIRSBExceptStmts(sbIn);
+
+   // Copy verbatim any IR preamble preceding the first IMark
+   i = 0;
+   while (i < sbIn->stmts_used && sbIn->stmts[i]->tag != Ist_IMark) {
+      addStmtToIRSB( clgs.sbOut, sbIn->stmts[i] );
+      i++;
+   }
+
+   // Get the first statement, and origAddr from it
+   CLG_ASSERT(sbIn->stmts_used >0);
+   CLG_ASSERT(i < sbIn->stmts_used);
+   st = sbIn->stmts[i];
+   CLG_ASSERT(Ist_IMark == st->tag);
+
+   origAddr = st->Ist.IMark.addr + st->Ist.IMark.delta;
+   CLG_ASSERT(origAddr == st->Ist.IMark.addr 
+                          + st->Ist.IMark.delta);  // XXX: check no overflow
+
+   /* Get BB struct (creating if necessary).
+    * JS: The hash table is keyed with orig_addr_noredir -- important!
+    * JW: Why? If it is because of different chasing of the redirection,
+    *     this is not needed, as chasing is switched off in callgrind
+    */
+   clgs.bb = CLG_(get_bb)(origAddr, sbIn, &(clgs.seen_before));
+
+   addBBSetupCall(&clgs);
+
+   // Set up running state
+   clgs.events_used = 0;
+   clgs.ii_index = 0;
+   clgs.instr_offset = 0;
+
+   for (/*use current i*/; i < sbIn->stmts_used; i++) 
+   {
+      st = sbIn->stmts[i];
+      CLG_ASSERT(isFlatIRStmt(st));
+
+      if (st->tag == Ist_IMark)
+      {
+         Addr   cia   = st->Ist.IMark.addr + st->Ist.IMark.delta;
+         UInt   isize = st->Ist.IMark.len;
+         CLG_ASSERT(clgs.instr_offset == cia - origAddr);
+         // If Vex fails to decode an instruction, the size will be zero.
+         // Pretend otherwise.
+         if (isize == 0) isize = VG_MIN_INSTR_SZB;
+         // Sanity-check size.
+         tl_assert( (VG_MIN_INSTR_SZB <= isize && isize <= VG_MAX_INSTR_SZB)
+            || VG_CLREQ_SZB == isize );
+
+         // Init the inode, record it as the current one.
+         // Subsequent Dr/Dw/Dm events from the same instruction will
+         // also use it.
+         curr_inode = next_InstrInfo (&clgs, cia, isize);
+      }
+
+      switch (st->tag) 
+      {
+         case Ist_NoOp:
+         case Ist_AbiHint:
+         case Ist_Put:
+         case Ist_PutI:
+         case Ist_MBE:
+             break;
+         case Ist_IMark: 
+         {
+            if (SGL_(clo).gen_instr == True)
+               addEvent_Ir( &clgs, curr_inode );
+            break;
+         }
+         case Ist_WrTmp: 
+         {
+            IRExpr* data = st->Ist.WrTmp.data;
+            switch (data->tag)
+            {
+               case Iex_Load:
+               {
+                  IRExpr* aexpr = data->Iex.Load.addr;
+                  // Note also, endianness info is ignored.  I guess
+                  // that's not interesting.
+                  if (SGL_(clo).gen_mem == True)
+                     addEvent_Dr( &clgs, curr_inode, sizeofIRType(data->Iex.Load.ty), aexpr );
+                  break;
+               }
+               case Iex_Unop:
+               case Iex_Binop:
+               case Iex_Triop:
+               case Iex_Qop:
+                  if (SGL_(clo).gen_comp == True)
+                  {
+                     IRType op_type = typeOfIRExpr(sbIn->tyenv, data);
+                     /* SIMD and decimal floating point ops are unsupported.
+                      * See VEX/pub/libvex_ir.h */
+                     if (op_type < Ity_D32 || op_type == Ity_F128)
+                        addEvent_Comp( &clgs, curr_inode, data->tag, op_type );
+                  }
+                  break;
+               default:
+                  /*don't care*/
+                  break;
+            }
+            break;
+         }
+         case Ist_Store: 
+         {
+            IRExpr* data  = st->Ist.Store.data;
+            IRExpr* aexpr = st->Ist.Store.addr;
+            if (SGL_(clo).gen_mem == True)
+               addEvent_Dw( &clgs, curr_inode, sizeofIRType(typeOfIRExpr(sbIn->tyenv, data)), aexpr );
+            break;
+         }
+         case Ist_StoreG: 
+         {
+            IRStoreG* sg   = st->Ist.StoreG.details;
+            IRExpr*   data = sg->data;
+            IRExpr*   addr = sg->addr;
+            IRType    type = typeOfIRExpr(tyenv, data);
+            tl_assert(type != Ity_INVALID);
+            if (SGL_(clo).gen_mem == True) {
+               addEvent_D_guarded( &clgs, curr_inode,
+                                   sizeofIRType(type),
+                                   addr, sg->guard, True/*isWrite*/ );
+            }
+            break;
+         }
+         case Ist_LoadG: 
+         {
+            IRLoadG* lg       = st->Ist.LoadG.details;
+            IRType   type     = Ity_INVALID; /* loaded type */
+            IRType   typeWide = Ity_INVALID; /* after implicit widening */
+            IRExpr*  addr     = lg->addr;
+            typeOfIRLoadGOp(lg->cvt, &typeWide, &type);
+            tl_assert(type != Ity_INVALID);
+            if (SGL_(clo).gen_mem == True) {
+               addEvent_D_guarded( &clgs, curr_inode,
+                                   sizeofIRType(type), addr, lg->guard,
+                                   False/*!isWrite*/ );
+            }
+            break;
+         }
+         case Ist_Dirty: 
+         {
+            Int      dataSize;
+            IRDirty* d = st->Ist.Dirty.details;
+            if (d->mFx != Ifx_None) 
+            {
+               /* This dirty helper accesses memory.  Collect the details. */
+               tl_assert(d->mAddr != NULL);
+               tl_assert(d->mSize != 0);
+               dataSize = d->mSize;
+
+               if (d->mFx == Ifx_Read || d->mFx == Ifx_Modify)
+               {
+                  if (SGL_(clo).gen_mem == True)
+                     addEvent_Dr( &clgs, curr_inode, dataSize, d->mAddr );
+               }
+               if (d->mFx == Ifx_Write || d->mFx == Ifx_Modify)
+               {
+                  if (SGL_(clo).gen_mem == True)
+                     addEvent_Dw( &clgs, curr_inode, dataSize, d->mAddr );
+               }
+            }
+            else 
+            {
+               tl_assert(d->mAddr == NULL);
+               tl_assert(d->mSize == 0);
+            }
+            break;
+         }
+         case Ist_CAS: 
+         {
+            /* We treat it as a read and a write of the location.  I
+               think that is the same behaviour as it was before IRCAS
+               was introduced, since prior to that point, the Vex
+               front ends would translate a lock-prefixed instruction
+               into a (normal) read followed by a (normal) write. */
+            Int    dataSize;
+            IRCAS* cas = st->Ist.CAS.details;
+            CLG_ASSERT(cas->addr && isIRAtom(cas->addr));
+            CLG_ASSERT(cas->dataLo);
+            dataSize = sizeofIRType(typeOfIRExpr(sbIn->tyenv, cas->dataLo));
+            if (cas->dataHi != NULL)
+               dataSize *= 2; /* since this is a doubleword-cas */
+            if (SGL_(clo).gen_mem == True) {
+               addEvent_Dr( &clgs, curr_inode, dataSize, cas->addr );
+               addEvent_Dw( &clgs, curr_inode, dataSize, cas->addr );
+            }
+            addEvent_G(  &clgs, curr_inode );
+            break;
+         }
+         case Ist_LLSC:
+            if (st->Ist.LLSC.storedata == NULL) {
+               /* LL */
+               IRType dataTy = typeOfIRTemp(sbIn->tyenv, st->Ist.LLSC.result);
+               if (SGL_(clo).gen_mem == True)
+                  addEvent_Dr( &clgs, curr_inode, sizeofIRType(dataTy), st->Ist.LLSC.addr );
+               /* flush events before LL, should help SC to succeed */
+               flushEvents( &clgs );
+            } else {
+               /* SC */
+               IRType dataTy = typeOfIRExpr(sbIn->tyenv, st->Ist.LLSC.storedata);
+               if (SGL_(clo).gen_mem == True)
+                  addEvent_Dw( &clgs, curr_inode, sizeofIRType(dataTy), st->Ist.LLSC.addr );
+               /* I don't know whether the global-bus-lock cost should
+                  be attributed to the LL or the SC, but it doesn't
+                  really matter since they always have to be used in
+                  pairs anyway.  Hence put it (quite arbitrarily) on
+                  the SC. */
+               addEvent_G(  &clgs, curr_inode );
+            }
+            break;
+         case Ist_Exit: {
+            Bool guest_exit, inverted;
+
+            /* VEX code generation sometimes inverts conditional branches.
+             * As Callgrind counts (conditional) jumps, it has to correct
+             * inversions. The heuristic is the following:
+             * (1) Callgrind switches off SB chasing and unrolling, and
+             *     therefore it assumes that a candidate for inversion only is
+             *     the last conditional branch in an SB.
+             * (2) inversion is assumed if the branch jumps to the address of
+             *     the next guest instruction in memory.
+             * This heuristic is precalculated in CLG_(collectBlockInfo)().
+             *
+             * Branching behavior is also used for branch prediction. Note that
+             * above heuristic is different from what Cachegrind does.
+             * Cachegrind uses (2) for all branches.
+             */
+            if (cJumps+1 == clgs.bb->cjmp_count)
+                inverted = clgs.bb->cjmp_inverted;
+            else
+                inverted = False;
+
+            // call branch predictor only if this is a branch in guest code
+            guest_exit = (st->Ist.Exit.jk == Ijk_Boring) ||
+                         (st->Ist.Exit.jk == Ijk_Call) ||
+                         (st->Ist.Exit.jk == Ijk_Ret);
+
+            if (guest_exit) {
+                /* Stuff to widen the guard expression to a host word, so
+                   we can pass it to the branch predictor simulation
+                   functions easily. */
+                IRType   tyW    = hWordTy;
+                IROp     widen  = tyW==Ity_I32  ? Iop_1Uto32  : Iop_1Uto64;
+                IROp     opXOR  = tyW==Ity_I32  ? Iop_Xor32   : Iop_Xor64;
+                IRTemp   guard1 = newIRTemp(clgs.sbOut->tyenv, Ity_I1);
+                IRTemp   guardW = newIRTemp(clgs.sbOut->tyenv, tyW);
+                IRTemp   guard  = newIRTemp(clgs.sbOut->tyenv, tyW);
+                IRExpr*  one    = tyW==Ity_I32 ? IRExpr_Const(IRConst_U32(1))
+                                               : IRExpr_Const(IRConst_U64(1));
+
+                /* Widen the guard expression. */
+                addStmtToIRSB( clgs.sbOut,
+                               IRStmt_WrTmp( guard1, st->Ist.Exit.guard ));
+                addStmtToIRSB( clgs.sbOut,
+                               IRStmt_WrTmp( guardW,
+                                             IRExpr_Unop(widen,
+                                                         IRExpr_RdTmp(guard1))) );
+                /* If the exit is inverted, invert the sense of the guard. */
+                addStmtToIRSB(
+                        clgs.sbOut,
+                        IRStmt_WrTmp(
+                                guard,
+                                inverted ? IRExpr_Binop(opXOR, IRExpr_RdTmp(guardW), one)
+                                    : IRExpr_RdTmp(guardW)
+                                    ));
+                /* And post the event. */
+                addEvent_Bc( &clgs, curr_inode, IRExpr_RdTmp(guard) );
+            }
+
+            /* We may never reach the next statement, so need to flush
+               all outstanding transactions now. */
+            flushEvents( &clgs );
+
+            CLG_ASSERT(clgs.ii_index>0);
+            if (!clgs.seen_before) {
+               ClgJumpKind jk;
+
+               if (st->Ist.Exit.jk == Ijk_Call) jk = jk_Call;
+               else if (st->Ist.Exit.jk == Ijk_Ret) jk = jk_Return;
+               else {
+                 if (IRConst2Addr(st->Ist.Exit.dst) == origAddr + curr_inode->instr_offset + curr_inode->instr_size)
+                    jk = jk_None;
+                 else
+                    jk = jk_Jump;
+               }
+
+               clgs.bb->jmp[cJumps].instr = clgs.ii_index-1;
+               clgs.bb->jmp[cJumps].jmpkind = jk;
+            }
+
+            /* Update global variable jmps_passed before the jump
+             * A correction is needed if VEX inverted the last jump condition
+             */
+            UInt val = inverted ? cJumps+1 : cJumps;
+            addConstMemStoreStmt( clgs.sbOut,
+                (UWord) &CLG_(current_state).jmps_passed,
+                val, hWordTy);
+
+            cJumps++;
+
+            break;
+         }
+         default:
+            tl_assert(0);
+            break;
+      } //end switch
+
+
+      /* Copy the original statement */
+      addStmtToIRSB( clgs.sbOut, st );
+
+      CLG_DEBUGIF(5) 
+      {
+         VG_(printf)("   pass  ");
+	      ppIRStmt(st);
+	      VG_(printf)("\n");
+      }
+   } //end foreach statement
+
+   /* Deal with branches to unknown destinations.  Except ignore ones
+      which are function returns as we assume the return stack
+      predictor never mispredicts. */
+   if ((sbIn->jumpkind == Ijk_Boring) || (sbIn->jumpkind == Ijk_Call)) {
+      if (0) { ppIRExpr( sbIn->next ); VG_(printf)("\n"); }
+      switch (sbIn->next->tag) {
+         case Iex_Const:
+            break; /* boring - branch to known address */
+         case Iex_RdTmp:
+            /* looks like an indirect branch (branch to unknown) */
+            addEvent_Bi( &clgs, curr_inode, sbIn->next );
+            break;
+         default:
+            /* shouldn't happen - if the incoming IR is properly
+               flattened, should only have tmp and const cases to
+               consider. */
+            tl_assert(0);
+      }
+   }
+
+   /* At the end of the bb.  Flush outstandings. */
+   flushEvents( &clgs );
+
+   /* Update global variable jmps_passed at end of SB.
+    * As CLG_(current_state).jmps_passed is reset to 0 in setup_bbcc,
+    * this can be omitted if there is no conditional jump in this SB.
+    * A correction is needed if VEX inverted the last jump condition
+    */
+   if (cJumps>0) {
+      UInt jmps_passed = cJumps;
+      if (clgs.bb->cjmp_inverted) jmps_passed--;
+      addConstMemStoreStmt( clgs.sbOut,
+			    (UWord) &CLG_(current_state).jmps_passed,
+			    jmps_passed, hWordTy);
+   }
+   CLG_ASSERT(clgs.bb->cjmp_count == cJumps);
+   CLG_ASSERT(clgs.bb->instr_count == clgs.ii_index);
+
+   /* Info for final exit from BB */
+   {
+     ClgJumpKind jk;
+
+     if      (sbIn->jumpkind == Ijk_Call) jk = jk_Call;
+     else if (sbIn->jumpkind == Ijk_Ret)  jk = jk_Return;
+     else {
+       jk = jk_Jump;
+       if ((sbIn->next->tag == Iex_Const) &&
+	   (IRConst2Addr(sbIn->next->Iex.Const.con) ==
+	    origAddr + clgs.instr_offset))
+	 jk = jk_None;
+     }
+     clgs.bb->jmp[cJumps].jmpkind = jk;
+     /* Instruction index of the call/ret at BB end
+      * (it is wrong for fall-through, but does not matter) */
+     clgs.bb->jmp[cJumps].instr = clgs.ii_index-1;
+   }
+
+   /* swap information of last exit with final exit if inverted */
+   if (clgs.bb->cjmp_inverted) {
+     ClgJumpKind jk;
+     UInt instr;
+
+     jk = clgs.bb->jmp[cJumps].jmpkind;
+     clgs.bb->jmp[cJumps].jmpkind = clgs.bb->jmp[cJumps-1].jmpkind;
+     clgs.bb->jmp[cJumps-1].jmpkind = jk;
+     instr = clgs.bb->jmp[cJumps].instr;
+     clgs.bb->jmp[cJumps].instr = clgs.bb->jmp[cJumps-1].instr;
+     clgs.bb->jmp[cJumps-1].instr = instr;
+   }
+
+   if (clgs.seen_before) {
+       CLG_ASSERT(clgs.bb->instr_len == clgs.instr_offset);
+   }
+   else {
+       clgs.bb->instr_len = clgs.instr_offset;
+   }
+
+   if (cJumps>0) {
+       CLG_DEBUG(3, "                     [ ");
+       for (i=0;(UInt)i<cJumps;i++)
+	   CLG_DEBUG(3, "%u ", clgs.bb->jmp[i].instr);
+       CLG_DEBUG(3, "], last inverted: %s \n",
+		 clgs.bb->cjmp_inverted ? "yes":"no");
+   }
+
+  return clgs.sbOut;
+}
+
+
+
+
+
+/* Generate code for all outstanding memory events, and mark the queue
+   empty.  Code is generated into cgs->sbOut, and this activity
+   'consumes' slots in cgs->bb. */
+static void flushEvents ( ClgState* clgs )
+{
+	Int        i, regparms, inew;
+	const HChar* helperName;
+	void*      helperAddr;
+	IRExpr**   argv;
+	IRExpr*    i_node_expr;
+	IRDirty*   di;
+	Event*     ev;
+	Event*     ev2;
+	Event*     ev3;
+
+	for(i = 0; i < clgs->events_used; i = inew) 
+	{
+		helperName = NULL;
+		helperAddr = NULL;
+		argv       = NULL;
+		regparms   = 0;
+
+		/* generate IR to notify event i and possibly the ones
+		immediately following it. */
+		tl_assert(i >= 0 && i < clgs->events_used);
+
+		ev  = &clgs->events[i];
+		ev2 = ( i < clgs->events_used-1 ? &clgs->events[i+1] : NULL );
+		ev3 = ( i < clgs->events_used-2 ? &clgs->events[i+2] : NULL );
+
+		CLG_DEBUGIF(5) 
+		{
+			VG_(printf)("  flush ");
+			showEvent( ev );
+		}
+
+		i_node_expr = mkIRExpr_HWord( (HWord)ev->inode );
+
+		/* Decide on helper fn to call and args to pass it, and advance
+		 * i appropriately.
+		 * Dm events have same effect as Dw events */
+		switch (ev->tag) 
+		{
+		case Ev_Ir:
+			/* Merge an Ir with a following Dr. */
+			if (ev2 && ev2->tag == Ev_Dr) 
+			{
+				tl_assert(ev2->inode == ev->inode);	/* Why is this true?  It's because we're merging an Ir
+													with a following Dr.  The Ir derives from the
+													instruction's IMark and the Dr from data
+													references which follow it.  In short it holds
+													because each insn starts with an IMark, hence an
+													Ev_Ir, and so these Dr must pertain to the
+													immediately preceding Ir.  Same applies to analogous
+													assertions in the subsequent cases. */
+				helperName = "log_1I1Dr";
+				helperAddr = SGL_(log_1I1Dr);
+				argv = mkIRExprVec_3( 
+						i_node_expr,
+						get_Event_dea(ev2),
+						mkIRExpr_HWord(get_Event_dszB(ev2)) 
+						);
+				regparms = 3;
+				inew = i+2;
+			}
+			/* Merge an Ir with a following Dw/Dm. */
+			else if (ev2 && (ev2->tag == Ev_Dw || ev2->tag == Ev_Dm)) 
+			{
+				tl_assert(ev2->inode == ev->inode);
+				helperName = "log_1I1Dw";
+				helperAddr = SGL_(log_1I1Dw);
+				argv = mkIRExprVec_3( 
+						i_node_expr,
+						get_Event_dea(ev2),
+						mkIRExpr_HWord(get_Event_dszB(ev2)) 
+						);
+				regparms = 3;
+				inew = i+2;
+			}
+			/* Merge an Ir with two following Irs. */
+			else if (ev2 && ev3 && ev2->tag == Ev_Ir && ev3->tag == Ev_Ir) 
+			{
+				helperName = "log_3I0D";
+				helperAddr = SGL_(log_3I0D);
+				argv = mkIRExprVec_3( 
+						i_node_expr,
+						mkIRExpr_HWord((HWord)ev2->inode),
+						mkIRExpr_HWord((HWord)ev3->inode) 
+						);
+				regparms = 3;
+				inew = i+3;
+			}
+			/* Merge an Ir with one following Ir. */
+			else
+			if (ev2 && ev2->tag == Ev_Ir) {
+				helperName = "log_2I0D";
+				helperAddr = SGL_(log_2I0D);
+				argv = mkIRExprVec_2(
+						i_node_expr,
+						mkIRExpr_HWord((HWord)ev2->inode) 
+						);
+				regparms = 2;
+				inew = i+2;
+			}
+			/* No merging possible; emit as-is. */
+			else {
+				helperName = "log_1I0D";
+				helperAddr = SGL_(log_1I0D);
+				argv = mkIRExprVec_1( i_node_expr );
+				regparms = 1;
+				inew = i+1;
+			}
+			break;
+		case Ev_Dr:
+			/* Data read or modify */
+			helperName = "log_0I1Dr";
+			helperAddr = SGL_(log_0I1Dr);
+			argv = mkIRExprVec_3( 
+					i_node_expr,
+					get_Event_dea(ev),
+					mkIRExpr_HWord(get_Event_dszB(ev)) 
+					);
+			regparms = 3;
+			inew = i+1;
+			break;
+		case Ev_Dw:
+		case Ev_Dm:
+		   /* Data write */
+		   helperName = "log_0I1Dw";
+		   helperAddr = SGL_(log_0I1Dw);
+		   argv = mkIRExprVec_3( 
+				   i_node_expr,
+		   		   get_Event_dea(ev),
+		   		   mkIRExpr_HWord( get_Event_dszB(ev)) 
+				   );
+		   regparms = 3;
+		   inew = i+1;
+		   break;
+		case Ev_Bc:
+			/* Conditional branch */
+			helperName = "log_cond_branch";
+			helperAddr = SGL_(log_cond_branch);
+			argv = mkIRExprVec_2( i_node_expr, ev->Ev.Bc.taken );
+			regparms = 2;
+			inew = i+1;
+			break;
+		case Ev_Bi:
+			/* Branch to an unknown destination */
+			helperName = "log_ind_branch";
+			helperAddr = SGL_(log_ind_branch);
+			argv = mkIRExprVec_2( i_node_expr, ev->Ev.Bi.dst );
+			regparms = 2;
+			inew = i+1;
+			break;
+		case Ev_G:
+			/* Global bus event (CAS, LOCK-prefix, LL-SC, etc) */
+			helperName = "log_global_event";
+			helperAddr = SGL_(log_global_event);
+			argv = mkIRExprVec_1( i_node_expr );
+			regparms = 1;
+			inew = i+1;
+			break;
+		case Ev_Comp:
+		   /* Compute */
+		   helperName = "log_comp_event";
+		   helperAddr = SGL_(log_comp_event);
+		   argv = mkIRExprVec_3( 
+				   i_node_expr,
+		   		   mkIRExpr_HWord(ev->Ev.Comp.op_type),
+		   		   mkIRExpr_HWord(ev->Ev.Comp.arity)
+				   );
+		   regparms = 3;
+		   inew = i+1;
+		   break;
+		default:
+			tl_assert(0);
+		}
+
+		CLG_DEBUGIF(5) {
+		if (inew > i+1) {
+			VG_(printf)("   merge ");
+			showEvent( ev2 );
+		}
+		if (inew > i+2) {
+			VG_(printf)("   merge ");
+			showEvent( ev3 );
+		}
+		if (helperAddr)
+			VG_(printf)("   call  %s (%p)\n",
+			helperName, helperAddr);
+		}
+		
+		/* helper could be unset depending on the simulator used */
+		if (helperAddr == 0) continue;
+		
+		/* Add the helper. */
+		tl_assert(helperName);
+		tl_assert(helperAddr);
+		tl_assert(argv);
+		di = unsafeIRDirty_0_N( regparms,
+		  	      helperName, VG_(fnptr_to_fnentry)( helperAddr ),
+		  	      argv );
+		addStmtToIRSB( clgs->sbOut, IRStmt_Dirty(di) );
+	}
+
+	clgs->events_used = 0;
+}
+
+static void addEvent_Ir ( ClgState* clgs, InstrInfo* inode )
+{
+   Event* evt;
+   tl_assert(clgs->seen_before || (inode->eventset == 0));
+
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag      = Ev_Ir;
+   evt->inode    = inode;
+   clgs->events_used++;
+}
+
+static
+void addEvent_Dr ( ClgState* clgs, InstrInfo* inode, Int datasize, IRAtom* ea )
+{
+   Event* evt;
+   tl_assert(isIRAtom(ea));
+   tl_assert(datasize >= 1);
+   
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag       = Ev_Dr;
+   evt->inode     = inode;
+   evt->Ev.Dr.szB = datasize;
+   evt->Ev.Dr.ea  = ea;
+   clgs->events_used++;
+}
+
+static
+void addEvent_Dw ( ClgState* clgs, InstrInfo* inode, Int datasize, IRAtom* ea )
+{
+   Event* lastEvt;
+   Event* evt;
+   tl_assert(isIRAtom(ea));
+   tl_assert(datasize >= 1);
+  
+   /* Is it possible to merge this write with the preceding read? */
+   lastEvt = &clgs->events[clgs->events_used-1];
+   if (clgs->events_used > 0
+       && lastEvt->tag       == Ev_Dr
+       && lastEvt->Ev.Dr.szB == datasize
+       && lastEvt->inode     == inode
+       && eqIRAtom(lastEvt->Ev.Dr.ea, ea))
+   {
+      lastEvt->tag   = Ev_Dm;
+      return;
+   }
+
+   /* No.  Add as normal. */
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag       = Ev_Dw;
+   evt->inode     = inode;
+   evt->Ev.Dw.szB = datasize;
+   evt->Ev.Dw.ea  = ea;
+   clgs->events_used++;
+}
+
+static
+void addEvent_D_guarded ( ClgState* clgs, InstrInfo* inode,
+                          Int datasize, IRAtom* ea, IRAtom* guard,
+                          Bool isWrite )
+{
+   tl_assert(isIRAtom(ea));
+   tl_assert(guard);
+   tl_assert(isIRAtom(guard));
+   tl_assert(datasize >= 1);
+ 
+   /* Adding guarded memory actions and merging them with the existing
+      queue is too complex.  Simply flush the queue and add this
+      action immediately.  Since guarded loads and stores are pretty
+      rare, this is not thought likely to cause any noticeable
+      performance loss as a result of the loss of event-merging
+      opportunities. */
+   tl_assert(clgs->events_used >= 0);
+   flushEvents(clgs);
+   tl_assert(clgs->events_used == 0);
+   /* Same as case Ev_Dw / case Ev_Dr in flushEvents, except with guard */
+   IRExpr*      i_node_expr;
+   const HChar* helperName;
+   void*        helperAddr;
+   IRExpr**     argv;
+   Int          regparms;
+   IRDirty*     di;
+   i_node_expr = mkIRExpr_HWord( (HWord)inode );
+   helperName  = isWrite ? "log_0I1Dw"
+                         : "log_0I1Dr";
+   helperAddr  = isWrite ? SGL_(log_0I1Dw)
+                         : SGL_(log_0I1Dr);
+   argv        = mkIRExprVec_3( i_node_expr,
+                                ea, mkIRExpr_HWord( datasize ) );
+   regparms    = 3;
+   di          = unsafeIRDirty_0_N(
+                    regparms, 
+                    helperName, VG_(fnptr_to_fnentry)( helperAddr ), 
+                    argv );
+   di->guard = guard;
+   addStmtToIRSB( clgs->sbOut, IRStmt_Dirty(di) );
+}
+
+static
+void addEvent_Bc ( ClgState* clgs, InstrInfo* inode, IRAtom* guard )
+{
+   Event* evt;
+   tl_assert(isIRAtom(guard));
+   tl_assert(typeOfIRExpr(clgs->sbOut->tyenv, guard)
+             == (sizeof(HWord)==4 ? Ity_I32 : Ity_I64));
+   if (!CLG_(clo).simulate_branch) return;
+
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag         = Ev_Bc;
+   evt->inode       = inode;
+   evt->Ev.Bc.taken = guard;
+   clgs->events_used++;
+}
+
+static
+void addEvent_Bi ( ClgState* clgs, InstrInfo* inode, IRAtom* whereTo )
+{
+   Event* evt;
+   tl_assert(isIRAtom(whereTo));
+   tl_assert(typeOfIRExpr(clgs->sbOut->tyenv, whereTo)
+             == (sizeof(HWord)==4 ? Ity_I32 : Ity_I64));
+   if (!CLG_(clo).simulate_branch) return;
+
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag       = Ev_Bi;
+   evt->inode     = inode;
+   evt->Ev.Bi.dst = whereTo;
+   clgs->events_used++;
+}
+
+static
+void addEvent_G ( ClgState* clgs, InstrInfo* inode )
+{
+   Event* evt;
+   if (!CLG_(clo).collect_bus) return;
+
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+   evt->tag       = Ev_G;
+   evt->inode     = inode;
+   clgs->events_used++;
+}
+
+static
+void addEvent_Comp( ClgState* clgs, InstrInfo* inode, IRExprTag arity, IRType op_type )
+{
+   Event* evt;
+
+   if (clgs->events_used == N_EVENTS)
+      flushEvents(clgs);
+   tl_assert(clgs->events_used >= 0 && clgs->events_used < N_EVENTS);
+   evt = &clgs->events[clgs->events_used];
+   init_Event(evt);
+
+   evt->tag       = Ev_Comp;
+   evt->inode     = inode;
+   evt->Ev.Comp.arity = arity;
+   evt->Ev.Comp.op_type = op_type;
+   clgs->events_used++;
+}
+
+/* Initialise or check (if already seen before) an InstrInfo for next insn.
+   We only can set instr_offset/instr_size here. The required event set and
+   resulting cost offset depend on events (Ir/Dr/Dw/Dm) in guest
+   instructions. The event set is extended as required on flush of the event
+   queue (when Dm events were determined), cost offsets are determined at
+   end of BB instrumentation. */
+static
+InstrInfo* next_InstrInfo ( ClgState* clgs, Addr instr_addr, UInt instr_size )
+{
+   InstrInfo* ii;
+   tl_assert(clgs->ii_index >= 0);
+   tl_assert(clgs->ii_index < clgs->bb->instr_count);
+   ii = &clgs->bb->instr[ clgs->ii_index ];
+
+   if (clgs->seen_before) {
+       CLG_ASSERT(ii->instr_offset == clgs->instr_offset);
+       CLG_ASSERT(ii->instr_size == instr_size);
+   }
+   else {
+       ii->instr_offset = clgs->instr_offset;
+       ii->instr_size = instr_size;
+	   ii->instr_addr = instr_addr;
+       ii->cost_offset = 0;
+       ii->eventset = 0;
+   }
+
+   clgs->ii_index++;
+   clgs->instr_offset += instr_size;
+   CLG_(stat).distinct_instrs++;
+
+   return ii;
+}
+
+/*--------------------------------------------------------------------*/
+/*--- Discarding BB info                                           ---*/
+/*--------------------------------------------------------------------*/
+
+// Called when a translation is removed from the translation cache for
+// any reason at all: to free up space, because the guest code was
+// unmapped or modified, or for any arbitrary reason.
+static
+void clg_discard_superblock_info ( Addr orig_addr, VexGuestExtents vge )
+{
+    tl_assert(vge.n_used > 0);
+
+   if (0)
+      VG_(printf)( "discard_superblock_info: %p, %p, %llu\n",
+                   (void*)orig_addr,
+                   (void*)vge.base[0], (ULong)vge.len[0]);
+
+   // Get BB info, remove from table, free BB info.  Simple!
+   // When created, the BB is keyed by the first instruction address,
+   // (not orig_addr, but eventually redirected address). Thus, we
+   // use the first instruction address in vge.
+   CLG_(delete_bb)(vge.base[0]);
+}
+
+
+/*------------------------------------------------------------*/
+/*--- CLG_(fini)() and related function                     ---*/
+/*------------------------------------------------------------*/
+
+
+
+static void zero_thread_cost(thread_info* t)
+{
+  Int i;
+
+  for(i = 0; i < CLG_(current_call_stack).sp; i++) {
+    if (!CLG_(current_call_stack).entry[i].jcc) continue;
+
+    /* reset call counters to current for active calls */
+    CLG_(current_call_stack).entry[i].jcc->call_counter = 0;
+  }
+
+  CLG_(forall_bbccs)(CLG_(zero_bbcc));
+
+}
+
+void CLG_(zero_all_cost)(Bool only_current_thread)
+{
+  if (VG_(clo_verbosity) > 1)
+    VG_(message)(Vg_DebugMsg, "  Zeroing costs...\n");
+
+  if (only_current_thread)
+    zero_thread_cost(CLG_(get_current_thread)());
+  else
+    CLG_(forall_threads)(zero_thread_cost);
+
+  if (VG_(clo_verbosity) > 1)
+    VG_(message)(Vg_DebugMsg, "  ...done\n");
+}
+
+static
+void unwind_thread(thread_info* t)
+{
+  /* unwind signal handlers */
+  while(CLG_(current_state).sig !=0)
+    CLG_(post_signal)(CLG_(current_tid),CLG_(current_state).sig);
+
+  /* unwind regular call stack */
+  while(CLG_(current_call_stack).sp>0)
+    CLG_(pop_call_stack)();
+
+  /* reset context and function stack for context generation */
+  CLG_(init_exec_state)( &CLG_(current_state) );
+  CLG_(current_fn_stack).top = CLG_(current_fn_stack).bottom;
+}
+
+void CLG_(set_instrument_state)(const HChar* reason, Bool state)
+{
+  if (CLG_(instrument_state) == state) {
+    CLG_DEBUG(2, "%s: instrumentation already %s\n",
+	     reason, state ? "ON" : "OFF");
+    return;
+  }
+  CLG_(instrument_state) = state;
+  CLG_DEBUG(2, "%s: Switching instrumentation %s ...\n",
+	   reason, state ? "ON" : "OFF");
+
+  VG_(discard_translations_safely)( (Addr)0x1000, ~(SizeT)0xfff, "callgrind");
+
+  /* reset internal state: call stacks, simulator */
+  CLG_(forall_threads)(unwind_thread);
+
+  if (VG_(clo_verbosity) > 1)
+    VG_(message)(Vg_DebugMsg, "%s: instrumentation switched %s\n",
+		 reason, state ? "ON" : "OFF");
+}
+
+
+/* Dump current state */
+static void dump_state_togdb(void)
+{
+    thread_info** th;
+    int t;
+    Int orig_tid = CLG_(current_tid);
+
+    VG_(gdb_printf)("instrumentation: %s\n",
+		    CLG_(instrument_state) ? "on":"off");
+    if (!CLG_(instrument_state)) return;
+
+    VG_(gdb_printf)("executed-bbs: %llu\n", CLG_(stat).bb_executions);
+    VG_(gdb_printf)("executed-calls: %llu\n", CLG_(stat).call_counter);
+    VG_(gdb_printf)("distinct-bbs: %d\n", CLG_(stat).distinct_bbs);
+    VG_(gdb_printf)("distinct-calls: %d\n", CLG_(stat).distinct_jccs);
+    VG_(gdb_printf)("distinct-functions: %d\n", CLG_(stat).distinct_fns);
+    VG_(gdb_printf)("distinct-contexts: %d\n", CLG_(stat).distinct_contexts);
+
+    /* threads */
+    th = CLG_(get_threads)();
+    VG_(gdb_printf)("threads:");
+    for(t=1;t<VG_N_THREADS;t++) {
+	if (!th[t]) continue;
+	VG_(gdb_printf)(" %d", t);
+    }
+    VG_(gdb_printf)("\n");
+    VG_(gdb_printf)("current-tid: %d\n", orig_tid);
+}
+
+  
+static void print_monitor_help ( void )
+{
+   VG_(gdb_printf) ("\n");
+   VG_(gdb_printf) ("callgrind monitor commands:\n");
+   VG_(gdb_printf) ("  dump [<dump_hint>]\n");
+   VG_(gdb_printf) ("        dump counters\n");
+   VG_(gdb_printf) ("  zero\n");
+   VG_(gdb_printf) ("        zero counters\n");
+   VG_(gdb_printf) ("  status\n");
+   VG_(gdb_printf) ("        print status\n");
+   VG_(gdb_printf) ("  instrumentation [on|off]\n");
+   VG_(gdb_printf) ("        get/set (if on/off given) instrumentation state\n");
+   VG_(gdb_printf) ("\n");
+}
+
+/* return True if request recognised, False otherwise */
+static Bool handle_gdb_monitor_command (ThreadId tid, const HChar *req)
+{
+   HChar* wcmd;
+   HChar s[VG_(strlen(req)) + 1]; /* copy for strtok_r */
+   HChar *ssaveptr;
+
+   VG_(strcpy) (s, req);
+
+   wcmd = VG_(strtok_r) (s, " ", &ssaveptr);
+   switch (VG_(keyword_id) ("help dump zero status instrumentation", 
+                            wcmd, kwd_report_duplicated_matches)) {
+   case -2: /* multiple matches */
+      return True;
+   case -1: /* not found */
+      return False;
+   case  0: /* help */
+      print_monitor_help();
+      return True;
+   case  1: { /* dump */
+      return True;
+   }
+   case  2: { /* zero */
+      CLG_(zero_all_cost)(False);
+      return True;
+   }
+
+   case 3: { /* status */
+     HChar* arg = VG_(strtok_r) (0, " ", &ssaveptr);
+     if (arg && (VG_(strcmp)(arg, "internal") == 0)) {
+       /* internal interface to callgrind_control */
+       dump_state_togdb();
+       return True;
+     }
+
+     if (!CLG_(instrument_state)) {
+       VG_(gdb_printf)("No status available as instrumentation is switched off\n");
+     } else {
+       // Status information to be improved ...
+       thread_info** th = CLG_(get_threads)();
+       Int t, tcount = 0;
+       for(t=1;t<VG_N_THREADS;t++)
+	 if (th[t]) tcount++;
+       VG_(gdb_printf)("%d thread(s) running.\n", tcount);
+     }
+     return True;
+   }
+
+   case 4: { /* instrumentation */
+     HChar* arg = VG_(strtok_r) (0, " ", &ssaveptr);
+     if (!arg) {
+       VG_(gdb_printf)("instrumentation: %s\n",
+		       CLG_(instrument_state) ? "on":"off");
+     }
+     else
+       CLG_(set_instrument_state)("Command", VG_(strcmp)(arg,"off")!=0);
+     return True;
+   }
+
+   default: 
+      tl_assert(0);
+      return False;
+   }
+}
+
+static
+Bool CLG_(handle_client_request)(ThreadId tid, UWord *args, UWord *ret)
+{
+   if (!VG_IS_TOOL_USERREQ('C','T',args[0])
+       && VG_USERREQ__GDB_MONITOR_COMMAND   != args[0])
+      return False;
+
+   switch(args[0]) 
+   {
+   case VG_USERREQ__DUMP_STATS:     
+      *ret = 0;                 /* meaningless */
+      break;
+
+   case VG_USERREQ__DUMP_STATS_AT:
+     {
+       const HChar *arg = (HChar*)args[1];
+       HChar buf[30 + VG_(strlen)(arg)];    // large enough
+       VG_(sprintf)(buf,"Client Request: %s", arg);
+       *ret = 0;                 /* meaningless */
+     }
+     break;
+
+   case VG_USERREQ__ZERO_STATS:
+     CLG_(zero_all_cost)(True);
+      *ret = 0;                 /* meaningless */
+      break;
+
+   case VG_USERREQ__TOGGLE_COLLECT:
+     CLG_(current_state).collect = !CLG_(current_state).collect;
+     CLG_DEBUG(2, "Client Request: toggled collection state to %s\n",
+	      CLG_(current_state).collect ? "ON" : "OFF");
+     *ret = 0;                 /* meaningless */
+     break;
+
+   case VG_USERREQ__START_INSTRUMENTATION:
+     CLG_(set_instrument_state)("Client Request", True);
+     *ret = 0;                 /* meaningless */
+     break;
+
+   case VG_USERREQ__STOP_INSTRUMENTATION:
+     CLG_(set_instrument_state)("Client Request", False);
+     *ret = 0;                 /* meaningless */
+     break;
+
+   case VG_USERREQ__GDB_MONITOR_COMMAND: {
+      Bool handled = handle_gdb_monitor_command (tid, (HChar*)args[1]);
+      if (handled)
+         *ret = 1;
+      else
+         *ret = 0;
+      return handled;
+   }
+
+   /*******************************************
+    * Synchronixation API intercepts 
+	*
+	* FIXME default OpenMP behavior modeled after
+	* KS need for SynchroTrace, reassess if this
+	* is appropriate */
+										 
+   case VG_USERREQ__SIGIL_PTHREAD_CREATE_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_CREATE_LEAVE:
+      /* enable and log once the thread has been CREATED and waiting */
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_CREATE, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_JOIN_ENTER:
+      /* log when the thread join is ENTERED and disable */
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_JOIN, args[1], UNUSED_SYNC_DATA);
+      }
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_JOIN_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      break;
+
+   case VG_USERREQ__SIGIL_GOMP_LOCK_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_SETLOCK_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_CRITSTART_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_ATOMICSTART_ENTER:
+   case VG_USERREQ__SIGIL_PTHREAD_LOCK_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_GOMP_SETLOCK_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_LOCK_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_CRITSTART_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_CRITNAMESTART_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_ATOMICSTART_LEAVE:
+   case VG_USERREQ__SIGIL_PTHREAD_LOCK_LEAVE:
+      /* enable and log once the lock has been acquired */
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_LOCK, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_GOMP_UNLOCK_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_UNSETLOCK_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_CRITEND_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_ATOMICEND_ENTER:
+   case VG_USERREQ__SIGIL_PTHREAD_UNLOCK_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_GOMP_UNLOCK_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_UNSETLOCK_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_CRITEND_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_CRITNAMEEND_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_ATOMICEND_LEAVE:
+   case VG_USERREQ__SIGIL_PTHREAD_UNLOCK_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_UNLOCK, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_GOMP_BARRIER_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_ENTER:
+   case VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_ENTER:
+   case VG_USERREQ__SIGIL_PTHREAD_BARRIER_ENTER:
+      /* log once the barrier is ENTERED and waiting and disable */
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_BARRIER, args[1], UNUSED_SYNC_DATA);
+      }
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_GOMP_BARRIER_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAIT_LEAVE:
+   case VG_USERREQ__SIGIL_GOMP_TEAMBARRIERWAITFINAL_LEAVE:
+   case VG_USERREQ__SIGIL_PTHREAD_BARRIER_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_CONDWAIT_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_CONDWAIT, args[1], args[2]);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_CONDSIG_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_CONDSIG_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_CONDSIG, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_CONDBROAD_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_CONDBROAD, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_SPINLOCK_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_SPINLOCK, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   case VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_ENTER:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = True;
+      break;
+   case VG_USERREQ__SIGIL_PTHREAD_SPINUNLOCK_LEAVE:
+      SGL_(thread_in_synccall)[SGL_(active_tid)] = False;
+      if ( EVENT_GENERATION_ENABLED )
+      {
+         SGL_(log_sync)((UChar)SGLPRIM_SYNC_SPINUNLOCK, args[1], UNUSED_SYNC_DATA);
+      }
+      break;
+
+   default:
+      return False;
+   }
+
+
+   return True;
+}
+
+
+/* Syscall Timing */
+
+/* struct timeval syscalltime[VG_N_THREADS]; */
+#if CLG_MICROSYSTIME
+ULong *syscalltime;
+#else
+UInt *syscalltime;
+#endif
+
+
+static
+void clg_print_stats(void)
+{
+}
+
+static void finish(void)
+{
+  CLG_DEBUG(0, "finish()\n");
+
+  /* pop all remaining items from CallStack for correct sum
+   */
+  CLG_(forall_threads)(unwind_thread);
+
+  /* finish IPC with Sigil2 */
+  SGL_(term_IPC)();
+  SGL_(end_logging)();
+}
+
+
+void CLG_(fini)(Int exitcode)
+{
+  finish();
+}
+
+
+/*--------------------------------------------------------------------*/
+/*--- Setup                                                        ---*/
+/*--------------------------------------------------------------------*/
+
+static void clg_start_client_code_callback ( ThreadId tid, ULong blocks_done )
+{
+   static ULong last_blocks_done = 0;
+
+   if (0)
+      VG_(printf)("%d R %llu\n", (Int)tid, blocks_done);
+
+   /* throttle calls to CLG_(run_thread) by number of BBs executed */
+   if (blocks_done - last_blocks_done < 5000) return;
+   last_blocks_done = blocks_done;
+
+   CLG_(run_thread)( tid );
+}
+
+static
+void CLG_(post_clo_init)(void)
+{
+   SGL_(init_IPC)(); // initialize interface to Sigil
+   SGL_(is_in_event_collect_func) = False;
+
+   if (SGL_(clo).collect_func == NULL &&
+       SGL_(clo).start_collect_func == NULL)
+   {
+      VG_(umsg)("*********************************************\n");
+      VG_(umsg)("Beginning event generation from program start\n");
+      VG_(umsg)("*********************************************\n");
+      SGL_(is_in_event_collect_func) = True;
+   }
+
+   if (SGL_(clo).gen_cf == True)
+      VG_(umsg)("WARNING: Control Flow events unsupported\n");
+   if (SGL_(clo).gen_bb == True)
+      VG_(umsg)("WARNING: Basic Block context events unsupported\n");
+
+   if (VG_(clo_vex_control).iropt_register_updates_default
+       != VexRegUpdSpAtMemAccess) {
+      CLG_DEBUG(1, " Using user specified value for "
+                "--vex-iropt-register-updates\n");
+   } else {
+      CLG_DEBUG(1, 
+                " Using default --vex-iropt-register-updates="
+                "sp-at-mem-access\n");
+   }
+
+   if (VG_(clo_px_file_backed) != VexRegUpdSpAtMemAccess) {
+      CLG_DEBUG(1, " Using user specified value for "
+                "--px-file-backed\n");
+   } else {
+      CLG_DEBUG(1, 
+                " Using default --px-file-backed="
+                "sp-at-mem-access\n");
+   }
+
+   if (VG_(clo_vex_control).iropt_unroll_thresh != 0) {
+      VG_(message)(Vg_UserMsg, 
+                   "callgrind only works with --vex-iropt-unroll-thresh=0\n"
+                   "=> resetting it back to 0\n");
+      VG_(clo_vex_control).iropt_unroll_thresh = 0;   // cannot be overriden.
+   }
+   if (VG_(clo_vex_control).guest_chase_thresh != 0) {
+      VG_(message)(Vg_UserMsg,
+                   "callgrind only works with --vex-guest-chase-thresh=0\n"
+                   "=> resetting it back to 0\n");
+      VG_(clo_vex_control).guest_chase_thresh = 0; // cannot be overriden.
+   }
+   
+   CLG_DEBUG(1, "  dump threads: %s\n", CLG_(clo).separate_threads ? "Yes":"No");
+   CLG_DEBUG(1, "  call sep. : %d\n", CLG_(clo).separate_callers);
+   CLG_DEBUG(1, "  rec. sep. : %d\n", CLG_(clo).separate_recursions);
+
+   if (!CLG_(clo).dump_line && !CLG_(clo).dump_instr && !CLG_(clo).dump_bb) {
+       VG_(message)(Vg_UserMsg, "Using source line as position.\n");
+       CLG_(clo).dump_line = True;
+   }
+
+   /* initialize hash tables */
+   CLG_(init_obj_table)();
+   CLG_(init_cxt_table)();
+   CLG_(init_bb_hash)();
+
+   CLG_(init_threads)();
+   CLG_(run_thread)(1);
+
+   CLG_(instrument_state) = CLG_(clo).instrument_atstart;
+}
+
+static
+void CLG_(pre_clo_init)(void)
+{
+    VG_(details_name)            ("Sigrind");
+    VG_(details_version)         (NULL);
+    VG_(details_description)     ("");
+    VG_(details_copyright_author)("Copyright (C) 2015-2017, "
+				  "by Michael Lui et al.");
+    VG_(details_bug_reports_to)  (VG_BUGS_TO);
+    VG_(details_avg_translation_sizeB) ( 500 );
+
+    VG_(clo_vex_control).iropt_register_updates_default
+       = VG_(clo_px_file_backed)
+       = VexRegUpdSpAtMemAccess; // overridable by the user.
+
+    VG_(clo_vex_control).iropt_unroll_thresh = 0;   // cannot be overriden.
+    VG_(clo_vex_control).guest_chase_thresh = 0;    // cannot be overriden.
+
+    VG_(basic_tool_funcs)        (CLG_(post_clo_init),
+                                  CLG_(instrument),
+                                  CLG_(fini));
+
+    VG_(needs_superblock_discards)(clg_discard_superblock_info);
+
+
+    VG_(needs_command_line_options)(CLG_(process_cmd_line_option),
+				    CLG_(print_usage),
+				    CLG_(print_debug_usage));
+
+    VG_(needs_client_requests)(CLG_(handle_client_request));
+    VG_(needs_print_stats)    (clg_print_stats);
+
+    VG_(track_start_client_code)  ( & clg_start_client_code_callback );
+    VG_(track_pre_deliver_signal) ( & CLG_(pre_signal) );
+    VG_(track_post_deliver_signal)( & CLG_(post_signal) );
+
+    /* Track syscalls */
+    /* XXX MDL20170226
+     * Right now syscalls are not being monitored.
+     * There hasn't been a convincing case made that memory reads/writes
+     * from syscalls are significant enough to warrant the extra monitoring.
+     * If required, the following callbacks can be used to get addt'l info
+     * every time a syscall writes to an address, and also in general when
+     * a syscall is invoked.
+     *
+     * VG_(track_post_mem_write) (vgcore_memwrite_callback)
+     * VG_(needs_syscall_wrapp)  (pre_syscall_callback,
+     *                            post_syscall_callback)
+     *
+     * For more info look in ../include/pub_tool_tooliface.h
+     * The other core event callbacks offered by Valgrind probably aren't
+     * relelvant for us. */
+
+    /* Defaults */
+    SGL_(set_clo_defaults)();
+    CLG_(set_clo_defaults)();
+}
+
+VG_DETERMINE_INTERFACE_VERSION(CLG_(pre_clo_init))
+
+/*------------------------------------------------------------*/
+/*--- Initialization and helpers                           ---*/
+/*------------------------------------------------------------*/
+
+
+static void init_Event ( Event* ev ) {
+   VG_(memset)(ev, 0, sizeof(Event));
+}
+
+static IRAtom* get_Event_dea ( Event* ev ) {
+   switch (ev->tag) {
+      case Ev_Dr: return ev->Ev.Dr.ea;
+      case Ev_Dw: return ev->Ev.Dw.ea;
+      case Ev_Dm: return ev->Ev.Dm.ea;
+      default:    tl_assert(0);
+   }
+}
+
+static Int get_Event_dszB ( Event* ev ) {
+   switch (ev->tag) {
+      case Ev_Dr: return ev->Ev.Dr.szB;
+      case Ev_Dw: return ev->Ev.Dw.szB;
+      case Ev_Dm: return ev->Ev.Dm.szB;
+      default:    tl_assert(0);
+   }
+}
+
+#if defined(VG_BIGENDIAN)
+# define CLGEndness Iend_BE
+#elif defined(VG_LITTLEENDIAN)
+# define CLGEndness Iend_LE
+#else
+# error "Unknown endianness"
+#endif
+
+static
+Addr IRConst2Addr(IRConst* con)
+{
+    Addr addr;
+
+    if (sizeof(Addr) == 4) {
+	CLG_ASSERT( con->tag == Ico_U32 );
+	addr = con->Ico.U32;
+    }
+    else if (sizeof(Addr) == 8) {
+	CLG_ASSERT( con->tag == Ico_U64 );
+	addr = con->Ico.U64;
+    }
+    else
+	VG_(tool_panic)("Callgrind: invalid Addr type");
+
+    return addr;
+}
+
+/* First pass over a BB to instrument, counting instructions and jumps
+ * This is needed for the size of the BB struct to allocate
+ *
+ * Called from CLG_(get_bb)
+ */
+void CLG_(collectBlockInfo)(IRSB* sbIn,
+			    /*INOUT*/ UInt* instrs,
+			    /*INOUT*/ UInt* cjmps,
+			    /*INOUT*/ Bool* cjmp_inverted)
+{
+    Int i;
+    IRStmt* st;
+    Addr instrAddr =0, jumpDst;
+    UInt instrLen = 0;
+    Bool toNextInstr = False;
+
+    // Ist_Exit has to be ignored in preamble code, before first IMark:
+    // preamble code is added by VEX for self modifying code, and has
+    // nothing to do with client code
+    Bool inPreamble = True;
+
+    if (!sbIn) return;
+
+    for (i = 0; i < sbIn->stmts_used; i++) {
+	  st = sbIn->stmts[i];
+	  if (Ist_IMark == st->tag) {
+	      inPreamble = False;
+
+	      instrAddr = st->Ist.IMark.addr;
+	      instrLen  = st->Ist.IMark.len;
+
+	      (*instrs)++;
+	      toNextInstr = False;
+	  }
+	  if (inPreamble) continue;
+	  if (Ist_Exit == st->tag) {
+	      jumpDst = IRConst2Addr(st->Ist.Exit.dst);
+	      toNextInstr =  (jumpDst == instrAddr + instrLen);
+
+	      (*cjmps)++;
+	  }
+    }
+
+    /* if the last instructions of BB conditionally jumps to next instruction
+     * (= first instruction of next BB in memory), this is a inverted by VEX.
+     */
+    *cjmp_inverted = toNextInstr;
+}
+
+static
+void addConstMemStoreStmt( IRSB* bbOut, UWord addr, UInt val, IRType hWordTy)
+{
+    addStmtToIRSB( bbOut,
+		   IRStmt_Store(CLGEndness,
+				IRExpr_Const(hWordTy == Ity_I32 ?
+					     IRConst_U32( addr ) :
+					     IRConst_U64( addr )),
+				IRExpr_Const(IRConst_U32(val)) ));
+}   
+
+
+
+static void showEvent ( Event* ev )
+{
+   switch (ev->tag) {
+      case Ev_Ir:
+	 VG_(printf)("Ir (InstrInfo %p) at +%u\n",
+		     ev->inode, ev->inode->instr_offset);
+	 break;
+      case Ev_Dr:
+	 VG_(printf)("Dr (InstrInfo %p) at +%u %d EA=",
+		     ev->inode, ev->inode->instr_offset, ev->Ev.Dr.szB);
+	 ppIRExpr(ev->Ev.Dr.ea);
+	 VG_(printf)("\n");
+	 break;
+      case Ev_Dw:
+	 VG_(printf)("Dw (InstrInfo %p) at +%u %d EA=",
+		     ev->inode, ev->inode->instr_offset, ev->Ev.Dw.szB);
+	 ppIRExpr(ev->Ev.Dw.ea);
+	 VG_(printf)("\n");
+	 break;
+      case Ev_Dm:
+	 VG_(printf)("Dm (InstrInfo %p) at +%u %d EA=",
+		     ev->inode, ev->inode->instr_offset, ev->Ev.Dm.szB);
+	 ppIRExpr(ev->Ev.Dm.ea);
+	 VG_(printf)("\n");
+	 break;
+      case Ev_Bc:
+         VG_(printf)("Bc %p   GA=", ev->inode);
+         ppIRExpr(ev->Ev.Bc.taken);
+         VG_(printf)("\n");
+         break;
+      case Ev_Bi:
+         VG_(printf)("Bi %p  DST=", ev->inode);
+         ppIRExpr(ev->Ev.Bi.dst);
+         VG_(printf)("\n");
+         break;
+      case Ev_G:
+         VG_(printf)("G  %p\n", ev->inode);
+         break;
+      default:
+	 tl_assert(0);
+	 break;
+   }
+}
+
+/*--------------------------------------------------------------------*/
+/*--- end                                                   main.c ---*/
+/*--------------------------------------------------------------------*/
diff --git a/sigrind/sigil2_ipc.c b/sigrind/sigil2_ipc.c
new file mode 100644
index 000000000..9b97a5947
--- /dev/null
+++ b/sigrind/sigil2_ipc.c
@@ -0,0 +1,266 @@
+#include "sigil2_ipc.h"
+#include "coregrind/pub_core_libcfile.h"
+#include "coregrind/pub_core_aspacemgr.h"
+#include "coregrind/pub_core_syscall.h"
+#include "pub_tool_basics.h"
+#include "pub_tool_vki.h"       // errnum, vki_timespec
+#include "pub_tool_vkiscnums.h" // __NR_nanosleep
+
+static Bool initialized = False;
+static Int emptyfd;
+static Int fullfd;
+static Sigil2DBISharedData* shmem;
+/* IPC channel */
+
+
+static UInt           curr_idx;
+static EventBuffer*   curr_ev_buf;
+static SglEvVariant*  curr_ev_slot;
+static NameBuffer*    curr_name_buf;
+static char*          curr_name_slot;
+/* cached IPC state */
+
+
+static Bool is_full[SIGIL2_IPC_BUFFERS];
+/* track available buffers */
+
+
+static inline void set_and_init_buffer(UInt buf_idx)
+{
+    curr_ev_buf = shmem->eventBuffers + buf_idx;
+    curr_ev_buf->used = 0;
+    curr_ev_slot = curr_ev_buf->events + curr_ev_buf->used;
+
+    curr_name_buf = shmem->nameBuffers + buf_idx;
+    curr_name_buf->used = 0;
+    curr_name_slot = curr_name_buf->names + curr_name_buf->used;
+}
+
+
+static inline void flush_to_sigil2(void)
+{
+    /* Mark that the buffer is being flushed,
+     * and tell Sigil2 the buffer is ready to consume */
+    is_full[curr_idx] = True;
+    Int res = VG_(write)(fullfd, &curr_idx, sizeof(curr_idx));
+    if (res != sizeof(curr_idx))
+    {
+        VG_(umsg)("error VG_(write)\n");
+        VG_(umsg)("error writing to Sigrind fifo\n");
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+}
+
+
+static inline void set_next_buffer(void)
+{
+    /* try the next buffer, circular */
+    ++curr_idx;
+    if (curr_idx == SIGIL2_IPC_BUFFERS)
+        curr_idx = 0;
+
+    /* if the next buffer is full,
+     * wait until Sigil2 communicates that it's free */
+    if (is_full[curr_idx])
+    {
+        UInt buf_idx;
+        Int res = VG_(read)(emptyfd, &buf_idx, sizeof(buf_idx));
+        if (res != sizeof(buf_idx))
+        {
+            VG_(umsg)("error VG_(read)\n");
+            VG_(umsg)("error reading from Sigrind fifo\n");
+            VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+            VG_(exit)(1);
+        }
+
+        tl_assert(buf_idx < SIGIL2_IPC_BUFFERS);
+        tl_assert(buf_idx == curr_idx);
+        curr_idx = buf_idx;
+        is_full[curr_idx] = False;
+    }
+
+    set_and_init_buffer(curr_idx);
+}
+
+
+static inline Bool is_events_full(void)
+{
+    return curr_ev_buf->used == SIGIL2_EVENTS_BUFFER_SIZE;
+}
+
+
+static inline Bool is_names_full(UInt size)
+{
+    return (curr_name_buf->used + size) > SIGIL2_EVENTS_BUFFER_SIZE;
+}
+
+
+SglEvVariant* SGL_(acq_event_slot)()
+{
+    tl_assert(initialized == True);
+
+    if (is_events_full())
+    {
+        flush_to_sigil2();
+        set_next_buffer();
+    }
+
+    curr_ev_buf->used++;
+    return curr_ev_slot++;
+}
+
+
+EventNameSlotTuple SGL_(acq_event_name_slot)(UInt size)
+{
+    tl_assert(initialized == True);
+
+    if (is_events_full() || is_names_full(size))
+    {
+        flush_to_sigil2();
+        set_next_buffer();
+    }
+
+    EventNameSlotTuple tuple = {curr_ev_slot, curr_name_slot, curr_name_buf->used};
+    curr_ev_buf->used   += 1;
+    curr_ev_slot        += 1;
+    curr_name_buf->used += size;
+    curr_name_slot      += size;
+
+    return tuple;
+}
+
+
+/******************************
+ * Initialization/Termination
+ ******************************/
+static int open_fifo(const HChar *fifo_path, int flags)
+{
+    tl_assert(initialized == False);
+
+    int tries = 0;
+    const int max_tries = 4;
+    int fd = VG_(fd_open)(fifo_path, flags, 0600);
+    while (fd < 0)
+    {
+        if (++tries < max_tries)
+        {
+#if defined(VGO_linux) && defined(VGA_amd64)
+            /* TODO any serious implications in Valgrind of calling syscalls directly?
+             * MDL20170220 The "VG_(syscall)" wrappers don't look like they do much
+             * else besides doing platform specific setup.
+             * In our case, we only accommodate x86_64 or aarch64. */
+            struct vki_timespec req;
+            req.tv_sec = 0;
+            req.tv_nsec = 500000000;
+            /* wait some time before trying to connect,
+             * giving Sigil2 time to bring up IPC */
+            VG_(do_syscall2)(__NR_nanosleep, (UWord)&req, 0);
+#else
+#error "Only linux is supported"
+#endif
+            fd = VG_(fd_open)(fifo_path, flags, 0600);
+        }
+        else
+        {
+            VG_(umsg)("FIFO for Sigrind failed\n");
+            VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+            VG_(exit) (1);
+        }
+    }
+
+    return fd;
+}
+
+
+static Sigil2DBISharedData* open_shmem(const HChar *shmem_path, int flags)
+{
+    tl_assert(initialized == False);
+
+    int shared_mem_fd = VG_(fd_open)(shmem_path, flags, 0600);
+    if (shared_mem_fd < 0)
+    {
+        VG_(umsg)("Cannot open shared_mem file %s\n", shmem_path);
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+
+    SysRes res = VG_(am_shared_mmap_file_float_valgrind)(sizeof(Sigil2DBISharedData),
+                                                         VKI_PROT_READ|VKI_PROT_WRITE,
+                                                         shared_mem_fd, (Off64T)0);
+    if (sr_isError(res))
+    {
+        VG_(umsg)("error %lu %s\n", sr_Err(res), VG_(strerror)(sr_Err(res)));
+        VG_(umsg)("error VG_(am_shared_mmap_file_float_valgrind) %s\n", shmem_path);
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+
+    Addr addr_shared = sr_Res (res);
+    VG_(close)(shared_mem_fd);
+
+    return (Sigil2DBISharedData*) addr_shared;
+}
+
+
+void SGL_(init_IPC)()
+{
+    tl_assert(initialized == False);
+
+    if (SGL_(clo).ipc_dir == NULL)
+    {
+       VG_(fmsg)("No --ipc-dir argument found, shutting down...\n");
+       VG_(exit)(1);
+    }
+
+    Int ipc_dir_len = VG_(strlen)(SGL_(clo).ipc_dir);
+    Int filename_len;
+
+    //len is strlen + null + other chars (/ and -0)
+    filename_len = ipc_dir_len + VG_(strlen)(SIGIL2_IPC_SHMEM_BASENAME) + 4;
+    HChar shmem_path[filename_len];
+    VG_(snprintf)(shmem_path, filename_len, "%s/%s-0", SGL_(clo).ipc_dir, SIGIL2_IPC_SHMEM_BASENAME);
+
+    filename_len = ipc_dir_len + VG_(strlen)(SIGIL2_IPC_EMPTYFIFO_BASENAME) + 4;
+    HChar emptyfifo_path[filename_len];
+    VG_(snprintf)(emptyfifo_path, filename_len, "%s/%s-0", SGL_(clo).ipc_dir, SIGIL2_IPC_EMPTYFIFO_BASENAME);
+
+    filename_len = ipc_dir_len + VG_(strlen)(SIGIL2_IPC_FULLFIFO_BASENAME) + 4;
+    HChar fullfifo_path[filename_len];
+    VG_(snprintf)(fullfifo_path, filename_len, "%s/%s-0", SGL_(clo).ipc_dir, SIGIL2_IPC_FULLFIFO_BASENAME);
+
+    emptyfd = open_fifo(emptyfifo_path, VKI_O_RDONLY);
+    fullfd  = open_fifo(fullfifo_path, VKI_O_WRONLY);
+    shmem   = open_shmem(shmem_path, VKI_O_RDWR);
+
+    /* initialize cached IPC state */
+    curr_idx = 0;
+    set_and_init_buffer(curr_idx);
+    for (UInt i=0; i<SIGIL2_IPC_BUFFERS; ++i)
+        is_full[i] = False;
+
+    initialized = True;
+}
+
+
+void SGL_(term_IPC)(void)
+{
+    tl_assert(initialized == True);
+
+    /* send finish sequence */
+    UInt finished = SIGIL2_IPC_FINISHED;
+    if (VG_(write)(fullfd, &curr_idx, sizeof(curr_idx)) != sizeof(curr_idx) ||
+        VG_(write)(fullfd, &finished, sizeof(finished)) != sizeof(finished))
+    {
+        VG_(umsg)("error VG_(write)\n");
+        VG_(umsg)("error writing to Sigrind fifo\n");
+        VG_(umsg)("Cannot recover from previous error. Good-bye.\n");
+        VG_(exit)(1);
+    }
+
+    /* wait until Sigrind disconnects */
+    while (VG_(read)(emptyfd, &finished, sizeof(finished)) > 0);
+
+    VG_(close)(emptyfd);
+    VG_(close)(fullfd);
+}
diff --git a/sigrind/sigil2_ipc.h b/sigrind/sigil2_ipc.h
new file mode 100644
index 000000000..d779653f3
--- /dev/null
+++ b/sigrind/sigil2_ipc.h
@@ -0,0 +1,28 @@
+#ifndef SGL_IPC_H
+#define SGL_IPC_H
+
+#include "Frontends/CommonShmemIPC.h"
+#include "global.h"
+
+/* An implementation of interprocess communication with the Sigil2 frontend.
+ * IPC includes initialization, termination, shared memory buffer writes, and
+ * synchronization via named pipes */
+
+typedef struct EventNameSlotTuple
+{
+    SglEvVariant*  event_slot;
+    char*          name_slot;
+    UInt           name_idx;
+} EventNameSlotTuple;
+
+void SGL_(init_IPC)(void);
+void SGL_(term_IPC)(void);
+
+SglEvVariant* SGL_(acq_event_slot)(void);
+/* Get a buffer slot to add an event */
+
+EventNameSlotTuple SGL_(acq_event_name_slot)(UInt size);
+/* Get a buffer slot to add an event (probably a context event)
+ * and a name slot to add a name with it (like a function name) */
+
+#endif
diff --git a/sigrind/tests/Makefile.am b/sigrind/tests/Makefile.am
new file mode 100644
index 000000000..972acc3ce
--- /dev/null
+++ b/sigrind/tests/Makefile.am
@@ -0,0 +1,5 @@
+
+include $(top_srcdir)/Makefile.tool-tests.am
+
+SUBDIRS = .
+DIST_SUBDIRS = .
diff --git a/sigrind/threads.c b/sigrind/threads.c
new file mode 100644
index 000000000..800250d14
--- /dev/null
+++ b/sigrind/threads.c
@@ -0,0 +1,451 @@
+/*--------------------------------------------------------------------*/
+/*--- Callgrind                                                    ---*/
+/*---                                                 ct_threads.c ---*/
+/*--------------------------------------------------------------------*/
+
+/*
+   This file is part of Callgrind, a Valgrind tool for call tracing.
+
+   Copyright (C) 2002-2015, Josef Weidendorfer (Josef.Weidendorfer@gmx.de)
+
+   This program is free software; you can redistribute it and/or
+   modify it under the terms of the GNU General Public License as
+   published by the Free Software Foundation; either version 2 of the
+   License, or (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful, but
+   WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+   General Public License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with this program; if not, write to the Free Software
+   Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA
+   02111-1307, USA.
+
+   The GNU General Public License is contained in the file COPYING.
+*/
+
+#include "log_events.h"
+#include "global.h"
+#include "Core/PrimitiveEnums.h"
+
+#include "pub_tool_threadstate.h"
+
+/* forward decls */
+static exec_state* exec_state_save(void);
+static exec_state* exec_state_restore(void);
+static exec_state* push_exec_state(int);
+static exec_state* top_exec_state(void);
+
+static exec_stack current_states;
+
+
+/*------------------------------------------------------------*/
+/*--- Support for multi-threading                          ---*/
+/*------------------------------------------------------------*/
+
+
+/*
+ * For Valgrind, MT is cooperative (no preemting in our code),
+ * so we don't need locks...
+ *
+ * Per-thread data:
+ *  - BBCCs
+ *  - call stack
+ *  - call hash
+ *  - event counters: last, current
+ *
+ * Even when ignoring MT, we need this functions to set up some
+ * datastructures for the process (= Thread 1).
+ */
+
+/* current running thread */
+ThreadId CLG_(current_tid);
+ThreadId SGL_(active_tid);
+
+static thread_info** thread;
+Bool* SGL_(thread_in_synccall);
+
+thread_info** CLG_(get_threads)()
+{
+  return thread;
+}
+
+thread_info* CLG_(get_current_thread)()
+{
+  return thread[CLG_(current_tid)];
+}
+
+void CLG_(init_threads)()
+{
+    UInt i;
+
+    thread = CLG_MALLOC("cl.threads.it.1", VG_N_THREADS * sizeof thread[0]);
+    SGL_(thread_in_synccall) = CLG_MALLOC("cl.threads.it.1", VG_N_THREADS * sizeof *SGL_(thread_in_synccall));
+
+    for(i=0;i<VG_N_THREADS;i++)
+    {
+        thread[i] = 0;
+        SGL_(thread_in_synccall)[i] = False;
+    }
+    CLG_(current_tid) = VG_INVALID_THREADID;
+    SGL_(active_tid) = VG_INVALID_THREADID;
+}
+
+/* switches through all threads and calls func */
+void CLG_(forall_threads)(void (*func)(thread_info*))
+{
+  Int t, orig_tid = CLG_(current_tid);
+
+  for(t=1;t<VG_N_THREADS;t++) {
+    if (!thread[t]) continue;
+    CLG_(switch_thread)(t);
+    (*func)(thread[t]);
+  }
+  CLG_(switch_thread)(orig_tid);
+}
+
+
+static
+thread_info* new_thread(void)
+{
+    thread_info* t;
+
+    t = (thread_info*) CLG_MALLOC("cl.threads.nt.1",
+                                  sizeof(thread_info));
+
+    /* init state */
+    CLG_(init_exec_stack)( &(t->states) );
+    CLG_(init_call_stack)( &(t->calls) );
+    CLG_(init_fn_stack)  ( &(t->fns) );
+    /* t->states.entry[0]->cxt = CLG_(get_cxt)(t->fns.bottom); */
+
+    /* init data containers */
+    CLG_(init_fn_array)( &(t->fn_active) );
+    CLG_(init_bbcc_hash)( &(t->bbccs) );
+    CLG_(init_jcc_hash)( &(t->jccs) );
+
+    return t;
+}
+
+void SGL_(switch_thread)(ThreadId tid)
+{
+  if (tid == SGL_(active_tid))
+    return;
+
+  SGL_(active_tid) = tid;
+
+  /* ML: always send thread switch events; 
+   * valgrind can change at any time, even if sigrind is 'inside' 
+   * a synchronization call or outside the function being collected */ 
+  SGL_(log_sync)(SGLPRIM_SYNC_SWAP, SGL_(active_tid), UNUSED_SYNC_DATA);
+}
+
+void CLG_(switch_thread)(ThreadId tid)
+{
+  SGL_(switch_thread)(tid);
+  if (tid == CLG_(current_tid)) return;
+
+  CLG_DEBUG(0, ">> thread %u (was %u)\n", tid, CLG_(current_tid));
+
+  if (CLG_(current_tid) != VG_INVALID_THREADID) {    
+    /* save thread state */
+    thread_info* t = thread[CLG_(current_tid)];
+
+    CLG_ASSERT(t != 0);
+
+    /* current context (including signal handler contexts) */
+    exec_state_save();
+    CLG_(copy_current_exec_stack)( &(t->states) );
+    CLG_(copy_current_call_stack)( &(t->calls) );
+    CLG_(copy_current_fn_stack)  ( &(t->fns) );
+
+    CLG_(copy_current_fn_array) ( &(t->fn_active) );
+    /* If we cumulate costs of threads, use TID 1 for all jccs/bccs */
+    if (!CLG_(clo).separate_threads) t = thread[1];
+    CLG_(copy_current_bbcc_hash)( &(t->bbccs) );
+    CLG_(copy_current_jcc_hash) ( &(t->jccs) );
+  }
+
+  CLG_(current_tid) = tid;
+  CLG_ASSERT(tid < VG_N_THREADS);
+
+  if (tid != VG_INVALID_THREADID) {
+    thread_info* t;
+
+    /* load thread state */
+
+    if (thread[tid] == 0) thread[tid] = new_thread();
+    t = thread[tid];
+
+    /* current context (including signal handler contexts) */
+    CLG_(set_current_exec_stack)( &(t->states) );
+    exec_state_restore();
+    CLG_(set_current_call_stack)( &(t->calls) );
+    CLG_(set_current_fn_stack)  ( &(t->fns) );
+
+    CLG_(set_current_fn_array)  ( &(t->fn_active) );
+    /* If we cumulate costs of threads, use TID 1 for all jccs/bccs */
+    if (!CLG_(clo).separate_threads) t = thread[1];
+    CLG_(set_current_bbcc_hash) ( &(t->bbccs) );
+    CLG_(set_current_jcc_hash)  ( &(t->jccs) );
+  }
+}
+
+
+void CLG_(run_thread)(ThreadId tid)
+{
+    /* now check for thread switch */
+    CLG_(switch_thread)(tid);
+}
+
+void CLG_(pre_signal)(ThreadId tid, Int sigNum, Bool alt_stack)
+{
+    exec_state *es;
+
+    CLG_DEBUG(0, ">> pre_signal(TID %u, sig %d, alt_st %s)\n",
+	     tid, sigNum, alt_stack ? "yes":"no");
+
+    /* switch to the thread the handler runs in */
+    CLG_(switch_thread)(tid);
+
+    /* save current execution state */
+    exec_state_save();
+
+    /* setup new cxtinfo struct for this signal handler */
+    es = push_exec_state(sigNum);
+    es->call_stack_bottom = CLG_(current_call_stack).sp;
+
+    /* setup current state for a spontaneous call */
+    CLG_(init_exec_state)( &CLG_(current_state) );
+    CLG_(current_state).sig = sigNum;
+    CLG_(push_cxt)(0);
+}
+
+/* Run post-signal if the stackpointer for call stack is at
+ * the bottom in current exec state (e.g. a signal handler)
+ *
+ * Called from CLG_(pop_call_stack)
+ */
+void CLG_(run_post_signal_on_call_stack_bottom)()
+{
+    exec_state* es = top_exec_state();
+    CLG_ASSERT(es != 0);
+    CLG_ASSERT(CLG_(current_state).sig >0);
+
+    if (CLG_(current_call_stack).sp == es->call_stack_bottom)
+	CLG_(post_signal)( CLG_(current_tid), CLG_(current_state).sig );
+}
+
+void CLG_(post_signal)(ThreadId tid, Int sigNum)
+{
+    exec_state* es;
+    UInt fn_number, *pactive;
+
+    CLG_DEBUG(0, ">> post_signal(TID %u, sig %d)\n",
+	     tid, sigNum);
+
+    /* thread switching potentially needed, eg. with instrumentation off */
+    CLG_(switch_thread)(tid);
+    CLG_ASSERT(sigNum == CLG_(current_state).sig);
+
+    /* Unwind call stack of this signal handler.
+     * This should only be needed at finalisation time
+     */
+    es = top_exec_state();
+    CLG_ASSERT(es != 0);
+    while(CLG_(current_call_stack).sp > es->call_stack_bottom)
+      CLG_(pop_call_stack)();
+
+    if (CLG_(current_state).cxt) {
+      /* correct active counts */
+      fn_number = CLG_(current_state).cxt->fn[0]->number;
+      pactive = CLG_(get_fn_entry)(fn_number);
+      (*pactive)--;
+      CLG_DEBUG(0, "  set active count of %s back to %u\n",
+	       CLG_(current_state).cxt->fn[0]->name, *pactive);
+    }
+
+    if (CLG_(current_fn_stack).top > CLG_(current_fn_stack).bottom) {
+	/* set fn_stack_top back.
+	 * top can point to 0 if nothing was executed in the signal handler;
+	 * this is possible at end on unwinding handlers.
+	 */
+	if (*(CLG_(current_fn_stack).top) != 0) {
+	    CLG_(current_fn_stack).top--;
+	    CLG_ASSERT(*(CLG_(current_fn_stack).top) == 0);
+	}
+      if (CLG_(current_fn_stack).top > CLG_(current_fn_stack).bottom)
+	CLG_(current_fn_stack).top--;
+    }
+
+    /* restore previous context */
+    es->sig = -1;
+    current_states.sp--;
+    es = top_exec_state();
+    CLG_(current_state).sig = es->sig;
+    exec_state_restore();
+
+    /* There is no way to reliable get the thread ID we are switching to
+     * after this handler returns. So we sync with actual TID at start of
+     * CLG_(setup_bb)(), which should be the next for callgrind.
+     */
+}
+
+
+
+/*------------------------------------------------------------*/
+/*--- Execution states in a thread & signal handlers       ---*/
+/*------------------------------------------------------------*/
+
+/* Each thread can be interrupted by a signal handler, and they
+ * themselves again. But as there's no scheduling among handlers
+ * of the same thread, we don't need additional stacks.
+ * So storing execution contexts and
+ * adding separators in the callstack(needed to not intermix normal/handler
+ * functions in contexts) should be enough.
+ */
+
+/* not initialized: call_stack_bottom, sig */
+void CLG_(init_exec_state)(exec_state* es)
+{
+  es->collect = CLG_(clo).collect_atstart;
+  es->cxt  = 0;
+  es->jmps_passed = 0;
+  es->bbcc = 0;
+  es->nonskipped = 0;
+}
+
+
+static exec_state* new_exec_state(Int sigNum)
+{
+    exec_state* es;
+    es = (exec_state*) CLG_MALLOC("cl.threads.nes.1",
+                                  sizeof(exec_state));
+
+    /* allocate real cost space: needed as incremented by
+     * simulation functions */
+
+    CLG_(init_exec_state)(es);
+    es->sig        = sigNum;
+    es->call_stack_bottom  = 0;
+
+    return es;
+}
+
+void CLG_(init_exec_stack)(exec_stack* es)
+{
+  Int i;
+
+  /* The first element is for the main thread */
+  es->entry[0] = new_exec_state(0);
+  for(i=1;i<MAX_SIGHANDLERS;i++)
+    es->entry[i] = 0;
+  es->sp = 0;
+}
+
+void CLG_(copy_current_exec_stack)(exec_stack* dst)
+{
+  Int i;
+
+  dst->sp = current_states.sp;
+  for(i=0;i<MAX_SIGHANDLERS;i++)
+    dst->entry[i] = current_states.entry[i];
+}
+
+void CLG_(set_current_exec_stack)(exec_stack* dst)
+{
+  Int i;
+
+  current_states.sp = dst->sp;
+  for(i=0;i<MAX_SIGHANDLERS;i++)
+    current_states.entry[i] = dst->entry[i];
+}
+
+
+/* Get top context info struct of current thread */
+static
+exec_state* top_exec_state(void)
+{
+  Int sp = current_states.sp;
+  exec_state* es;
+
+  CLG_ASSERT((sp >= 0) && (sp < MAX_SIGHANDLERS));
+  es = current_states.entry[sp];
+  CLG_ASSERT(es != 0);
+  return es;
+}
+
+/* Allocates a free context info structure for a new entered
+ * signal handler, putting it on the context stack.
+ * Returns a pointer to the structure.
+ */
+static exec_state* push_exec_state(int sigNum)
+{
+  Int sp;
+  exec_state* es;
+
+  current_states.sp++;
+  sp = current_states.sp;
+
+  CLG_ASSERT((sigNum > 0) && (sigNum <= _VKI_NSIG));
+  CLG_ASSERT((sp > 0) && (sp < MAX_SIGHANDLERS));
+  es = current_states.entry[sp];
+  if (!es) {
+    es = new_exec_state(sigNum);
+    current_states.entry[sp] = es;
+  }
+  else
+    es->sig = sigNum;
+
+  return es;
+}
+
+/* Save current context to top cxtinfo struct */
+static
+exec_state* exec_state_save(void)
+{
+  exec_state* es = top_exec_state();
+
+  es->cxt         = CLG_(current_state).cxt;
+  es->collect     = CLG_(current_state).collect;
+  es->jmps_passed = CLG_(current_state).jmps_passed;
+  es->bbcc        = CLG_(current_state).bbcc;
+  es->nonskipped  = CLG_(current_state).nonskipped;
+  CLG_ASSERT(es->cost == CLG_(current_state).cost);
+
+  CLG_DEBUGIF(1) {
+    CLG_DEBUG(1, "  cxtinfo_save(sig %d): collect %s, jmps_passed %d\n",
+	     es->sig, es->collect ? "Yes": "No", es->jmps_passed);	
+    CLG_(print_bbcc)(-9, es->bbcc);
+  }
+
+  /* signal number does not need to be saved */
+  CLG_ASSERT(CLG_(current_state).sig == es->sig);
+
+  return es;
+}
+
+static
+exec_state* exec_state_restore(void)
+{
+  exec_state* es = top_exec_state();
+
+  CLG_(current_state).cxt     = es->cxt;
+  CLG_(current_state).collect = es->collect;
+  CLG_(current_state).jmps_passed = es->jmps_passed;
+  CLG_(current_state).bbcc    = es->bbcc;
+  CLG_(current_state).nonskipped = es->nonskipped;
+  CLG_(current_state).cost    = es->cost;
+  CLG_(current_state).sig     = es->sig;
+
+  CLG_DEBUGIF(1) {
+	CLG_DEBUG(1, "  exec_state_restore(sig %d): collect %s, jmps_passed %d\n",
+		  es->sig, es->collect ? "Yes": "No", es->jmps_passed);
+	CLG_(print_bbcc)(-9, es->bbcc);
+	CLG_(print_cxt)(-9, es->cxt, 0);
+  }
+
+  return es;
+}
-- 
2.16.2

