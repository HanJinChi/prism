From 19959f2fb8c4775f387740345eac2883ba71eb09 Mon Sep 17 00:00:00 2001
From: Mike Lui <mike.d.lui@gmail.com>
Date: Tue, 1 Aug 2017 13:29:28 -0400
Subject: [PATCH] patch

---
 clients/drsigil/CMakeLists.txt         |  28 ++
 clients/drsigil/drsigil.c              | 370 ++++++++++++++++++
 clients/drsigil/drsigil.h              | 328 ++++++++++++++++
 clients/drsigil/instrument.c           | 687 +++++++++++++++++++++++++++++++++
 clients/drsigil/ipc.c                  | 376 ++++++++++++++++++
 clients/drsigil/parser.c               |  90 +++++
 clients/drsigil/pthread_defines.h      | 283 ++++++++++++++
 clients/drsigil/start_stop_functions.h |  62 +++
 8 files changed, 2224 insertions(+)
 create mode 100644 clients/drsigil/CMakeLists.txt
 create mode 100644 clients/drsigil/drsigil.c
 create mode 100644 clients/drsigil/drsigil.h
 create mode 100644 clients/drsigil/instrument.c
 create mode 100644 clients/drsigil/ipc.c
 create mode 100644 clients/drsigil/parser.c
 create mode 100644 clients/drsigil/pthread_defines.h
 create mode 100644 clients/drsigil/start_stop_functions.h

diff --git a/clients/drsigil/CMakeLists.txt b/clients/drsigil/CMakeLists.txt
new file mode 100644
index 0000000..97ebc8e
--- /dev/null
+++ b/clients/drsigil/CMakeLists.txt
@@ -0,0 +1,28 @@
+# from Deployment example
+
+# Removes relocation bug, occuring in glibc 2.25,
+# also simplifies relocation in DR
+# set(DynamoRIO_USE_LIBC OFF)
+
+add_library(drsigil SHARED drsigil.c instrument.c ipc.c parser.c)
+find_package(DynamoRIO)
+if (NOT DynamoRIO_FOUND)
+  message(FATAL_ERROR "DynamoRIO package required to build")
+endif(NOT DynamoRIO_FOUND)
+configure_DynamoRIO_client(drsigil)
+
+# IPC w/ Sigil2
+include_directories(../../../..)
+
+# Sigil2 Primitives
+include_directories(../../../../..)
+
+# target DynamoRIO extensions
+use_DynamoRIO_extension(drsigil drmgr)
+use_DynamoRIO_extension(drsigil drutil)
+use_DynamoRIO_extension(drsigil drwrap)
+use_DynamoRIO_extension(drsigil drreg)
+use_DynamoRIO_extension(drsigil drsyms)
+
+install(TARGETS drsigil
+	DESTINATION ${INSTALL_CLIENTS_LIB})
diff --git a/clients/drsigil/drsigil.c b/clients/drsigil/drsigil.c
new file mode 100644
index 0000000..1a10986
--- /dev/null
+++ b/clients/drsigil/drsigil.c
@@ -0,0 +1,370 @@
+#include "drsigil.h"
+#include "pthread_defines.h"
+#include "start_stop_functions.h"
+#include <stddef.h> /* for offsetof */
+#include <string.h>
+
+#include "drmgr.h"
+#include "drwrap.h"
+#include "drutil.h"
+#include "drreg.h"
+#include "drsyms.h"
+
+/* Custom define strlen to remove dependency on libc */
+
+
+volatile bool roi = true;
+/* unused currently */
+
+static uint64 num_threads = 0;
+/* Thread IDs are generated by the order of each thread's initialization */
+
+int      tls_idx;
+reg_id_t raw_tls_seg;
+uint     raw_tls_memref_offs;
+
+///////////////////////////////////////////////////////
+// DynamoRIO event callbacks
+///////////////////////////////////////////////////////
+static dr_emit_flags_t
+event_bb_instrument(void *drcontext, /*UNUSED*/ void *tag,
+                    instrlist_t *ilist, instr_t *where,
+                    /*UNUSED*/ bool for_trace,
+                    /*UNUSED*/ bool translating,
+                    /*UNUSED*/ void *user_data)
+{
+    if (roi == false)
+        return DR_EMIT_DEFAULT;
+
+    if (!instr_is_app(where) ||
+        !instr_get_app_pc(where))
+        return DR_EMIT_DEFAULT;
+
+    per_thread_t *tcxt = drmgr_get_tls_field(drcontext, tls_idx);
+
+    /* reset the event block state if a new event block*/
+    if (instrlist_first_app(ilist) == where ||
+        instr_is_cti(instr_get_prev_app(where)))
+        instrument_reset(drcontext, ilist, where, tcxt);
+
+    /* set up the current iblock */
+    /* TODO rename these */
+    instr_block_t *iblock = tcxt->iblocks + tcxt->iblock_count;
+    tcxt->iblock_count += 1;
+
+    if (clo.enable_context_instr)
+    {
+        iblock->instr = where;
+        tcxt->event_block_events += 1;
+    }
+
+    if (clo.enable_mem)
+    {
+        iblock->mem_ref_count = 0;
+        if (instr_reads_memory(where) || instr_writes_memory(where))
+        {
+            /* if we don't need details of the memory operation,
+             * skip the extra instrumentation */
+            if (clo.memref_needed)
+                instrument_mem_cache(drcontext, ilist, where, &iblock->mem_ref_count);
+            else
+                instrument_mem_cache_no_memref(drcontext, ilist, where, &iblock->mem_ref_count);
+        }
+        tcxt->event_block_events += iblock->mem_ref_count;
+    }
+
+    if (clo.enable_comp)
+    {
+        iblock->comp_count = 0;
+        instrument_comp_cache(where, &iblock->comp_count, tcxt->current_iblock_comp);
+        tcxt->event_block_events += iblock->comp_count;
+        tcxt->current_iblock_comp += iblock->comp_count;
+    }
+
+    if (instr_is_cti(where) || where == instrlist_last_app(ilist))
+        instrument_sigil_events(drcontext, ilist, where, tcxt);
+
+    return DR_EMIT_DEFAULT;
+}
+
+
+static dr_emit_flags_t
+event_bb_app2app(void *drcontext, void *tag, instrlist_t *bb,
+                 bool for_trace, bool translating)
+{
+    /* we transform string loops into regular loops so we can more easily
+     * monitor every memory reference they make */
+    /* XXX
+     * We run into reachability problems,
+     * as-per the documentation on 'drutil_expand_rep_string',
+     * due to extra instrumentation added by Sigil2.
+     *
+     * We don't expect string loops to be significant in benchmarks,
+     * so this should be OK.
+
+    if (!drutil_expand_rep_string(drcontext, bb)) {
+        DR_ASSERT(false);
+    }
+
+     */
+    return DR_EMIT_DEFAULT;
+}
+
+static void
+event_thread_init(void *drcontext)
+{
+    drreg_set_bb_properties(drcontext,
+                            DRREG_CONTAINS_SPANNING_CONTROL_FLOW | DRREG_IGNORE_CONTROL_FLOW);
+
+    per_thread_t *init = dr_thread_alloc(drcontext, sizeof(per_thread_t));
+    init->memrefs      = dr_thread_alloc(drcontext, MEM_REF_BUF_SIZE);
+    init->mems         = dr_thread_alloc(drcontext, MEM_BUF_SIZE);
+    init->comps        = dr_thread_alloc(drcontext, COMP_BUF_SIZE);
+    init->iblocks      = dr_thread_alloc(drcontext, IBLOCK_BUF_SIZE);
+    init->sync_ev      = dr_thread_alloc(drcontext, sizeof(SglSyncEv));
+
+    if (init          == NULL ||
+        init->memrefs == NULL ||
+        init->comps   == NULL ||
+        init->sync_ev == NULL ||
+        init->iblocks == NULL)
+        DR_ABORT_MSG("Failed to allocate per-thread data\n");
+
+    init->thread_id = __sync_add_and_fetch(&num_threads,1);
+    init->has_channel_lock = false;
+    init->is_blocked = false;
+    init->current_iblock_comp = init->comps;
+    init->current_iblock_mem = init->mems;
+    init->iblock_count = 0;
+    init->event_block_events = 0;
+
+    init->seg_base = dr_get_dr_segment_base(raw_tls_seg);
+    PERTHR_PTR(init->seg_base)     = init;
+    IBLOCKS_PTR(init->seg_base)    = init->iblocks;
+    MEMREFBASE_PTR(init->seg_base) = init->memrefs;
+    MEMREFCURR_PTR(init->seg_base) = init->memrefs;
+    SGLEV_PTR(init->seg_base)      = NULL;
+    SGLEND_PTR(init->seg_base)     = NULL;
+    SGLUSED_PTR(init->seg_base)    = NULL;
+    SGLSYNCEV_PTR(init->seg_base)  = NULL;
+    ACTIVE(init->seg_base)         = true;
+    /* TODO(someday) are we sure we want to activate
+     * this thread right away?
+     * If ROI has not been entered yet, and this gets set to false,
+     * how do we notify all threads to activate?
+     * Would need to lock all threads and set, or some type of flush
+     * of all code caches */
+    drmgr_set_tls_field(drcontext, tls_idx, init);
+}
+
+
+static void
+event_thread_exit(void *drcontext)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(drcontext, tls_idx);
+    force_thread_flush(tcxt);
+    dr_thread_free(drcontext, tcxt->memrefs, MEM_REF_BUF_SIZE);
+    dr_thread_free(drcontext, tcxt->mems, MEM_BUF_SIZE);
+    dr_thread_free(drcontext, tcxt->comps, COMP_BUF_SIZE);
+    dr_thread_free(drcontext, tcxt->iblocks, IBLOCK_BUF_SIZE);
+    dr_thread_free(drcontext, tcxt->sync_ev, sizeof(SglSyncEv));
+    dr_thread_free(drcontext, tcxt, sizeof(per_thread_t));
+}
+
+
+static void
+event_exit(void)
+{
+    for(int i=0; i<clo.frontend_threads; ++i)
+        terminate_IPC(i);
+
+    if (!dr_raw_tls_cfree(raw_tls_memref_offs, MEMREF_TLS_COUNT))
+        DR_ABORT_MSG("failed to free raw tls");
+
+    if (!drmgr_unregister_thread_init_event(event_thread_init) ||
+        !drmgr_unregister_thread_exit_event(event_thread_exit) ||
+        !drmgr_unregister_tls_field(tls_idx))
+        DR_ABORT_MSG("failed to unregister drmgr event callbacks");
+
+    drutil_exit();
+    drmgr_exit();
+    drwrap_exit();
+    drreg_exit();
+    drsym_exit();
+}
+
+static void
+module_load_event(void *drcontext, const module_data_t *mod, bool loaded)
+{
+    app_pc towrap;
+    const char *module_name = dr_module_preferred_name(mod);
+
+    /* TODO if start_func/stop_func are the same
+     * start/stop functions may be in loaded library */
+    if ((clo.start_func != NULL) &&
+        (towrap = (app_pc)dr_get_proc_address(mod->handle, clo.start_func)) != NULL)
+        drwrap_wrap(towrap, wrap_pre_start_func, wrap_post_start_func);
+    if ((clo.stop_func != NULL) &&
+        (towrap = (app_pc)dr_get_proc_address(mod->handle, clo.stop_func)) != NULL)
+        drwrap_wrap(towrap, wrap_pre_stop_func, wrap_post_stop_func);
+
+    /* pthread_mutex_lock/unlock also exist in libc
+     * only wrap once when pthread lib is loaded */ 
+    if (strstr(module_name, "libpthread.so") == module_name)
+    {
+        if ((towrap = (app_pc)dr_get_proc_address(mod->handle, P_CREATE)) != NULL)
+            drwrap_wrap(towrap, wrap_pre_pthread_create, wrap_post_pthread_create);
+
+        if ((towrap = (app_pc)dr_get_proc_address(mod->handle, P_JOIN)) != NULL)
+            drwrap_wrap(towrap, wrap_pre_pthread_join, wrap_post_pthread_join);
+
+        if ((towrap = (app_pc)dr_get_proc_address(mod->handle, P_MUTEX_LOCK)) != NULL)
+            drwrap_wrap(towrap, wrap_pre_pthread_mutex_lock, wrap_post_pthread_mutex_lock);
+
+        if ((towrap = (app_pc)dr_get_proc_address(mod->handle, P_MUTEX_UNLOCK)) != NULL)
+            drwrap_wrap(towrap, wrap_pre_pthread_mutex_unlock, wrap_post_pthread_mutex_unlock);
+
+        if ((towrap = (app_pc)dr_get_proc_address(mod->handle, P_BARRIER)) != NULL)
+            drwrap_wrap(towrap, wrap_pre_pthread_barrier, wrap_post_pthread_barrier);
+
+        if ((towrap = (app_pc)dr_get_proc_address(mod->handle, P_COND_WAIT)) != NULL)
+            drwrap_wrap(towrap, wrap_pre_pthread_cond_wait, wrap_post_pthread_cond_wait);
+
+        if ((towrap = (app_pc)dr_get_proc_address(mod->handle, P_COND_SIG)) != NULL)
+            drwrap_wrap(towrap, wrap_pre_pthread_cond_sig, wrap_post_pthread_cond_sig);
+
+        if ((towrap = (app_pc)dr_get_proc_address(mod->handle, P_SPIN_LOCK)) != NULL)
+            drwrap_wrap(towrap, wrap_pre_pthread_spin_lock, wrap_post_pthread_spin_lock);
+
+        if ((towrap = (app_pc)dr_get_proc_address(mod->handle, P_SPIN_UNLOCK)) != NULL)
+            drwrap_wrap(towrap, wrap_pre_pthread_spin_unlock, wrap_post_pthread_spin_unlock);
+    }
+}
+
+
+///////////////////////////////////////////////////////
+// DynamoRIO client initialization
+///////////////////////////////////////////////////////
+static void
+check_lookup_symbol_error(drsym_error_t err)
+{
+    if (err == DRSYM_ERROR_LOAD_FAILED)
+        DR_ABORT_MSG("Could not load provided executable path");
+    else if (err != DRSYM_ERROR_SYMBOL_NOT_FOUND) /* unfound symbol not an error */
+        DR_ABORT_MSG("Error looking start/stop function in symbol table");
+}
+
+
+static void
+start_stop_roi_wrap()
+{
+    /* check if the start/stop ROI functions are in the main module
+     * e.g. if we only want to instrument at 'main' */
+
+    DR_ASSERT(clo.start_func != NULL || clo.stop_func != NULL);
+
+    module_data_t *main_mod = dr_get_main_module();
+    size_t modoffs = 0;
+
+    if (strcmp(clo.start_func, clo.stop_func) == 0)
+    {
+        /* if start function == stop function,
+         * then instrumentation is only for that one function */
+        drsym_error_t ret = drsym_lookup_symbol(main_mod->full_path,
+                                                clo.start_func,
+                                                &modoffs, 0);
+        if (ret == DRSYM_SUCCESS)
+        {
+            drwrap_wrap((app_pc)(main_mod->start+modoffs),
+                        wrap_pre_start_stop_same, wrap_post_start_stop_same);
+        }
+        else
+            check_lookup_symbol_error(ret);
+    }
+    else
+    {
+        if (clo.start_func != NULL)
+        {
+            drsym_error_t ret = drsym_lookup_symbol(main_mod->full_path,
+                                                    clo.start_func,
+                                                    &modoffs, 0);
+            if (ret == DRSYM_SUCCESS)
+            {
+                drwrap_wrap((app_pc)(main_mod->start+modoffs),
+                            wrap_pre_start_func, wrap_post_start_func);
+            }
+            else
+                check_lookup_symbol_error(ret);
+        }
+
+        if (clo.stop_func != NULL)
+        {
+            drsym_error_t ret = drsym_lookup_symbol(main_mod->full_path,
+                                                    clo.stop_func,
+                                                    &modoffs, 0);
+            if (ret == DRSYM_SUCCESS)
+            {
+                drwrap_wrap((app_pc)(main_mod->start+modoffs),
+                            wrap_pre_stop_func, wrap_post_stop_func);
+            }
+            else
+                check_lookup_symbol_error(ret);
+        }
+    }
+}
+
+
+DR_EXPORT void
+dr_client_main(client_id_t id, int argc, const char *argv[])
+{
+    dr_set_client_name("DrSigil",
+                       "https://github.com/VANDAL/sigil2/issues");
+
+    parse(argc, (char**)argv);
+
+    dr_register_exit_event(event_exit);
+
+    drreg_options_t ops = {sizeof(ops), 5, false};
+
+    if (!drmgr_init() ||
+        !drutil_init() ||
+        !drwrap_init() ||
+        drsym_init(0) != DRSYM_SUCCESS ||
+        drreg_init(&ops) != DRREG_SUCCESS)
+        DR_ABORT_MSG("failed to initialize DynamoRIO extensions");
+
+    /* Specify priority relative to other instrumentation operations: */
+    drmgr_priority_t priority = {
+        sizeof(priority), /* size of struct */
+        "sigil2",         /* name of our operation */
+        NULL,             /* optional name of operation we should precede */
+        NULL,             /* optional name of operation we should follow */
+        0};               /* numeric priority */
+
+    if (!drmgr_register_bb_instrumentation_event(NULL, event_bb_instrument, NULL) ||
+        !drmgr_register_bb_app2app_event(event_bb_app2app, &priority) ||
+        !drmgr_register_thread_init_event(event_thread_init) ||
+        !drmgr_register_thread_exit_event(event_thread_exit) ||
+        !drmgr_register_module_load_event(module_load_event))
+        DR_ABORT_MSG("failed to register drmgr event callbacks");
+
+    /* threads spawned so far */
+    num_threads = 0;
+
+    /* Initialize IPC */
+    /* There are 'frontend_threads' number of channels */
+    for(int i=0; i<clo.frontend_threads; ++i)
+        init_IPC(i, clo.ipc_dir, clo.standalone);
+
+    /* initialize thread local resources */
+    tls_idx = drmgr_register_tls_field();
+
+    /* use a raw tls for the mem ref buffer since it's accessed often
+     * OPT can also FORCE user to use -thread_private CLO,
+     * which enables the use of absolute TLS references during instrumentation */
+    if (!dr_raw_tls_calloc(&raw_tls_seg, &raw_tls_memref_offs, MEMREF_TLS_COUNT, 0))
+        DR_ABORT_MSG("failed to allocate raw tls");
+
+    if (clo.start_func != NULL || clo.stop_func != NULL)
+        start_stop_roi_wrap();
+}
diff --git a/clients/drsigil/drsigil.h b/clients/drsigil/drsigil.h
new file mode 100644
index 0000000..b261b92
--- /dev/null
+++ b/clients/drsigil/drsigil.h
@@ -0,0 +1,328 @@
+#ifndef DRSIGIL_H
+#define DRSIGIL_H
+
+#include "dr_api.h"
+#include "Frontends/CommonShmemIPC.h"
+
+#define DR_ABORT_MSG(msg) DR_ASSERT_MSG(false, msg)
+
+#define MINSERT instrlist_meta_preinsert
+
+#define RESERVE_REGISTER(reg) \
+    if (drreg_reserve_register(drcontext, ilist, where, NULL, &reg) != DRREG_SUCCESS) \
+        DR_ASSERT(false);
+#define UNRESERVE_REGISTER(reg) \
+    if (drreg_unreserve_register(drcontext, ilist, where, reg) != DRREG_SUCCESS) \
+        DR_ASSERT(false);
+
+#ifdef SGLDEBUG
+#define SGL_DEBUG(...) dr_printf(__VA_ARGS__)
+#else
+#define SGL_DEBUG(...)
+#endif
+
+
+/////////////////////////////////////////////////////////////////////
+//                          IPC Management                         //
+/////////////////////////////////////////////////////////////////////
+
+typedef struct _ticket_node_t ticket_node_t;
+struct _ticket_node_t
+{
+    void *dr_event;
+    ticket_node_t *next;
+    uint thread_id;
+    volatile bool waiting;
+};
+
+
+typedef struct _ticket_queue_t
+{
+    /* Manage threads waiting to write to the shared memory
+     *
+     * Each thread will write directly to shared memory to
+     * avoid the memory usage+bandwidth overhead of writing
+     * to a local buffer and then copying to shared memory. */
+
+    volatile ticket_node_t *head;
+    volatile ticket_node_t *tail;
+    volatile bool locked;
+} ticket_queue_t;
+
+
+typedef struct _ipc_channel_t
+{
+    /* The shared memory channel between this DynamoRIO client application and
+     * Sigil2. Multiple channels can exist to reduce contention on the channels;
+     * the number of channels is determined by Sigil2 when DynamoRIO is invoked,
+     * via command line. Additionally, the number of channels will match the
+     * number of frontend Sigil2 threads, so that each thread will process one
+     * buffer. The buffer an application thread writes to depends on its thread
+     * id (thread id % number of channels). That is, if there is one channel,
+     * then all threads vie over that channel. */
+
+    void *queue_lock;
+    ticket_queue_t ticket_queue;
+    /* Multiple threads can write via this IPC channel.
+     * Only allow one at a time. */
+
+    Sigil2DBISharedData *shared_mem;
+    /* Produce data to this buffer */
+
+    file_t full_fifo;
+    /* Update Sigil2 via this fifo which buffers
+     * are full and ready to be consumed */
+
+    file_t empty_fifo;
+    /* Sigil2 updates DynamoRIO with the last
+     * buffer consumed(empty) via this fifo */
+
+    uint shmem_buf_idx;
+    /* The current buffer being filled in shared memory
+     * Must wrap around back to 0 at 'SIGIL2_DBI_BUFFERS' */
+
+    bool empty_buf_idx[SIGIL2_IPC_BUFFERS];
+    /* Corresponds to each buffer that is available for writing */
+
+    uint last_active_tid;
+    /* Required to let Sigil2 know when the TID of the current thread has changed */
+
+    bool initialized;
+    /* If this is a valid channel */
+
+    bool standalone;
+    /* Will be TRUE if this channel was not initialized with Sigil2 IPC;
+     * will 'fake' any IPC. */
+} ipc_channel_t;
+
+/////////////////////////////////////////////////////////////////////
+//                           Thread Data                           //
+/////////////////////////////////////////////////////////////////////
+
+typedef struct _mem_ref_t
+{
+    MemType type;
+    ushort size; // XXX big enough?
+    void *addr;
+} mem_ref_t;
+
+#define MAX_NUM_MEM_REFS 2048
+#define MAX_NUM_MEMS MAX_NUM_MEM_REFS/2
+#define MEM_REF_BUF_SIZE (sizeof(mem_ref_t) * MAX_NUM_MEM_REFS)
+#define MEM_BUF_SIZE (sizeof(SglMemEv) * MAX_NUM_MEMS)
+/* we should not have more than this many memory references per basic block */
+
+typedef struct _instr_block_t
+{
+    instr_t *instr;
+    uint mem_ref_count;
+    uint comp_count;
+} instr_block_t;
+
+#define MAX_NUM_COMPS 1024
+#define COMP_BUF_SIZE (sizeof(SglCompEv) * MAX_NUM_COMPS)
+
+#define MAX_NUM_IBLOCKS 1024
+#define IBLOCK_BUF_SIZE (sizeof(instr_block_t) * MAX_NUM_IBLOCKS)
+/* we should not need more than this per basic block */
+
+typedef struct _per_thread_t
+{
+    /* per-application-thread data
+     *
+     * This data tracks Sigil2 events for a given thread.
+     * The events are buffered from buf_base to buf_end,
+     * and flushed when either the buffer is full, or the thread exits.
+     *
+     * Synchronization events, i.e. thread library calls like pthread_create
+     * should only be tracked at a high level. The memory and compute events
+     * within each library call should not be tracked */
+
+    uint thread_id;
+    /* Unique ID
+     * Sigil2 expects threads to start from '1' */
+
+    bool has_channel_lock;
+    /* Is allowed to use the ipc channel */
+
+    bool is_blocked;
+    /* Mostly used for debugging.
+     * Is about to wait on a application-side lock.
+     * We must take care to ensure this thread never
+     * has the channel lock while blocked, otherwise
+     * we end up with an application-side deadlock */
+
+    mem_ref_t *memrefs;
+    SglMemEv *mems;
+    SglMemEv *current_iblock_mem;
+    /* Track memory references per event block.
+     * Memory addresses are recorded at execution time in a buffer,
+     * since they can't be analyzed at instrumentation time.
+     * All other memory info is stored per basic block in a small cache of
+     * direct Sigil2 memory event primitives */
+
+    SglCompEv *comps;
+    SglCompEv *current_iblock_comp;
+    /* Keep a local buffer of compute per event block.
+     * This just simplifies instrumentation */
+
+    SglSyncEv *sync_ev;
+    /* The latest sync event */
+
+    instr_block_t *iblocks;
+    uint iblock_count;
+    /* Each iblock holds sigil event data for a single instruction.
+     * The iblock buffer holds event data for a single event block */
+
+    uint event_block_events;
+    /* total sigil events for the current event block
+     * (single-entry, single-exit) */
+
+    byte *seg_base;
+    /* So we can access the raw TLS from client clean calls */
+
+} per_thread_t;
+
+#define MIN_DR_PER_THREAD_BUFFER_EVENTS (1UL << 20)
+
+volatile extern bool roi;
+/* Region-Of-Interest (ROI)
+ *
+ * If data should be collected or not, depending on command line arguments.
+ * If no relevant args are supplied, then the ROI is assumed to be the
+ * entirety of the application.
+ *
+ * This value should be used to control instrumentation.
+ * I.e. when not in the ROI, there should be no added instrumentation.
+ * After the ROI passes, instrumentation should be flushed.
+ *
+ * XXX Currently this implementation assumes roi will be set
+ * BEFORE threads are spawned, in a serial portion,
+ * and then unset AFTER all threads are joined, in a serial portion
+ * XXX Assumes only one ROI per execution
+ *
+ * TODO Why did I make this volatile? */
+
+extern int tls_idx;
+/* thread-local storage for per_thread_t */
+
+enum
+{
+    MEMREF_TLS_OFFS_PERTHR_PTR = 0,
+    /* the client's TLS (from drmgr) */
+
+    MEMREF_TLS_OFFS_SGLEV_PTR,
+    MEMREF_TLS_OFFS_SGLEND_PTR,
+    MEMREF_TLS_OFFS_SGLUSED_PTR,
+    /* sigil shared memory buffer */
+
+    MEMREF_TLS_OFFS_IBLOCKS_PTR,
+
+    MEMREF_TLS_OFFS_MEMREFBASE_PTR,
+    MEMREF_TLS_OFFS_MEMREFCURR_PTR,
+    /* the mem cache where addresses are stored */
+
+    MEMREF_TLS_OFFS_SGLSYNCEV_PTR,
+    /* holds NULL if no event, otherwise holds a SglSyncEv */
+
+    MEMREF_TLS_OFFS_ACTIVE,
+    /* whether the thread is under active instrumentation,
+     * e.g. a thread is inactive in a lock */
+
+    MEMREF_TLS_COUNT,
+};
+extern reg_id_t raw_tls_seg;
+extern uint     raw_tls_memref_offs;
+/* raw tls for faster access from instrumented code */
+
+#define TLS_SLOT(tls_base, enum_val) (void **)((byte *)(tls_base)+raw_tls_memref_offs+(sizeof(void*)*enum_val))
+
+#define PERTHR_PTR(tls_base) *(per_thread_t **)TLS_SLOT(tls_base, MEMREF_TLS_OFFS_PERTHR_PTR)
+#define SGLEV_PTR(tls_base) *(SglEvVariant **)TLS_SLOT(tls_base, MEMREF_TLS_OFFS_SGLEV_PTR)
+#define SGLEND_PTR(tls_base) *(SglEvVariant **)TLS_SLOT(tls_base, MEMREF_TLS_OFFS_SGLEND_PTR)
+#define SGLUSED_PTR(tls_base) *(size_t **)TLS_SLOT(tls_base, MEMREF_TLS_OFFS_SGLUSED_PTR)
+#define IBLOCKS_PTR(tls_base) *(instr_block_t **)TLS_SLOT(tls_base, MEMREF_TLS_OFFS_IBLOCKS_PTR)
+#define MEMREFBASE_PTR(tls_base) *(mem_ref_t **)TLS_SLOT(tls_base, MEMREF_TLS_OFFS_MEMREFBASE_PTR)
+#define MEMREFCURR_PTR(tls_base) *(mem_ref_t **)TLS_SLOT(tls_base, MEMREF_TLS_OFFS_MEMREFCURR_PTR)
+#define ACTIVE(tls_base) *(bool *)TLS_SLOT(tls_base, MEMREF_TLS_OFFS_ACTIVE)
+#define SGLSYNCEV_PTR(tls_base) *(SglSyncEv **)TLS_SLOT(tls_base, MEMREF_TLS_OFFS_SGLSYNCEV_PTR)
+
+#define PERTHR_OFFS (raw_tls_memref_offs + MEMREF_TLS_OFFS_PERTHR_PTR*sizeof(void*))
+#define SGLEV_OFFS (raw_tls_memref_offs + MEMREF_TLS_OFFS_SGLEV_PTR*sizeof(void*))
+#define SGLEND_OFFS (raw_tls_memref_offs + MEMREF_TLS_OFFS_SGLEND_PTR*sizeof(void*))
+#define SGLUSED_OFFS (raw_tls_memref_offs + MEMREF_TLS_OFFS_SGLUSED_PTR*sizeof(void*))
+#define IBLOCKS_OFFS (raw_tls_memref_offs + MEMREF_TLS_OFFS_IBLOCKS_PTR*sizeof(void*))
+#define MEMREFBASE_OFFS (raw_tls_memref_offs + MEMREF_TLS_OFFS_MEMREFBASE_PTR*sizeof(void*))
+#define MEMREFCURR_OFFS (raw_tls_memref_offs + MEMREF_TLS_OFFS_MEMREFCURR_PTR*sizeof(void*))
+#define ACTIVE_OFFS (raw_tls_memref_offs + MEMREF_TLS_OFFS_ACTIVE*sizeof(void*))
+#define SGLSYNCEV_OFFS (raw_tls_memref_offs + MEMREF_TLS_OFFS_SGLSYNCEV_PTR*sizeof(void*))
+
+/////////////////////////////////////////////////////////////////////
+//                           Option Parsing                        //
+/////////////////////////////////////////////////////////////////////
+typedef struct _command_line_options command_line_options;
+struct _command_line_options
+{
+    const char *ipc_dir;
+    /* Directory where shared memory and named fifos
+     * are located; generated by Sigil2 core */
+
+    const char *start_func;
+    const char *stop_func;
+    /* DrSigil will begin and end event generation at these functions */
+
+    int frontend_threads;
+    /* Essentially, DrSigil will serialize the
+     * instrumented binary into this many threads */
+
+    bool standalone;
+    /* In some cases (mainly testing), it is desirable
+     * to run this tool without Sigil2.
+     * This flag instructs the tool to ignore IPC with the
+     * Sigil2 core */
+
+    int enable_mem;
+    int enable_mem_type;
+    int enable_mem_addr;
+    int enable_mem_size;
+    int enable_comp;
+    int enable_comp_type;
+    int enable_sync;
+    int enable_sync_type;
+    int enable_sync_data;
+    int enable_context_instr;
+    /* instrumentation switches */
+
+    bool memref_needed;
+    /* if extra memory instrumentation features required */
+} clo;
+
+
+/////////////////////////////////////////////////////////////////////
+//                         FUNCTION DECLARATIONS                   //
+/////////////////////////////////////////////////////////////////////
+
+/* The instrumentation functions MUST be called in a specific order */
+/* TODO document preconditions/postconditions */
+void instrument_reset(void *drcontext, instrlist_t *ilist, instr_t *where,
+                      per_thread_t *tcxt);
+
+void instrument_mem_cache(void *drcontext, instrlist_t *ilist, instr_t *where,
+                          uint *mem_ref_count);
+void instrument_mem_cache_no_memref(void *drcontext, instrlist_t *ilist, instr_t *where,
+                                    uint *mem_ref_count);
+
+void instrument_comp_cache(instr_t *instr, uint *comp_count, SglCompEv *cache);
+
+void instrument_sigil_events(void *drcontext, instrlist_t *ilist, instr_t *where,
+                             per_thread_t *tcxt);
+
+/* IPC */
+void init_IPC(int idx, const char *path, bool standalone);
+void terminate_IPC(int idx);
+void set_shared_memory_buffer(per_thread_t *tcxt);
+void force_thread_flush(per_thread_t *tcxt);
+
+void parse(int argc, char *argv[]);
+
+#endif
diff --git a/clients/drsigil/instrument.c b/clients/drsigil/instrument.c
new file mode 100644
index 0000000..decee76
--- /dev/null
+++ b/clients/drsigil/instrument.c
@@ -0,0 +1,687 @@
+#include "drsigil.h"
+#include "drmgr.h"
+#include "drutil.h"
+#include "drreg.h"
+#include <stddef.h> /* for offsetof */
+#include <limits.h> /* for INT_MAX */
+
+#define SIZEOF_EVENT_SLOT sizeof(SglEvVariant)
+
+void
+instrument_reset(void *drcontext, instrlist_t *ilist, instr_t *where,
+                 per_thread_t *tcxt)
+{
+    /* Reset event block data */
+    tcxt->iblock_count = 0;
+    tcxt->event_block_events = 0;
+    tcxt->current_iblock_comp = tcxt->comps;
+    tcxt->current_iblock_mem = tcxt->mems;
+
+    if (clo.memref_needed)
+    {
+        /* reset mem ref buffer to point to the beginning */
+        reg_id_t reg;
+        RESERVE_REGISTER(reg);
+        dr_insert_read_raw_tls(drcontext, ilist, where,
+                               raw_tls_seg, MEMREFBASE_OFFS, reg);
+        dr_insert_write_raw_tls(drcontext, ilist, where,
+                                raw_tls_seg, MEMREFCURR_OFFS, reg);
+        UNRESERVE_REGISTER(reg);
+    }
+}
+
+
+static void
+instrument_mem_cache_helper(void *drcontext, instrlist_t *ilist, instr_t *where,
+                            reg_id_t buf_ptr, reg_id_t addr_reg, reg_id_t scratch_reg,
+                            ushort size, MemType type, opnd_t ref)
+{
+    /* set the attributes */
+
+    if (clo.enable_mem_type)
+    {
+        /* READ/WRITE */
+        MINSERT(ilist, where,
+                XINST_CREATE_store_1byte(drcontext,
+                                         OPND_CREATE_MEM8(buf_ptr, offsetof(mem_ref_t, type)),
+                                         OPND_CREATE_INT8(type)));
+    }
+
+    if (clo.enable_mem_size)
+    {
+        /* size of the mem ref operand */
+        MINSERT(ilist, where,
+                XINST_CREATE_store_2bytes(drcontext,
+                                          OPND_CREATE_MEM16(buf_ptr, offsetof(mem_ref_t, size)),
+                                          OPND_CREATE_INT16(size)));
+    }
+
+    if (clo.enable_mem_addr)
+    {
+        /* memory address */
+        drutil_insert_get_mem_addr(drcontext, ilist, where, ref, addr_reg, scratch_reg);
+        MINSERT(ilist, where,
+                XINST_CREATE_store(drcontext,
+                                   OPND_CREATE_MEMPTR(buf_ptr, offsetof(mem_ref_t, addr)),
+                                   opnd_create_reg(addr_reg)));
+    }
+}
+void
+instrument_mem_cache(void *drcontext, instrlist_t *ilist, instr_t *where,
+                     uint *mem_ref_count)
+{
+    reg_id_t buf_ptr, scratch1, scratch2;
+
+    drreg_reserve_aflags(drcontext, ilist, where);
+    RESERVE_REGISTER(buf_ptr);
+    RESERVE_REGISTER(scratch1);
+    RESERVE_REGISTER(scratch2);
+
+    /* current mem ref buffer pointer */
+    dr_insert_read_raw_tls(drcontext, ilist, where,
+                           raw_tls_seg, MEMREFCURR_OFFS, buf_ptr);
+
+    if (instr_reads_memory(where))
+    {
+        for (int i=0; i<instr_num_srcs(where); ++i)
+        {
+            opnd_t ref = instr_get_src(where, i);
+            if (opnd_is_memory_reference(ref))
+            {
+                instrument_mem_cache_helper(drcontext, ilist, where,
+                                            buf_ptr, scratch1, scratch2,
+                                            drutil_opnd_mem_size_in_bytes(ref, where),
+                                            SGLPRIM_MEM_LOAD, ref);
+
+                /* update count and buffer */
+                ++*mem_ref_count;
+                MINSERT(ilist, where,
+                        XINST_CREATE_add(drcontext,
+                                         opnd_create_reg(buf_ptr),
+                                         OPND_CREATE_INT32(sizeof(mem_ref_t))));
+            }
+        }
+    }
+
+    if (instr_writes_memory(where))
+    {
+        for (int i=0; i<instr_num_dsts(where); ++i)
+        {
+            opnd_t ref = instr_get_dst(where, i);
+            if (opnd_is_memory_reference(ref))
+            {
+
+                instrument_mem_cache_helper(drcontext, ilist, where,
+                                            buf_ptr, scratch1, scratch2,
+                                            drutil_opnd_mem_size_in_bytes(ref, where),
+                                            SGLPRIM_MEM_STORE, ref);
+
+                /* update count and buffer */
+                ++*mem_ref_count;
+                MINSERT(ilist, where,
+                        XINST_CREATE_add(drcontext,
+                                         opnd_create_reg(buf_ptr),
+                                         OPND_CREATE_INT32(sizeof(mem_ref_t))));
+            }
+        }
+    }
+
+    /* update the TLS mem ref pointer
+     * so new mem refs will be in the right place */
+    dr_insert_write_raw_tls(drcontext, ilist, where,
+                            raw_tls_seg, MEMREFCURR_OFFS, buf_ptr);
+
+    UNRESERVE_REGISTER(scratch2);
+    UNRESERVE_REGISTER(scratch1);
+    UNRESERVE_REGISTER(buf_ptr);
+    drreg_unreserve_aflags(drcontext, ilist, where);
+}
+void
+instrument_mem_cache_no_memref(void *drcontext, instrlist_t *ilist, instr_t *where,
+                               uint *mem_ref_count)
+{
+    /* if we don't need details of the memory operation,
+     * call this function and skip the extra instrumentation */
+
+    if (instr_reads_memory(where))
+    {
+        for (int i=0; i<instr_num_srcs(where); ++i)
+        {
+            opnd_t ref = instr_get_src(where, i);
+            if (opnd_is_memory_reference(ref))
+            {
+                ++*mem_ref_count;
+            }
+        }
+    }
+
+    if (instr_writes_memory(where))
+    {
+        for (int i=0; i<instr_num_dsts(where); ++i)
+        {
+            opnd_t ref = instr_get_dst(where, i);
+            if (opnd_is_memory_reference(ref))
+            {
+                ++*mem_ref_count;
+            }
+        }
+    }
+}
+
+
+
+
+void
+instrument_comp_cache(instr_t *instr, uint *comp_count, SglCompEv *cache)
+{
+    /* TODO(soon) review these conditions */
+    dr_fp_type_t fp_t;
+    if(instr_is_floating_ex(instr, &fp_t) && (fp_t == DR_FP_MATH))
+    {
+        cache->type = SGLPRIM_COMP_FLOP;
+        ++*comp_count;
+    }
+    else
+    {
+        /* Brute force checking of opcode */
+        switch(instr_get_opcode(instr))
+        {
+        case OP_add:
+        case OP_xadd:
+        case OP_paddq:
+        case OP_adc:
+        case OP_sub:
+        case OP_sbb:
+        case OP_mul:
+        case OP_imul:
+        case OP_div:
+        case OP_idiv:
+        case OP_neg:
+        case OP_inc:
+        case OP_dec:
+        case OP_xor:
+        case OP_and:
+        case OP_or:
+        case OP_bt:
+        case OP_bts:
+        case OP_btr:
+        case OP_aas:
+            cache->type = SGLPRIM_COMP_IOP;
+            ++*comp_count;
+        default:
+            break;
+        }
+    }
+}
+
+static void
+setup_sgl_ev_buf_clean_call(void)
+{
+    set_shared_memory_buffer(drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx));
+}
+
+static void
+instrument_setup_buffer(void *drcontext, instrlist_t *ilist, instr_t *where,
+                        reg_id_t evptr_reg, reg_id_t scratch1, reg_id_t scratch2,
+                        uint size)
+{
+    /* Make sure enough space exists in the current shared memory event buffer */
+
+    /* scratch1 = events_ptr + size */
+    const size_t bytes = size * SIZEOF_EVENT_SLOT;
+    DR_ASSERT(bytes < INT_MAX);
+    MINSERT(ilist, where,
+            XINST_CREATE_add_2src(drcontext,
+                                  opnd_create_reg(scratch1),
+                                  opnd_create_reg(evptr_reg),
+                                  OPND_CREATE_INT32(bytes)));
+
+    /* Load (TLS) per_thread_t->buffer.events_end into scratch2 */
+    dr_insert_read_raw_tls(drcontext, ilist, where,
+                           raw_tls_seg, SGLEND_OFFS, scratch2);
+
+    /* if (events_ptr+size >= events_end)
+     *     perform clean call to reserve space in event buffer */
+    instr_t *end = INSTR_CREATE_label(drcontext);
+    MINSERT(ilist, where,
+            XINST_CREATE_cmp(drcontext,
+                             opnd_create_reg(scratch1),
+                             opnd_create_reg(scratch2)));
+    MINSERT(ilist, where,
+            INSTR_CREATE_jcc(drcontext,
+                             OP_jb_short,
+                             opnd_create_instr(end)));
+    dr_insert_clean_call(drcontext, ilist, where,
+                         (void *)setup_sgl_ev_buf_clean_call, false, 0);
+    /* reload evptr_reg, since it's been updated by the clean call */
+    dr_insert_read_raw_tls(drcontext, ilist, where,
+                           raw_tls_seg, SGLEV_OFFS, evptr_reg);
+
+    MINSERT(ilist, where, end);
+    /* update the events_used (events_ptr is updated as events are filled in)
+     * *(per_thread_t->buffer.events_used) += size */
+    dr_insert_read_raw_tls(drcontext, ilist, where,
+                           raw_tls_seg, SGLUSED_OFFS, scratch1);
+    MINSERT(ilist, where,
+            XINST_CREATE_add_s(drcontext,
+                               OPND_CREATE_MEMPTR(scratch1, 0),
+                               OPND_CREATE_INT32(size)));
+}
+
+
+static void
+instrument_mem(void *drcontext, instrlist_t *ilist, instr_t *where,
+               reg_id_t evptr_reg, reg_id_t memref_xax,
+               reg_id_t scratch_reg, reg_id_t h_scratch_reg, reg_id_t l_scratch_reg,
+               uint mem_used)
+{
+    for (uint i=0; i<mem_used; ++i)
+    {
+        /* Fill in event slots */
+        /* Set the tag
+         * ev->tag = SGL_MEM_TAG */
+        MINSERT(ilist, where,
+                XINST_CREATE_store_1byte(drcontext,
+                                         OPND_CREATE_MEM8(evptr_reg,
+                                                          offsetof(SglEvVariant, tag)),
+                                         OPND_CREATE_INT8(SGL_MEM_TAG)));
+
+        if (clo.enable_mem_type)
+        {
+            /* Set read/write
+             * ev->mem.type = memref->type */
+            MINSERT(ilist, where,
+                    XINST_CREATE_load_1byte_zext4(drcontext,
+                                                  opnd_create_reg(scratch_reg),
+                                                  OPND_CREATE_MEM8(memref_xax,
+                                                                   offsetof(mem_ref_t, type))));
+            MINSERT(ilist, where,
+                    XINST_CREATE_store_1byte(drcontext,
+                                             OPND_CREATE_MEM8(evptr_reg,
+                                                              offsetof(SglEvVariant, mem) +
+                                                              offsetof(SglMemEv, type)),
+                                             opnd_create_reg(l_scratch_reg)));
+        }
+
+        if (clo.enable_mem_size)
+        {
+            /* Set size
+             * ev->mem.size = memref->size */
+            MINSERT(ilist, where,
+                    XINST_CREATE_load_2bytes(drcontext,
+                                             opnd_create_reg(h_scratch_reg),
+                                             OPND_CREATE_MEM16(memref_xax,
+                                                               offsetof(mem_ref_t, size))));
+            MINSERT(ilist, where,
+                    XINST_CREATE_store_2bytes(drcontext,
+                                              OPND_CREATE_MEM16(evptr_reg,
+                                                                offsetof(SglEvVariant, mem) +
+                                                                offsetof(SglMemEv, size)),
+                                              opnd_create_reg(h_scratch_reg)));
+        }
+
+        if (clo.enable_mem_addr)
+        {
+            /* Set address
+             * ev->mem.begin_addr = memref->addr */
+            MINSERT(ilist, where,
+                    XINST_CREATE_load(drcontext,
+                                      opnd_create_reg(scratch_reg),
+                                      OPND_CREATE_MEMPTR(memref_xax,
+                                                         offsetof(mem_ref_t, addr))));
+            MINSERT(ilist, where,
+                    XINST_CREATE_store(drcontext,
+                                       OPND_CREATE_MEMPTR(evptr_reg,
+                                                          offsetof(SglEvVariant, mem) +
+                                                          offsetof(SglMemEv, begin_addr)),
+                                       opnd_create_reg(scratch_reg)));
+        }
+
+        /* Increment to the next event slot
+         * ev += 1 */
+        MINSERT(ilist, where,
+                XINST_CREATE_add(drcontext,
+                                 opnd_create_reg(evptr_reg),
+                                 OPND_CREATE_INT32(SIZEOF_EVENT_SLOT)));
+
+        /* Increment to next mem ref
+         * memref += 1 */
+        MINSERT(ilist, where,
+                XINST_CREATE_add(drcontext,
+                                 opnd_create_reg(memref_xax),
+                                 OPND_CREATE_INT32(sizeof(mem_ref_t))));
+    }
+}
+
+static void
+instrument_instr(void *drcontext, instrlist_t *ilist, instr_t *where,
+                 reg_id_t evptr_reg, reg_id_t scratch1,
+                 instr_t *instr)
+{
+    /* Store pc */
+    app_pc pc = instr_get_app_pc(instr);
+
+    /* For 64-bit, we can't use a 64-bit immediate so we split pc into two halves.
+     * We use a convenience routine that does the two-step store for us. */
+    opnd_t pc_opnd = opnd_create_reg(scratch1);
+    instr_t *first, *second;
+    instrlist_insert_mov_immed_ptrsz(drcontext, (ptr_int_t) pc, pc_opnd,
+                                     ilist, where, &first, &second);
+    instr_set_meta(first);
+    if (second != NULL)
+        instr_set_meta(second);
+
+    /* set the event slot attributes */
+    /* ev->tag = SGL_CXT_TAG */
+    MINSERT(ilist, where,
+            XINST_CREATE_store_1byte(drcontext,
+                                     OPND_CREATE_MEM8(evptr_reg, offsetof(SglEvVariant, tag)),
+                                     OPND_CREATE_INT8(SGL_CXT_TAG)));
+
+    /* CXT.TYPE
+     * ev->cxt.type = SGLPRIM_CXT_INSTR */
+    DR_ASSERT(sizeof(CxtType) == 1);
+    MINSERT(ilist, where,
+            XINST_CREATE_store(drcontext,
+                               OPND_CREATE_MEM8(evptr_reg,
+                                                (offsetof(SglEvVariant, cxt) +
+                                                 offsetof(SglCxtEv, type))),
+                               OPND_CREATE_INT8(SGLPRIM_CXT_INSTR)));
+
+    /* CXT.ID (instruction addr)
+     * ev->cxt.id = pc */
+    DR_ASSERT(sizeof(PtrVal) == 8);
+    MINSERT(ilist, where,
+            XINST_CREATE_store(drcontext,
+                               OPND_CREATE_MEMPTR(evptr_reg,
+                                                  (offsetof(SglEvVariant, cxt) +
+                                                   offsetof(SglCxtEv, id))),
+                               pc_opnd));
+
+    /* increment the sigil event slot
+     * ev += 1 */
+    MINSERT(ilist, where,
+            XINST_CREATE_add(drcontext,
+                             opnd_create_reg(evptr_reg),
+                             OPND_CREATE_INT32(SIZEOF_EVENT_SLOT)));
+}
+
+
+static void
+instrument_comp(void *drcontext, instrlist_t *ilist, instr_t *where,
+                reg_id_t evptr_reg, uint comp_used, SglCompEv **ev)
+{
+    for (uint i=0; i<comp_used; ++i)
+    {
+        /* Set the tag */
+        /* TAG
+         * ev->tag = SGL_COMP_TAG */
+        MINSERT(ilist, where,
+                XINST_CREATE_store_1byte(drcontext,
+                                         OPND_CREATE_MEM8(evptr_reg, offsetof(SglEvVariant, tag)),
+                                         OPND_CREATE_INT8(SGL_COMP_TAG)));
+
+        if (clo.enable_comp_type)
+        {
+            /* COMP.COMPCOSTTYPE
+             * ev->sync.type = compev->type */
+            DR_ASSERT(sizeof(CompCostType) == 1);
+            MINSERT(ilist, where,
+                    XINST_CREATE_store_1byte(drcontext,
+                                             OPND_CREATE_MEM8(evptr_reg,
+                                                              (offsetof(SglEvVariant, comp) +
+                                                               offsetof(SglCompEv, type))),
+                                             OPND_CREATE_INT8((*ev)->type)));
+        }
+
+        /* Increment to the next sigil event slot
+         * ev += 1 */
+        MINSERT(ilist, where,
+                XINST_CREATE_add(drcontext,
+                                 opnd_create_reg(evptr_reg),
+                                 OPND_CREATE_INT32(SIZEOF_EVENT_SLOT)));
+
+        /* Increment to the next cached compute event */
+        ++*ev;
+    }
+}
+
+static void
+instrument_sync(void *drcontext, instrlist_t *ilist, instr_t *where,
+                reg_id_t evptr_reg, reg_id_t sglsyncptr_reg,
+                reg_id_t scratch_reg, reg_id_t l_scratch_reg)
+{
+    /* TAG
+     * ev->tag = SGL_SYNC_TAG */
+    MINSERT(ilist, where,
+            XINST_CREATE_store_1byte(drcontext,
+                                     OPND_CREATE_MEM8(evptr_reg, offsetof(SglEvVariant, tag)),
+                                     OPND_CREATE_INT8(SGL_SYNC_TAG)));
+
+    if (clo.enable_sync_type)
+    {
+        DR_ASSERT(sizeof(SyncType) == 1);
+        /* SYNC.SYNCTYPE
+         * ev->sync.type = TLS syncev->type */
+        MINSERT(ilist, where,
+                XINST_CREATE_load_1byte_zext4(drcontext,
+                                              opnd_create_reg(scratch_reg),
+                                              OPND_CREATE_MEM8(sglsyncptr_reg,
+                                                               offsetof(SglSyncEv, type))));
+        MINSERT(ilist, where,
+                XINST_CREATE_store_1byte(drcontext,
+                                         OPND_CREATE_MEM8(evptr_reg,
+                                                          (offsetof(SglEvVariant, sync) +
+                                                           offsetof(SglSyncEv, type))),
+                                         opnd_create_reg(l_scratch_reg)));
+    }
+
+    if (clo.enable_sync_data)
+    {
+        /* SYNC.SYNCID[0]
+         * ev->sync.data[0] = TLS syncev->data[0] */
+        DR_ASSERT(sizeof(SyncID) == 8);
+        MINSERT(ilist, where,
+                XINST_CREATE_load(drcontext,
+                                  opnd_create_reg(scratch_reg),
+                                  OPND_CREATE_MEMPTR(sglsyncptr_reg,
+                                                     offsetof(SglSyncEv, data))));
+        MINSERT(ilist, where,
+                XINST_CREATE_store(drcontext,
+                                   OPND_CREATE_MEMPTR(evptr_reg,
+                                                      (offsetof(SglEvVariant, sync) +
+                                                       offsetof(SglSyncEv, data))),
+                                   opnd_create_reg(scratch_reg)));
+        /* SYNC.SYNCID[1]
+         * ev->sync.data[1] = TLS syncev->data[1] */
+        MINSERT(ilist, where,
+                XINST_CREATE_load(drcontext,
+                                  opnd_create_reg(scratch_reg),
+                                  OPND_CREATE_MEMPTR(sglsyncptr_reg,
+                                                     offsetof(SglSyncEv, data) +
+                                                     1*sizeof(SyncID))));
+        MINSERT(ilist, where,
+                XINST_CREATE_store(drcontext,
+                                   OPND_CREATE_MEMPTR(evptr_reg,
+                                                      (offsetof(SglEvVariant, sync) +
+                                                       offsetof(SglSyncEv, data) +
+                                                       1*sizeof(SyncID))),
+                                   opnd_create_reg(scratch_reg)));
+    }
+
+    /* reset the sync event
+     * TLS syncev = (SglSyncEv*)0 */
+    MINSERT(ilist, where,
+            XINST_CREATE_load_int(drcontext,
+                                  opnd_create_reg(scratch_reg),
+                                  OPND_CREATE_INTPTR(NULL)));
+    dr_insert_write_raw_tls(drcontext, ilist, where,
+                            raw_tls_seg, SGLSYNCEV_OFFS, scratch_reg);
+
+    /* Increment to the next event slot
+     * ev += 1 */
+    MINSERT(ilist, where,
+            XINST_CREATE_add(drcontext,
+                             opnd_create_reg(evptr_reg),
+                             OPND_CREATE_INT32(SIZEOF_EVENT_SLOT)));
+}
+
+
+void
+instrument_sigil_events(void *drcontext, instrlist_t *ilist, instr_t *where,
+                        per_thread_t *tcxt)
+{
+    /* need to specify so we can access 1/2 byte registers.
+     * e.g. XAX so we can access AL / AX
+     * cannot use drreg here (no API for this) */
+    reg_id_t xax, xbx, xcx, evptr_reg;
+    dr_spill_slot_t spill_xax, spill_xbx, spill_xcx, spill_evptr, spill_aflags;
+
+    spill_aflags = SPILL_SLOT_10;
+    dr_save_arith_flags(drcontext, ilist, where, spill_aflags);
+
+    xax = DR_REG_XAX;
+    spill_xax = SPILL_SLOT_11;
+    dr_save_reg(drcontext, ilist, where, xax, spill_xax);
+
+    xbx = DR_REG_XBX;
+    spill_xbx = SPILL_SLOT_12;
+    dr_save_reg(drcontext, ilist, where, xbx, spill_xbx);
+
+    xcx = DR_REG_XCX;
+    spill_xcx = SPILL_SLOT_13;
+    dr_save_reg(drcontext, ilist, where, xcx, spill_xcx);
+
+    evptr_reg = DR_REG_XDX;
+    spill_evptr = SPILL_SLOT_14;
+    dr_save_reg(drcontext, ilist, where, evptr_reg, spill_evptr);
+
+    //-----------------------------------------------------------
+    /* skip over instrumented event generation, if not enabled,
+     * e.g. inside a pthread event.
+     * This is always required since a thread about to block
+     * needs to be INACTIVE, so it doesn't try to acqurie any
+     * DR locks */
+
+    /* skip sigil events if inactive */
+    instr_t *skip_sigil = INSTR_CREATE_label(drcontext);
+
+    /* load per_thread_t->active from TLS into reg1 */
+    /* test if active, can't do a (jecxz) near jump */
+    dr_insert_read_raw_tls(drcontext, ilist, where,
+                           raw_tls_seg, ACTIVE_OFFS, xax);
+    MINSERT(ilist, where,
+            XINST_CREATE_cmp(drcontext,
+                             opnd_create_reg(xax),
+                             OPND_CREATE_INT8(false)));
+    MINSERT(ilist, where,
+            INSTR_CREATE_jcc(drcontext,
+                             OP_je,
+                             opnd_create_instr(skip_sigil)));
+
+    /* Load (TLS) per_thread_t->buffer.events_ptr into evptr_reg
+     * this is used and updated by all instrumentation to write event attr */
+    dr_insert_read_raw_tls(drcontext, ilist, where,
+                           raw_tls_seg, SGLEV_OFFS, evptr_reg);
+    //-----------------------------------------------------------
+
+    /* skip if event buffer already setup*/
+    instr_t *skip_setup = INSTR_CREATE_label(drcontext);
+
+    if (clo.enable_sync)
+    {
+        /* Check if we've just exited a synchronization event
+         * Skip if no synch event, can't do a (jecxz) near jump */
+        dr_insert_read_raw_tls(drcontext, ilist, where,
+                               raw_tls_seg, SGLSYNCEV_OFFS, xax);
+        /* if (TLS syncevptr == NULL)
+         * can't do a 64 bit cmp to NULL, need a reg :( */
+        MINSERT(ilist, where,
+                XINST_CREATE_load_int(drcontext,
+                                      opnd_create_reg(xbx),
+                                      OPND_CREATE_INTPTR(NULL)));
+        MINSERT(ilist, where,
+                XINST_CREATE_cmp(drcontext,
+                                 opnd_create_reg(xax),
+                                 opnd_create_reg(xbx)));
+        /* jmp skip_sync */
+        instr_t *skip_sync = INSTR_CREATE_label(drcontext);
+        MINSERT(ilist, where,
+                INSTR_CREATE_jcc(drcontext,
+                                 OP_je,
+                                 opnd_create_instr(skip_sync)));
+
+        /* if (TLS syncevptr != NULL) */
+        instrument_setup_buffer(drcontext, ilist, where,
+                                evptr_reg, xbx, xcx,
+                                tcxt->event_block_events + 1);
+        instrument_sync(drcontext, ilist, where, evptr_reg, xax, xbx, DR_REG_BL);
+
+        MINSERT(ilist, where,
+                XINST_CREATE_jump(drcontext,
+                                  opnd_create_instr(skip_setup)));
+
+        /* SKIP SYNC LABEL */
+        MINSERT(ilist, where, skip_sync);
+    }
+
+    /* add sigil event instrumentation */
+    /* TODO check if we hang here when there's no instrumentation
+     * enabled */
+    instrument_setup_buffer(drcontext, ilist, where,
+                            evptr_reg, xbx, xcx,
+                            tcxt->event_block_events);
+
+    if (clo.enable_sync)
+    {
+        /* Only needed if synchronization events enabled */
+        /* SKIP SETUP BUFFER LABEL */
+        MINSERT(ilist, where, skip_setup);
+    }
+
+    /* sigil mem/compute events are filled in from buffer[0] to buffer[N] */
+    SglCompEv *comp_ev = tcxt->comps;
+    SglMemEv *mem_ev = tcxt->mems;
+
+    if (clo.memref_needed)
+    {
+        /* xax <= TLS memrefbase */
+        dr_insert_read_raw_tls(drcontext, ilist, where,
+                               raw_tls_seg, MEMREFBASE_OFFS, xax);
+    }
+
+    for (uint i=0; i<tcxt->iblock_count; ++i)
+    {
+        /* Iterate over all potential events.
+         * There may be multiple memory and compute events per instruction.
+         * Upon each instrumentation, the memory/compute cache may be
+         * incremented multiple times as each event is written to sigil shared
+         * memory. This means both the cache pointers for both need to be
+         * persistent */
+        instr_block_t *iblock = tcxt->iblocks + i;
+        if (clo.enable_context_instr)
+            instrument_instr(drcontext, ilist, where,
+                             evptr_reg, xcx, iblock->instr);
+        if (clo.enable_mem)
+            instrument_mem(drcontext, ilist, where,
+                           evptr_reg, xax, xcx, DR_REG_CX, DR_REG_CL,
+                           iblock->mem_ref_count);
+        if (clo.enable_comp)
+            instrument_comp(drcontext, ilist, where,
+                            evptr_reg, iblock->comp_count, &comp_ev);
+    }
+
+    //-----------------------------------------------------------
+    /* Finally, update (TLS) per_thread_t->buffer.events_ptr */
+    dr_insert_write_raw_tls(drcontext, ilist, where,
+                            raw_tls_seg, SGLEV_OFFS, evptr_reg);
+    //-----------------------------------------------------------
+
+    MINSERT(ilist, where, skip_sigil);
+
+    dr_restore_reg(drcontext, ilist, where, evptr_reg, spill_evptr);
+    dr_restore_reg(drcontext, ilist, where, xcx, spill_xcx);
+    dr_restore_reg(drcontext, ilist, where, xbx, spill_xbx);
+    dr_restore_reg(drcontext, ilist, where, xax, spill_xax);
+    dr_restore_arith_flags(drcontext, ilist, where, spill_aflags);
+}
diff --git a/clients/drsigil/ipc.c b/clients/drsigil/ipc.c
new file mode 100644
index 0000000..ca9a8a5
--- /dev/null
+++ b/clients/drsigil/ipc.c
@@ -0,0 +1,376 @@
+#include "drsigil.h"
+#include <string.h>
+#include <time.h>
+#include <limits.h>
+#include <unistd.h>
+#include <sys/syscall.h>
+#include <sys/types.h>
+
+#define STRINGIFY(x) #x
+
+#define MAX_IPC_CHANNELS 256 //fudge number
+ipc_channel_t IPC[MAX_IPC_CHANNELS];
+/* Initialize all possible IPC channels (some will not be used) */
+
+static inline void
+notify_full_buffer(ipc_channel_t *channel)
+{
+    /* Tell Sigil2 that the active buffer, on the given IPC channel,
+     * is full and ready to be consumed */
+
+    if (channel->standalone == false)
+        dr_write_file(channel->full_fifo,
+                      &channel->shmem_buf_idx, sizeof(channel->shmem_buf_idx));
+
+    /* Flag the shared memory buffer as used (by Sigil2) */
+    channel->empty_buf_idx[channel->shmem_buf_idx] = false;
+}
+
+
+static inline EventBuffer*
+get_next_buffer(ipc_channel_t *channel)
+{
+    /* Increment to the next buffer and try to acquire it for writing */
+
+    /* Circular buffer, must be power of 2 */
+    channel->shmem_buf_idx = (channel->shmem_buf_idx+1) & (SIGIL2_IPC_BUFFERS-1);
+
+    /* Sigil2 tells us when it's finished with a shared memory buffer */
+    if(channel->empty_buf_idx[channel->shmem_buf_idx] == false)
+    {
+        if (channel->standalone == false)
+            dr_read_file(channel->empty_fifo,
+                         &channel->shmem_buf_idx, sizeof(channel->shmem_buf_idx));
+        channel->empty_buf_idx[channel->shmem_buf_idx] = true;
+    }
+
+    channel->shared_mem->eventBuffers[channel->shmem_buf_idx].used = 0;
+    channel->shared_mem->nameBuffers[channel->shmem_buf_idx].used = 0;
+    return channel->shared_mem->eventBuffers + channel->shmem_buf_idx;
+}
+
+
+static inline EventBuffer*
+get_buffer(ipc_channel_t *channel, uint required)
+{
+    /* Check if enough space is available in the current buffer */
+    EventBuffer *current_shmem_buffer = channel->shared_mem->eventBuffers +
+                                        channel->shmem_buf_idx;
+    uint available = SIGIL2_EVENTS_BUFFER_SIZE - current_shmem_buffer->used;
+
+    if(available < required)
+    {
+        /* First inform Sigil2 the current buffer can be read */
+        notify_full_buffer(channel);
+
+        /* Then get a new buffer for writing */
+        current_shmem_buffer = get_next_buffer(channel);
+    }
+
+    return current_shmem_buffer;
+}
+
+
+static inline void
+ordered_lock(ipc_channel_t *channel, uint tid)
+{
+    dr_mutex_lock(channel->queue_lock);
+
+    ticket_queue_t *q = &channel->ticket_queue;
+    if (q->locked)
+    {
+        ticket_node_t *node = dr_global_alloc(sizeof(ticket_node_t));
+        if (node == NULL)
+            DR_ABORT_MSG("Failed to allocate ticket node\n");
+        node->next = NULL;
+        node->dr_event = dr_event_create();
+        node->waiting = true;
+        node->thread_id = tid;
+
+        DR_ASSERT(q->tail->next == NULL);
+        q->tail = q->tail->next = node;
+
+        SGL_DEBUG("Sleeping Thread :%d\n", tid);
+        dr_mutex_unlock(channel->queue_lock);
+
+        /* MDL20170425 TODO(soonish)
+         * how likely is it that we'll miss a wakeup here? */
+        while (node->waiting)
+            dr_event_wait(node->dr_event);
+
+        dr_mutex_lock(channel->queue_lock);
+
+        q->head->next = node->next;
+        if (q->tail == node)
+            q->tail = q->head;
+
+        dr_event_destroy(node->dr_event);
+        dr_global_free(node, sizeof(ticket_node_t));
+
+        SGL_DEBUG("Awakened Thread :%d\n", tid);
+    }
+    else
+    {
+        q->locked = true;
+    }
+
+    dr_mutex_unlock(channel->queue_lock);
+}
+
+
+static inline void
+ordered_unlock(ipc_channel_t *channel)
+{
+    /* Calling thread MUST be the ticket lock owner */
+
+    dr_mutex_lock(channel->queue_lock);
+
+    ticket_queue_t *q = &channel->ticket_queue;
+    DR_ASSERT(q->locked);
+
+    if (q->head == q->tail)
+    {
+        q->locked = false;
+    }
+    else
+    {
+        q->head->next->waiting = false;
+        dr_event_signal(q->head->next->dr_event);
+    }
+
+    dr_mutex_unlock(channel->queue_lock);
+}
+
+
+static inline ipc_channel_t*
+get_locked_channel(per_thread_t *tcxt)
+{
+    /* Calculate the channel index.
+     * Each native dynamoRIO thread will write to a runtime-determined
+     * shared memory buffer, in order to reduce the amount of contention
+     * for sending data to Sigil2. However, more buffers from dynamoRIO
+     * means more load (threads) on Sigil2. Therefore, the total number
+     * of buffers from DynamoRIO -> Sigil2 is a command line variable. */
+    uint channel_idx = tcxt->thread_id % clo.frontend_threads;
+    ipc_channel_t *channel = &IPC[channel_idx];
+
+    /* Requeue self to lock channel,
+     * to avoid starvation of other threads */
+    if(tcxt->has_channel_lock)
+        ordered_unlock(channel);
+
+    /* Lock the shared memory channel */
+    ordered_lock(channel, tcxt->thread_id);
+    tcxt->has_channel_lock = true;
+
+    return channel;
+}
+
+
+static inline void
+set_shared_memory_buffer_helper(per_thread_t *tcxt, ipc_channel_t *channel)
+{
+    EventBuffer *current_shmem_buffer = get_buffer(channel, MIN_DR_PER_THREAD_BUFFER_EVENTS);
+    SglEvVariant *current_event = current_shmem_buffer->events + current_shmem_buffer->used;
+    SGLEV_PTR(tcxt->seg_base) = current_event;
+    SGLEND_PTR(tcxt->seg_base) = current_event + MIN_DR_PER_THREAD_BUFFER_EVENTS;
+    SGLUSED_PTR(tcxt->seg_base) = &(current_shmem_buffer->used);
+}
+
+
+/////////////////////////////////////////////////////////////////////
+// IPC interface
+/////////////////////////////////////////////////////////////////////
+void set_shared_memory_buffer(per_thread_t *tcxt)
+{
+    ipc_channel_t *channel = get_locked_channel(tcxt);
+    set_shared_memory_buffer_helper(tcxt, channel);
+
+    if(channel->last_active_tid != tcxt->thread_id)
+    {
+        /* Write thread swap event */
+        SglSyncEv ev = {
+            .type    = SGLPRIM_SYNC_SWAP,
+            .data[0] = tcxt->thread_id
+        };
+        SglEvVariant *slot = SGLEV_PTR(tcxt->seg_base);
+        slot->tag = SGL_SYNC_TAG;
+        slot->sync = ev;
+        ++(SGLEV_PTR(tcxt->seg_base));
+        ++*(SGLUSED_PTR(tcxt->seg_base));
+    }
+
+    channel->last_active_tid = tcxt->thread_id;
+}
+
+void force_thread_flush(per_thread_t *tcxt)
+{
+    if(tcxt->has_channel_lock)
+    {
+        uint channel_idx = tcxt->thread_id % clo.frontend_threads;
+        ipc_channel_t *channel = &IPC[channel_idx];
+        notify_full_buffer(channel);
+        get_next_buffer(channel);
+        ordered_unlock(channel);
+        tcxt->has_channel_lock = false;
+        SGLEV_PTR(tcxt->seg_base) = NULL;
+        SGLEND_PTR(tcxt->seg_base) = NULL;
+        SGLUSED_PTR(tcxt->seg_base) = NULL;
+    }
+}
+
+static file_t
+open_sigil2_fifo(const char *path, int flags)
+{
+    /* Wait for Sigil2 to create pipes
+     * Timeout is empirical */
+    uint max_tests = 10;
+    for(uint i=0; i<max_tests+1; ++i)
+    {
+        if(dr_file_exists(path))
+            break;
+
+        if(i == max_tests)
+        {
+            dr_printf("%s\n", path);
+            DR_ASSERT_MSG(false, "DrSigil timed out waiting for sigil2 fifos");
+        }
+
+        struct timespec ts;
+        ts.tv_sec  = 0;
+        ts.tv_nsec = 200000000L;
+        nanosleep(&ts, NULL);
+    }
+
+    file_t f = dr_open_file(path, flags);
+    if(f == INVALID_FILE)
+        DR_ABORT_MSG("error opening empty fifo");
+
+    return f;
+}
+
+void
+init_IPC(int idx, const char *path, bool standalone)
+{
+    DR_ASSERT(idx < MAX_IPC_CHANNELS);
+
+    int path_len, pad_len, shmem_len, fullfifo_len, emptyfifo_len;
+    ipc_channel_t *channel = &IPC[idx];
+
+    channel->standalone = standalone;
+
+    /* Initialize channel state */
+    ticket_node_t *node = dr_global_alloc(sizeof(ticket_node_t));
+    if (node == NULL)
+        DR_ABORT_MSG("Failed to allocate ticket node\n");
+    node->next = NULL;
+    node->dr_event = NULL;
+    node->waiting = false;
+    node->thread_id = 0;
+    channel->ticket_queue.head = node;
+    channel->ticket_queue.tail = node;
+    channel->ticket_queue.locked = false;
+    channel->queue_lock = dr_mutex_create();
+
+    channel->shared_mem = NULL;
+    channel->full_fifo = -1;
+    channel->empty_fifo = -1;
+    channel->shmem_buf_idx = 0;
+
+    for(uint i=0; i<sizeof(channel->empty_buf_idx)/sizeof(channel->empty_buf_idx[0]); ++i)
+        channel->empty_buf_idx[i] = true;
+
+    channel->last_active_tid = 0;
+    channel->initialized     = false;
+
+    if (standalone)
+    {
+        /* mimic shared memory writes */
+        channel->shared_mem = dr_raw_mem_alloc(sizeof(Sigil2DBISharedData),
+                                               DR_MEMPROT_READ | DR_MEMPROT_WRITE,
+                                               NULL);
+        if (channel->shared_mem == NULL)
+            DR_ABORT_MSG("Failed to allocate pseudo shared memory buffer\n");
+        for (int i=0; i<SIGIL2_IPC_BUFFERS; ++i)
+            channel->shared_mem->eventBuffers[i].used = 0;
+    }
+    else
+    {
+        /* Connect to Sigil2 */
+        path_len = strlen(path);
+        pad_len = 4; /* extra space for '/', 2x'-', '\0' */
+        shmem_len     = (path_len + pad_len +
+                         sizeof(SIGIL2_IPC_SHMEM_BASENAME) +
+                         sizeof(STRINGIFY(MAX_IPC_CHANNELS)));
+        fullfifo_len  = (path_len + pad_len +
+                         sizeof(SIGIL2_IPC_FULLFIFO_BASENAME) +
+                         sizeof(STRINGIFY(MAX_IPC_CHANNELS)));
+        emptyfifo_len = (path_len + pad_len +
+                         sizeof(SIGIL2_IPC_EMPTYFIFO_BASENAME) +
+                         sizeof(STRINGIFY(MAX_IPC_CHANNELS)));
+
+        /* set up names of IPC files */
+        char shmem_name[shmem_len];
+        sprintf(shmem_name, "%s/%s-%d", path, SIGIL2_IPC_SHMEM_BASENAME, idx);
+
+        char fullfifo_name[fullfifo_len];
+        sprintf(fullfifo_name, "%s/%s-%d", path, SIGIL2_IPC_FULLFIFO_BASENAME, idx);
+
+        char emptyfifo_name[emptyfifo_len];
+        sprintf(emptyfifo_name, "%s/%s-%d", path, SIGIL2_IPC_EMPTYFIFO_BASENAME, idx);
+
+
+        /* initialize read/write pipes */
+        channel->empty_fifo = open_sigil2_fifo(emptyfifo_name, DR_FILE_READ);
+        channel->full_fifo = open_sigil2_fifo(fullfifo_name, DR_FILE_WRITE_ONLY);
+
+        /* no need to timeout on file because shared memory MUST be initialized
+         * by Sigil2 before the fifos are created */
+        file_t map_file = dr_open_file(shmem_name, DR_FILE_READ|DR_FILE_WRITE_APPEND);
+        if(map_file == INVALID_FILE)
+            DR_ABORT_MSG("error opening shared memory file");
+
+        size_t mapped_size = sizeof(Sigil2DBISharedData);
+        channel->shared_mem = dr_map_file(map_file, &mapped_size,
+                                          0, 0, /* assume this is not honored */
+                                          DR_MEMPROT_READ|DR_MEMPROT_WRITE, 0);
+
+        if(mapped_size != sizeof(Sigil2DBISharedData) || channel->shared_mem == NULL)
+            DR_ABORT_MSG("error mapping shared memory");
+
+        dr_close_file(map_file);
+    }
+
+    channel->initialized = true;
+}
+
+
+void
+terminate_IPC(int idx)
+{
+    ipc_channel_t *channel = &IPC[idx];
+
+    if (channel->standalone)
+    {
+        dr_raw_mem_free(channel->shared_mem, sizeof(Sigil2DBISharedData));
+    }
+    else
+    {
+        /* send terminate sequence */
+        uint finished = SIGIL2_IPC_FINISHED;
+        uint last_buffer = channel->shmem_buf_idx;
+        if(dr_write_file(channel->full_fifo, &last_buffer, sizeof(last_buffer)) != sizeof(last_buffer) ||
+           dr_write_file(channel->full_fifo, &finished,    sizeof(finished))    != sizeof(finished))
+            DR_ABORT_MSG("error writing finish sequence sigil2 fifos");
+
+        /* wait for sigil2 to disconnect */
+        while(dr_read_file(channel->empty_fifo, &finished, sizeof(finished)) > 0);
+
+        dr_close_file(channel->empty_fifo);
+        dr_close_file(channel->full_fifo);
+        dr_unmap_file(channel->shared_mem, sizeof(Sigil2DBISharedData));
+    }
+
+    dr_global_free((void*)channel->ticket_queue.head, sizeof(ticket_queue_t));
+    dr_mutex_destroy(channel->queue_lock);
+}
diff --git a/clients/drsigil/parser.c b/clients/drsigil/parser.c
new file mode 100644
index 0000000..0ccad94
--- /dev/null
+++ b/clients/drsigil/parser.c
@@ -0,0 +1,90 @@
+#include <string.h>
+#include <getopt.h>
+#include "drsigil.h"
+
+static struct option long_options[] =
+{
+    {"standalone",           no_argument,       0, 's'},
+    {"num-frontend-threads", required_argument, 0, 'n'},
+    {"ipc-dir",              required_argument, 0, 'd'},
+    {"start-func",           required_argument, 0, 'b'},
+    {"stop-func",            required_argument, 0, 'e'},
+    {"enable-mem",           no_argument, &clo.enable_mem,           1},
+    {"enable-mem-type",      no_argument, &clo.enable_mem_type,      1},
+    {"enable-mem-addr",      no_argument, &clo.enable_mem_addr,      1},
+    {"enable-mem-size",      no_argument, &clo.enable_mem_size,      1},
+    {"enable-comp",          no_argument, &clo.enable_comp,          1},
+    {"enable-comp-type",     no_argument, &clo.enable_comp_type,     1},
+    {"enable-sync",          no_argument, &clo.enable_sync,          1},
+    {"enable-sync-type",     no_argument, &clo.enable_sync_type,     1},
+    {"enable-sync-data",     no_argument, &clo.enable_sync_data,     1},
+    {"enable-context-instr", no_argument, &clo.enable_context_instr, 1},
+    {0, 0, 0, 0},
+};
+
+void
+parse(int argc, char *argv[])
+{
+    /* init args */
+    int c = 0;
+    int option_index = 0;
+
+    /* defaults */
+    clo.ipc_dir          = NULL;
+    clo.start_func       = NULL;
+    clo.stop_func        = NULL;
+    clo.frontend_threads = 0;
+    clo.standalone       = false;
+
+    clo.enable_mem           = false;
+    clo.enable_mem_type      = false;
+    clo.enable_mem_addr      = false;
+    clo.enable_mem_size      = false;
+    clo.enable_comp          = false;
+    clo.enable_comp_type     = false;
+    clo.enable_sync          = false;
+    clo.enable_sync_type     = false;
+    clo.enable_sync_data     = false;
+    clo.enable_context_instr = false;
+
+    clo.memref_needed = false;
+
+    while( (c = getopt_long(argc, argv, "sn:d:b:e:", long_options, &option_index)) >= 0 )
+    {
+        switch(c)
+        {
+        case 'n':
+            clo.frontend_threads = atoi(optarg);
+            break;
+        case 'd':
+            clo.ipc_dir = optarg;
+            break;
+        case 'b':
+            clo.start_func = optarg;
+            break;
+        case 'e':
+            clo.stop_func = optarg;
+            break;
+        case 's':
+            clo.standalone = true;
+            break;
+        default:
+            break;
+        }
+    }
+
+    if (clo.frontend_threads <= 0)
+        DR_ABORT_MSG("Invalid number of frontend threads specified");
+
+	if (clo.standalone == false && clo.ipc_dir == NULL)
+        DR_ABORT_MSG("IPC directory must be specified");
+        /* okay if in standalone mode when ipc dir unused */
+
+    if (clo.start_func != NULL)
+        roi = false;
+
+    clo.memref_needed = clo.enable_mem_type || clo.enable_mem_addr || clo.enable_mem_size;
+
+    /* TODO(soon) sanity check on enables
+     * (e.g. mem must be enabled if mem_type is enabled) */
+}
diff --git a/clients/drsigil/pthread_defines.h b/clients/drsigil/pthread_defines.h
new file mode 100644
index 0000000..dc69823
--- /dev/null
+++ b/clients/drsigil/pthread_defines.h
@@ -0,0 +1,283 @@
+#ifndef PTHREAD_DEFINES_H
+#define PTHREAD_DEFINES_H
+
+#include "drsigil.h"
+#include "drmgr.h"
+#include "drwrap.h"
+#include "pthread.h"
+
+#define MAIN           "main"
+#define P_CREATE       "pthread_create"
+#define P_JOIN         "pthread_join"
+#define P_MUTEX_LOCK   "pthread_mutex_lock"
+#define P_MUTEX_UNLOCK "pthread_mutex_unlock"
+#define P_BARRIER      "pthread_barrier_wait"
+#define P_COND_WAIT    "pthread_cond_wait"
+#define P_COND_SIG     "pthread_cond_signal"
+#define P_SPIN_LOCK    "pthread_spin_lock"
+#define P_SPIN_UNLOCK  "pthread_spin_unlock"
+
+#ifdef SGLDEBUG
+#define LOG_SYNC_ENTER(sync_event, thread_id) \
+    dr_printf("entering "#sync_event": %d\n", thread_id)
+#define LOG_SYNC_EXIT(sync_event, thread_id) \
+    dr_printf("exiting  "#sync_event": %d\n", thread_id)
+#else
+#define LOG_SYNC_ENTER(sync_event, thread_id)
+#define LOG_SYNC_EXIT(sync_event, thread_id)
+#endif
+
+static inline void
+set_sync_event(per_thread_t *tcxt, SyncType type, SyncID data[const static 2])
+{
+    tcxt->sync_ev->type = type;
+    tcxt->sync_ev->data[0] = data[0];
+    tcxt->sync_ev->data[1] = data[1];
+}
+static inline void
+send_sync_event(per_thread_t *tcxt)
+{
+    /* sets the raw TLS sync event pointer,
+     * letting instrumentation know to send the
+     * event to sigil */
+    SGLSYNCEV_PTR(tcxt->seg_base) = tcxt->sync_ev;
+}
+
+static inline void
+set_blocked_and_deactivate(per_thread_t *tcxt)
+{
+    ACTIVE(tcxt->seg_base) = false;
+    tcxt->is_blocked = true;
+    force_thread_flush(tcxt);
+}
+static inline void
+set_unblocked_and_reactivate(per_thread_t *tcxt)
+{
+    ACTIVE(tcxt->seg_base) = true;
+    tcxt->is_blocked = false;
+}
+static inline void
+deactivate(per_thread_t *tcxt)
+{
+    ACTIVE(tcxt->seg_base) = false;
+}
+static inline void
+reactivate(per_thread_t *tcxt)
+{
+    ACTIVE(tcxt->seg_base) = true;
+}
+
+////////////////////////////////////////////
+// PTHREAD CREATE
+////////////////////////////////////////////
+static void
+wrap_pre_pthread_create(void *wrapcxt, OUT void **user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_ENTER(pthread_create, tcxt->thread_id);
+    deactivate(tcxt);
+
+    /* TODO dereference the pthread_t? */
+    SyncID data[2] = {(uintptr_t)(pthread_t*)drwrap_get_arg(wrapcxt, 0), 0};
+    set_sync_event(tcxt, SGLPRIM_SYNC_CREATE, data);
+}
+static void
+wrap_post_pthread_create(void *wrapcxt, void *user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_EXIT(pthread_create, tcxt->thread_id);
+    reactivate(tcxt);
+
+    send_sync_event(tcxt);
+}
+
+
+////////////////////////////////////////////
+// PTHREAD JOIN
+////////////////////////////////////////////
+static void
+wrap_pre_pthread_join(void *wrapcxt, OUT void **user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_ENTER(pthread_join, tcxt->thread_id);
+    set_blocked_and_deactivate(tcxt);
+
+    SyncID data[2] = {(uintptr_t)(pthread_t*)drwrap_get_arg(wrapcxt, 0), 0};
+    set_sync_event(tcxt, SGLPRIM_SYNC_JOIN, data);
+}
+static void
+wrap_post_pthread_join(void *wrapcxt, void *user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_EXIT(pthread_join, tcxt->thread_id);
+    set_unblocked_and_reactivate(tcxt);
+    send_sync_event(tcxt);
+}
+
+
+////////////////////////////////////////////
+// PTHREAD MUTEX LOCK
+////////////////////////////////////////////
+static void
+wrap_pre_pthread_mutex_lock(void *wrapcxt, OUT void **user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(),
+                                             tls_idx);
+    LOG_SYNC_ENTER(pthread_mutex_lock, tcxt->thread_id);
+    set_blocked_and_deactivate(tcxt);
+
+    SyncID data[2] = {(uintptr_t)(pthread_mutex_t*)drwrap_get_arg(wrapcxt, 0), 0};
+    set_sync_event(tcxt, SGLPRIM_SYNC_LOCK, data);
+}
+static void
+wrap_post_pthread_mutex_lock(void *wrapcxt, void *user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_EXIT(pthread_mutex_lock, tcxt->thread_id);
+    set_unblocked_and_reactivate(tcxt);
+
+    send_sync_event(tcxt);
+}
+
+
+////////////////////////////////////////////
+// PTHREAD MUTEX UNLOCK
+////////////////////////////////////////////
+static void
+wrap_pre_pthread_mutex_unlock(void *wrapcxt, OUT void **user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_ENTER(pthread_mutex_unlock, tcxt->thread_id);
+    deactivate(tcxt);
+
+    SyncID data[2] = {(uintptr_t)(pthread_mutex_t*)drwrap_get_arg(wrapcxt, 0), 0};
+    set_sync_event(tcxt, SGLPRIM_SYNC_UNLOCK, data);
+}
+static void
+wrap_post_pthread_mutex_unlock(void *wrapcxt, void *user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_EXIT(pthread_mutex_unlock, tcxt->thread_id);
+    reactivate(tcxt);
+
+    send_sync_event(tcxt);
+}
+
+
+////////////////////////////////////////////
+// PTHREAD BARRIER
+////////////////////////////////////////////
+static void
+wrap_pre_pthread_barrier(void *wrapcxt, OUT void **user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_ENTER(pthread_barrier, tcxt->thread_id);
+    set_blocked_and_deactivate(tcxt);
+
+    SyncID data[2] = {(uintptr_t)(pthread_barrier_t*)drwrap_get_arg(wrapcxt, 0), 0};
+    set_sync_event(tcxt, SGLPRIM_SYNC_BARRIER, data);
+}
+static void
+wrap_post_pthread_barrier(void *wrapcxt, void *user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_EXIT(pthread_barrier, tcxt->thread_id);
+    set_unblocked_and_reactivate(tcxt);
+
+    send_sync_event(tcxt);
+}
+
+
+////////////////////////////////////////////
+// PTHREAD CONDITIONAL WAIT
+////////////////////////////////////////////
+static void
+wrap_pre_pthread_cond_wait(void *wrapcxt, OUT void **user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_ENTER(pthread_cond_wait, tcxt->thread_id);
+    set_blocked_and_deactivate(tcxt);
+
+    SyncID data[2] = {(uintptr_t)(pthread_cond_t*)drwrap_get_arg(wrapcxt, 0),
+                      (uintptr_t)(pthread_mutex_t*)drwrap_get_arg(wrapcxt, 1)};
+    set_sync_event(tcxt, SGLPRIM_SYNC_CONDWAIT, data);
+}
+static void
+wrap_post_pthread_cond_wait(void *wrapcxt, void *user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_EXIT(pthread_cond_wait, tcxt->thread_id);
+    set_unblocked_and_reactivate(tcxt);
+
+    send_sync_event(tcxt);
+}
+
+
+////////////////////////////////////////////
+// PTHREAD CONDITIONAL SIGNAL
+////////////////////////////////////////////
+static void
+wrap_pre_pthread_cond_sig(void *wrapcxt, OUT void **user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_ENTER(pthread_cond_sig, tcxt->thread_id);
+    deactivate(tcxt);
+
+    SyncID data[2] = {(uintptr_t)(pthread_cond_t*)drwrap_get_arg(wrapcxt, 0), 0};
+    set_sync_event(tcxt, SGLPRIM_SYNC_CONDSIG, data);
+}
+static void
+wrap_post_pthread_cond_sig(void *wrapcxt, void *user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_EXIT(pthread_cond_sig, tcxt->thread_id);
+    reactivate(tcxt);
+
+    send_sync_event(tcxt);
+}
+
+
+////////////////////////////////////////////
+// PTHREAD SPIN LOCK
+////////////////////////////////////////////
+static void
+wrap_pre_pthread_spin_lock(void *wrapcxt, OUT void **user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_ENTER(pthread_spin_lock, tcxt->thread_id);
+
+    SyncID data[2] = {(uintptr_t)(pthread_spinlock_t*)drwrap_get_arg(wrapcxt, 0), 0};
+    set_sync_event(tcxt, SGLPRIM_SYNC_SPINLOCK, data);
+}
+static void
+wrap_post_pthread_spin_lock(void *wrapcxt, void *user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_EXIT(pthread_spin_lock, tcxt->thread_id);
+
+    send_sync_event(tcxt);
+}
+
+
+////////////////////////////////////////////
+// PTHREAD SPIN UNLOCK
+////////////////////////////////////////////
+static void
+wrap_pre_pthread_spin_unlock(void *wrapcxt, OUT void **user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_ENTER(pthread_spin_unlock, tcxt->thread_id);
+
+    SyncID data[2] = {(uintptr_t)(pthread_spinlock_t*)drwrap_get_arg(wrapcxt, 0), 0};
+    set_sync_event(tcxt, SGLPRIM_SYNC_SPINUNLOCK, data);
+}
+static void
+wrap_post_pthread_spin_unlock(void *wrapcxt, void *user_data)
+{
+    per_thread_t *tcxt = drmgr_get_tls_field(dr_get_current_drcontext(), tls_idx);
+    LOG_SYNC_EXIT(pthread_spin_unlock, tcxt->thread_id);
+
+    send_sync_event(tcxt);
+}
+
+#endif
diff --git a/clients/drsigil/start_stop_functions.h b/clients/drsigil/start_stop_functions.h
new file mode 100644
index 0000000..32d6b8e
--- /dev/null
+++ b/clients/drsigil/start_stop_functions.h
@@ -0,0 +1,62 @@
+#ifndef START_STOP_H
+#define START_STOP_H
+
+#include "drsigil.h"
+#include "drmgr.h"
+
+static void
+flush_and_enable_instrumentation()
+{
+    /* TODO */
+    roi = true;
+    /* instrumentation will not begin for future bb's */
+    dr_printf("DrSigil Beginning ROI\n");
+}
+
+
+static void
+flush_and_disable_instrumentation()
+{
+    /* TODO */
+    dr_printf("DrSigil Ending ROI\n");
+}
+
+static void
+wrap_pre_start_stop_same()
+{
+    flush_and_enable_instrumentation();
+}
+static void
+wrap_post_start_stop_same()
+{
+    flush_and_disable_instrumentation();
+}
+
+////////////////////////////////////////////
+// START COLLECTING EVENTS
+////////////////////////////////////////////
+static void
+wrap_pre_start_func(void *wrapcxt, OUT void **user_data)
+{
+    flush_and_enable_instrumentation();
+}
+static void
+wrap_post_start_func(void *wrapcxt, void *user_data)
+{
+}
+
+
+////////////////////////////////////////////
+// STOP COLLECTING EVENTS
+////////////////////////////////////////////
+static void
+wrap_pre_stop_func(void *wrapcxt, OUT void **user_data)
+{
+    flush_and_disable_instrumentation();
+}
+static void
+wrap_post_stop_func(void *wrapcxt, void *user_data)
+{
+}
+
+#endif
-- 
1.8.3.1

